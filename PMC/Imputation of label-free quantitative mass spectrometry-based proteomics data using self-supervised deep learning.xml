<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">
<pmc-articleset><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38926340</article-id><article-id pub-id-type="pmc">11208500</article-id>
<article-id pub-id-type="publisher-id">48711</article-id><article-id pub-id-type="doi">10.1038/s41467-024-48711-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8833-7617</contrib-id><name><surname>Webel</surname><given-names>Henry</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4571-4368</contrib-id><name><surname>Niu</surname><given-names>Lili</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Nielsen</surname><given-names>Annelaura Bach</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2879-9224</contrib-id><name><surname>Locard-Paulet</surname><given-names>Marie</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1292-4799</contrib-id><name><surname>Mann</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Lars Juhl</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6323-9041</contrib-id><name><surname>Rasmussen</surname><given-names>Simon</given-names></name><address><email>srasmuss@sund.ku.dk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/035b05819</institution-id><institution-id institution-id-type="GRID">grid.5254.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0674 042X</institution-id><institution>Novo Nordisk Foundation Center for Protein Research, Faculty of Health and Medical Sciences, </institution><institution>University of Copenhagen, </institution></institution-wrap>Copenhagen N, Denmark </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5254.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0674 042X</institution-id><institution>Novo Nordisk Foundation Center for Basic Metabolic Research, Faculty of Health and Medical Sciences, </institution><institution>University of Copenhagen, </institution></institution-wrap>Copenhagen N, Denmark </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.15781.3a</institution-id><institution-id institution-id-type="ISNI">0000 0001 0723 035X</institution-id><institution>Institut de Pharmacologie et de Biologie Structurale (IPBS), </institution><institution>Université de Toulouse, CNRS, Université Toulouse III - Paul Sabatier (UT3), </institution></institution-wrap>Toulouse, France </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04py35477</institution-id><institution-id institution-id-type="GRID">grid.418615.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 0491 845X</institution-id><institution>Department of Proteomics and Signal Transduction, </institution><institution>Max Planck Institute of Biochemistry, </institution></institution-wrap>Martinsried, Germany </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05a0ya142</institution-id><institution-id institution-id-type="GRID">grid.66859.34</institution-id><institution-id institution-id-type="ISNI">0000 0004 0546 1623</institution-id><institution>The Novo Nordisk Foundation Center for Genomic Mechanisms of Disease, </institution><institution>Broad Institute of MIT and Harvard, </institution></institution-wrap>Cambridge, MA 02142 USA </aff></contrib-group><pub-date pub-type="epub"><day>26</day><month>6</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>26</day><month>6</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>15</volume><elocation-id>5405</elocation-id><history><date date-type="received"><day>1</day><month>2</month><year>2023</year></date><date date-type="accepted"><day>13</day><month>5</month><year>2024</year></date></history><permissions><copyright-statement>© The Author(s) 2024</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Imputation techniques provide means to replace missing measurements with a value and are used in almost all downstream analysis of mass spectrometry (MS) based proteomics data using label-free quantification (LFQ). Here we demonstrate how collaborative filtering, denoising autoencoders, and variational autoencoders can impute missing values in the context of LFQ at different levels. We applied our method, proteomics imputation modeling mass spectrometry (PIMMS), to an alcohol-related liver disease (ALD) cohort with blood plasma proteomics data available for 358 individuals. Removing 20 percent of the intensities we were able to recover 15 out of 17 significant abundant protein groups using PIMMS-VAE imputations. When analyzing the full dataset we identified 30 additional proteins (+13.2%) that were significantly differentially abundant across disease stages compared to no imputation and found that some of these were predictive of ALD progression in machine learning models. We, therefore, suggest the use of deep learning approaches for imputing missing values in MS-based proteomics on larger datasets and provide workflows for these.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Imputation in mass spectrometry-based proteomics is a recurrent step of importance for downstream analysis. Here, the authors offer an extensive comparison workflow of 27 established with three new scalable, fast and performant methods from deep learning for large and high-dimensional data.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Data processing</kwd><kwd>Proteome informatics</kwd><kwd>Mass spectrometry</kwd><kwd>Computational models</kwd><kwd>Proteomics</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100009708</institution-id><institution>Novo Nordisk Fonden (Novo Nordisk Foundation)</institution></institution-wrap></funding-source><award-id>NNF21SA0072102</award-id><award-id>NNF14CC0001</award-id><award-id>NNF19SA0035440</award-id><award-id>NNF14CC0001</award-id><award-id>NNF14CC0001</award-id><award-id>NNF14CC0001</award-id><award-id>NNF14CC0001</award-id><award-id>NNF14CC0001</award-id><principal-award-recipient><name><surname>Webel</surname><given-names>Henry</given-names></name><name><surname>Niu</surname><given-names>Lili</given-names></name><name><surname>Nielsen</surname><given-names>Annelaura Bach</given-names></name><name><surname>Locard-Paulet</surname><given-names>Marie</given-names></name><name><surname>Mann</surname><given-names>Matthias</given-names></name><name><surname>Jensen</surname><given-names>Lars Juhl</given-names></name><name><surname>Rasmussen</surname><given-names>Simon</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>© Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front>
  <body>
    <sec id="Sec1" sec-type="introduction">
      <title>Introduction</title>
      <p id="Par3">Proteomics is a technology for the identification and quantification of proteins to answer a broad set of biological questions<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> and together with RNA and DNA sequencing offers a way to map the composition of biological systems. It is widely applied across many fields of research including identification of biomarkers and drug targets for diseases such as alcoholic liver disease (ALD)<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, ovarian cancer<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and Alzheimer’s disease<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Different workflows have been developed for analysis of body fluids, cells, frozen tissues and tissue slides, and are rapidly evolving. Recent technological advancements have enabled proteome analysis at the single cell or single cell-population level<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, allowing the selection of single cells using image recognition<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. However, for most approaches, missing values are abundant due to the semi-stochastic nature of precursor selection for fragmentation and need to be replaced for at least some parts of the data analysis. Currently, imputation of missing values in proteomics data usually assumes that the protein abundance was below the instrument detection limit or the protein was absent. In general, the community differentiates between missing at random (MAR) which is assumed to affect all intensities across the dynamic range, whereas missing not at random (MNAR) becomes more prevalent the more the intensity of a peptide approaches the limit of detection of the instrument. However, not all missing values are due to this mechanism, and by assuming the limit of detection as the reason for missingness will lead to potentially wrong imputations and subsequently to biased statistical results that are limiting the conclusion from data. A strategy is therefore to combine missing completely at random (MCAR) and simulated MNAR in comparisons<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
      <p id="Par4">Various acquisition methods have been developed including data-independent acquisition (DIA), BoxCar and PASEF to alleviate the “missing value” problem in data-dependent acquisition (DDA) methods<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup>. Advances in informatics solutions have also greatly improved data analysis of mass spectra acquired by these acquisition methods and consequently proteome depth and data completeness<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. However, missing value imputation of search results for downstream analysis remains a recurring task for most applications. The noise in data from the instrument as well as peptide identification is most abundant for label-free quantification proteomics in DDA with missingness ranging from 10-40%<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, but for instance, blood plasma measured using DIA in a study of ALD still contained 37% missing values across all samples and protein groups before any filtering. Independent of the proteomics setup, once data is to be analyzed, the remaining missing values between samples have to be imputed for most methods. Therefore, how they are handled will influence the downstream results.</p>
      <p id="Par5">Several methods have been evaluated for imputation of MS proteomics data with an overview and a benchmark provided in work by Wang et al., using NAguideR<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Despite an abundance of imputation methods, an often-used approach to impute data at the protein group level is to use random draws from a down-shifted normal (RSN) distribution. The mass spectrometry (MS) signal comes from ions and most people are interested in the summary of ions through peptide spectrum matches to groups of proteins. The protein intensities, stemming from aggregation of the precursor and/or fragment ion values in MS1 and MS2 scans, are assumed to be log-normally distributed, i.e. the log transformed intensities are entirely determined by their mean and variance. In RSN replacements are then drawn using a normal distribution with a mean shifted towards the lower detection limit with a reduced variance. This is done on the assumption that the data is left-censored, i.e. that proteins are missing due to absence or lower abundance than the instrument detection limit. Following this line of thought, several studies focus on determining what works best for different causes of missing values using some form of simulation<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. Other studies focus their analysis on post-translational modifications<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, the best combination of software tools, datasets and imputation method<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, normalization and batch effects correction<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> or downstream analysis<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. Other methods have been developed to handle specific missing mechanisms, for instance, random imputation, fixed value imputation such as limit of detection or x-quantile of feature, model-based imputation using k-nearest neighbor (KNN), linear models<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> or tree-based models<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. These either impute using a global minimum, a statistic calculated on a single feature or a few features, with the need to iteratively consider each feature at a time. Finally, approaches such as DAPAR and Prostar offer several methods for imputing left-censored data, e.g. the widely used drawing from a normal distribution around the lower detection limit where the Gaussian mean and variance are estimated using quantile regression, abbreviated QRILC<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. Their latest development is Pirat exploiting correlations of precursors from the same protein group<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. MSStats offers end-to-end statistical analysis. It requires MaxQuant running samples jointly and providing both precursor as well as protein groups next to grouping information and does not impute all values.</p>
      <p id="Par6">Most previous work on developing methods for imputation of MS proteomics data focus on small scale setups where they for instance evaluate two separate groups in three replicates<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Other studies prefer to reduce the number of initial missing values by transferring identification from one run to the next using e.g. Match Between Run implemented in MaxQuant or cross-assignment by Proline<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. Alternatively, an established laboratory method to tackle variability in small-scale setups is to use replicates of samples to have complementary measurements, in order to transfer identifications between runs<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The specific evaluation strategy varies between the setup of the data and the missing values simulation approach, but to our knowledge, no scalable workflows are provided to run the evaluation on a new single tabular dataset generating a validation and test dataset, as well as a generic and flexible approach to imputation.</p>
      <p id="Par7">We turn to machine learning for imputation of missing intensities as it offers the possibility to learn from the data itself. Deep Learning (DL) has been used to improve over existing machine learning models in a variety of biological data problems<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>. DL has been successfully applied to predict peptide features such as retention times, collisional cross-sections and tandem mass spectra, significantly boosting the peptide identifications and precision of searches of MS-based proteomics<sup><xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR34">34</xref></sup>. We use intensities from search results generated from tools like e.g. MaxQuant or Spectronaut and apply DL methods to impute these. We considered three types of models that process the search results of precursors, peptides or protein groups slightly differently. First, we considered a collaborative filtering (CF) approach, where each feature and each sample is assigned a trainable embedding. Second, we considered an autoencoder with a deterministic latent representation - a denoising autoencoder (DAE). Third, we considered a variational autoencoder (VAE) as a generative model that encodes a stochastic latent representation, i.e. a high-dimensional Gaussian distribution. Although the inputs to all models are intensities for a feature and sample combination, the training objectives, complexity, and therefore capabilities of the models are different. The CF and autoencoder objective only focus on reconstruction, whereas the VAE adds a constraint on the latent representation. Furthermore, the first two modeling approaches use a mean-squared error (MSE) reconstruction loss, whereas the VAE uses a probabilistic loss to assess the reconstruction error.</p>
      <p id="Par8">Here, we use large (<italic>N</italic> ≈ 450) and smaller (<italic>N</italic> ≈ 50) MS-based proteomics datasets of HeLa cell line tryptic lysates acquired on a single machine (Q Executive HF-X Orbitrap) over a period of roughly two years to evaluate the general performance on repeated measurements of an homogenous biological sample on a medium to large-scale dataset. We apply three different DL models (CF, DAE, VAE) which create feature representation holistic for the entire distribution in a given dataset prior to any normalization. For evaluation, we develop a workflow that allows comparison between the three DL models and 27 approaches (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). Finally, we apply the VAE to a study of ALD patients and identify 30 (+ 13.2%) more significantly differential abundant protein groups in comparison to no imputation and that additional protein groups can be leveraged for predicting disease. We name our set of models and workflows proteomics imputation modeling mass spectrometry (PIMMS) and make the workflows, code, and example configs available at <ext-link ext-link-type="uri" xlink:href="https://github.com/RasmussenLab/pimms">https://github.com/RasmussenLab/pimms</ext-link>. To enable reproducibility and adaptation to new data and strategies, we share our Python code along snakemake workflows.</p>
    </sec>
    <sec id="Sec2" sec-type="results">
      <title>Results</title>
      <sec id="Sec3">
        <title>Evaluating self-supervised models for imputation of MS data</title>
        <p id="Par9">We assessed the capability of three unsupervised models for proteomics data imputation. First, we considered modeling proteomics data using CF assigning each sample and each feature an embedding vector and using their combination to predict intensity values. Second, we considered a standard autoencoder, training it using a denoising strategy that has to learn to reconstruct masked values making it a DAE. Third, we applied a VAE with a stochastic latent space (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). The two autoencoder architectures used all features to represent a sample in a low dimensional space, which was used to reconstruct the original data. In contrast, the CF model had to learn both a latent embedding space for the samples and features. We compared these to a heuristic-based approach of median imputation per feature across samples and 26 other approaches such as k-nearest neighbors (KNN) or random forest (RF) (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). While the DL methods, KNN and median imputation were able to also impute large datasets, nine methods implemented in R failed to scale to high-dimensional data.<fig id="Fig1"><label>Fig. 1</label><caption><title>Overview of workflow for downstream analysis tasks and HeLa dataset.</title><p><bold>a</bold> Single tabular results taken from MS data analysis software (search and quantification) were used as input for downstream analysis. Here we used MaxQuant for data dependent acquisition to analyze raw MS data. We compared three different self-supervised DL approaches with 27 other methods: median imputation and KNN interpolation exemplified. Green and red not-available (NA) indicate simulated and real missing values. <bold>b</bold> Principal component one versus two of 539 selected HeLa runs for protein groups recorded on one instrument. <bold>c</bold> Same as (<bold>b</bold>), based on the 50 runs forming the small development dataset. We used a cutoff of 25% feature prevalence across samples to be included into the workflow shown in (<bold>a</bold>). Samples were filtered in a second step by their completeness of the selected features (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>).</p></caption><graphic xlink:href="41467_2024_48711_Fig1_HTML" id="d33e438"/></fig></p>
        <p id="Par10">Our development dataset consisted of 564 HeLa runs of one Q Executive HF-X Orbitrap generated during continuous quality control of the mass spectrometers<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. We initially investigated the structure of the dataset using the first two principal components (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>), which grouped the samples into two separate clusters. The median prevalence per protein group, i.e. the median number of samples where a protein group was detected, was 526 samples [min: 45, max: 564], and samples had a median of 3768 protein groups [min: 2170, max: 4185]. We generated another smaller development dataset using the most recent 50 samples (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>) to test dependence of performance on sample size. Whereas median imputation per feature across samples did not condition their imputation on the value of other features in a given sample, our and other machine learning approaches consider other feature values. Validation and test data were drawn with 25, 50 or 75 percent MNAR using the procedure laid out in Lazar et al.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> from all samples in a dataset. This ensures that lower intensities are represented sufficiently, but also makes training harder due to additionally removing values from low abundant features with fewer quantifications.</p>
      </sec>
      <sec id="Sec4">
        <title>Imputing precursors, aggregated peptides, and protein group data</title>
        <p id="Par11">We applied the imputation methods to the development dataset, e.g. consisting of 564 samples for protein groups using our selection criteria (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). We ran several configurations using a grid search to find the best configurations using simulated missing values in a validation and test split from all samples (see Methods, Supplementary Data <xref rid="MOESM4" ref-type="media">1</xref>, <xref rid="MOESM5" ref-type="media">2</xref>). In absolute numbers these were 100,001 for the protein group, 616,561 aggregated peptides and 661,817 precursors intensities in the test set for our development dataset of instrument 6070. When investigating the performance of the imputation methods we used the mean absolute error (MAE) on the log2 scaled intensities between predicted and true measured intensity values on our simulated missing values. Focusing on the best performing models from the grid search, we evaluated these in a setup of 25 percent MNAR in the simulated missing data. We found that the DL approaches had less than half MAE compared (0.55, 0.54 and 0.58 for CF, DAE and VAE, respectively) to the median imputation with MAE of 1.24. KNN of samples across HeLa cell line measurements had a MAE of 0.59 using the scikit learn implementation and of 0.68 using an R based implementation using impute<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. The R based random forest (RF) imputation<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> only completed for the protein groups and had an MAE of 0.59 and a Bayesian principal component analysis (BPCA) with 0.53. Thus, BPCA was slightly better (0.01 MAE) compared to the second-best model on protein groups however it did not finish for the other two datasets within 24 h. Overall, when comparing the performance across levels of data aggregation for the best models (Fig. <xref rid="Fig2" ref-type="fig">2a,c</xref>, Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>) we found that the self-supervised models performed similarly to other methods. Across all models, we found the overall performance to be the worst for the protein-level data, better for aggregated peptides, and best for precursors. This is in line with previous results of Lazar and co-workers that showed better performance for lower levels of aggregations as it avoids implicit imputations by a neutral element<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. A higher share (25, 50 and 75 percent) of MNAR simulated missing values was associated with a decreased overall performance of most methods (Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>). One reason for this could be that features with fewer observations are oversampled and the total number of available training data points drops as low as four intensities in the training data split. To balance available data for low abundant features and statistical power in evaluation, we set the share of MNAR to 25 percent in our evaluations.<fig id="Fig2"><label>Fig. 2</label><caption><title>Different levels of MS-based proteomics data can be imputed using self-supervised DL models.</title><p><bold>a</bold> Performance of best imputation methods at the level of protein groups, aggregated peptides, and precursors for MaxQuant outputs with 25 percent MNAR. Mean absolute error (MAE) is shown on the y-axis. Blue: KNN (scikit-learn), green: CF, red: DAE, purple: VAE, olive: BPCA, brown: random forest (missForest), orange: KNN_IMPUTE (impute), pink: IMPSEQ, dark-orange: MICE-NORM, sand: SEQKNN. The DL methods performed better or equally good in comparison to other models. Performance was similar for all three DL models on each data level and less aggregated data, i.e models on precursors and aggregated peptides performed better compared to models on protein groups. BPCA and RF did not finish for aggregated peptides and precursors within 24 h. Models were ordered by overall performance on the three datasets combining the best five for each. N: number of samples, M: number of features. <bold>b</bold> As (<bold>a</bold>) but showing a decrease in performance for a subset of the data with a maximum of 50 samples. The CF adapted better to smaller sample sizes. <bold>c</bold>, <bold>d</bold> Protein groups medians were binned by their integer median value and the boxplot of the proportion of missing values per protein group is shown for large and small development. N: Number of protein groups in bin in parentheses. The box of the boxplots extends from the first quartile to the third quartile of the proportions with their median as a separate line. The whiskers extend no more than one and a half times the interquartile range, ending at the farthest proportion within that interval. Outliers are plotted as separate dots. <bold>e</bold> MAE 95% bootstrapped confidence interval for protein groups intensities in the test split of the larger development dataset binned by the integer value of the protein group’s median intensity in the training data split (N: number of intensities in a bin from test split, 4495 protein groups, 103,902 intensities, models ordered by overall performance, 1000 draws to compute the confidence interval). Models on protein groups with a median intensity above 26 performed in the range of the overall performance shown in (<bold>a</bold>). <bold>f</bold> MAE 95% bootstrapped confidence interval for protein groups in test split of smaller development dataset binned by integer value of intensity (N: number of intensities in a bin from test split, 4405 protein groups, 9327 intensities, models ordered by overall performance, 1000 draws to compute the confidence interval). <bold>g</bold> Imputed and ground truth (observed) intensities for test data on large protein group development dataset for top four overall models.</p></caption><graphic xlink:href="41467_2024_48711_Fig2_HTML" id="d33e528"/></fig></p>
        <p id="Par12">Runtime for imputation of protein groups including data loading, manipulations and training were between 14 s using the minimum and roughly 8 h for IRM (for the best models in h:min:sec: KNN-IMPUTE: 0:19, KNN: 0:41, VAE: 1:58, DAE: 2:24, CF: 2:28, RF: 1:05:02, BPCA: 6:37:56) using 1 CPU and up to 192 GB of shared memory for the PIMMS models (Supplementary Data <xref rid="MOESM7" ref-type="media">4</xref>). Runtimes of PIMMS models for the precursor dataset were higher (KNN: 4:17, DAE: 33:08, VAE: 1:35:51 and CF: 18:23), but could run faster with special hardware (GPUs). Among the models with good performance we found that BPCA, MICE-NORM, SEQKNN and RF did not scale to the high dimensional datasets of peptides and precursors for the large datasets. IMPSEQ ran into errors for all larger datasets. In general runtimes varied based on the number of epochs, mini batch-size, model architecture, patience for early stopping and the initial random weights. Additionally, the grid search results showed that the models could be trained without prior normalization and were able to fit the data using many different hyperparameter configurations (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>). Reducing the training sample by roughly a tenth, performance on the smaller development dataset (N = 50) was comparable to the performance on the larger development dataset, while runtime reduced by 5 to 15 times for precursors (Median: 0:23, KNN: 0:33, VAE: 1:04, DAE: 0:52, CF: 1:06). On the medium sized HeLa datasets all models completed (Supplementary Data <xref rid="MOESM7" ref-type="media">4</xref>) for the protein groups and 26 out of 27 for the peptides and precursors. We found that IMPSEQ<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and BPCA<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> performed best on the 50 homogenous HeLa samples although some with longer runtimes compared to PIMMS models (e.g. on precursors: IMPSEQ: 1:09, BPAC: 20:44, MICE-NORM: 4:42). For fewer than 50 samples the PIMMS models can fit the data, but alternatives were better suited (Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>). In summary, this indicated that on an unnormalized, intensity varying dataset (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3a</xref>) from a single machine the PIMMS models were able to capture patterns between detected features to impute values for as few as 50 samples with competitive performance to other state-of-the-art methods.</p>
      </sec>
      <sec id="Sec5">
        <title>Imputation was consistent across a wide range of intensities</title>
        <p id="Par13">We evaluated the performance across the dynamic range of intensities by binning test intensities by their feature’s training median and reporting the average error per bin for the best five models. For protein groups on both development datasets the minimal median intensity was 23 for protein groups (Fig. <xref rid="Fig2" ref-type="fig">2c, d</xref>). The average error for intensities was roughly twice as large for intensities from the minimum median intensity bin in comparison to the best performance (Fig. <xref rid="Fig2" ref-type="fig">2e, f</xref>). Interestingly, the worst performance on the large HeLa dataset was observed for protein groups with the highest observed median intensity, which for intensities from one protein group had a two to four times worse MAE than the overall MAE. The relative performance between the models was consistent across the bins. If we compared this to the relative distribution of imputed intensities (with errors) to the unmodified intensities in the test split, we found that low intensity values were partly imputed towards the center of the intensity distribution (Fig. <xref rid="Fig2" ref-type="fig">2g</xref>). Furthermore, we investigated whether there was a difference in the accuracy of the imputation based on how often a protein group was observed. Here we found that the MAE varied between 0.6 and 0.8 for proteins observed in 25-80% of the samples, whereas for proteins observed in more than 80% of the samples the MAE decreased to below 0.4 (Fig. <xref rid="Fig2" ref-type="fig">2c, e</xref>). We observed a similar trend when analyzing the smaller dataset of only 50 samples (Fig. <xref rid="Fig2" ref-type="fig">2d, f</xref>). This indicated that some protein groups were harder for all the methods to impute than others, but also that the CF, DAE, and VAE predicted consistently across a wide range of protein groups intensities. We found similar results for the two other levels of data, aggregated peptides and precursors (Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>). Finally, we investigated how stable the self-supervised models trained. In a first step, we randomly permuted all protein groups of the large development dataset, which leaves the median performance unchanged. Accordingly, training models on randomly permuted data could not outperform the imputation by the median of the training data split with an MAE of 1.25 (Supplementary Fig. <xref rid="MOESM1" ref-type="media">5a</xref>, Supplementary Data <xref rid="MOESM8" ref-type="media">5</xref>, CF: 1.24, VAE: 1.26, KNN_IMPUTE: 1,30 DAE: 1.32). Next, we trained the self-supervised models five times on the same data split as well as repeated training five times on new splits. The performance between fitted models varied in a narrow margin (protein groups: CF: 0.538-0.550, DAE: 0.535-0.545, VAE: 0.561-0.587 [Min-Max]) (Supplementary Figs. <xref rid="MOESM1" ref-type="media">5b,c</xref>, Supplementary Data <xref rid="MOESM8" ref-type="media">5</xref>). We therefore conclude that the DL models could be consistently fitted to data and that self-supervised models were able to fit the data holistically for imputation purposes.</p>
      </sec>
      <sec id="Sec6">
        <title>Competitive within-sample and feature-wise between-sample correlation</title>
        <p id="Par14">We evaluated performance without a specific distance measure by evaluating Pearson correlations of simulated missing values to the truth (Supplementary Fig. <xref rid="MOESM1" ref-type="media">6</xref>, Supplementary Data <xref rid="MOESM9" ref-type="media">6</xref>). The mean Pearson correlation between samples for protein groups was a bit higher for BPCA than the others, including the self-supervised models for the imputations on the large development dataset (BPCA, DAE: 0.86, CF: 0.85, VAE: 0.84, RF: 0.83, KNN: 0.82, KNN_IMPUTE: 0.77). The correlation between features within a sample was higher in general, with a mean correlation of around 0.95 for all models seen among the best along the sorting by the MAE (BPCA, DAE, CF: 0.96, VAE, RF, KNN: 0.95, KNN_IMPUTE: 0.93). This showed that the ordering within a sample was better than the correlation of protein groups between samples as the overall abundance level of single protein groups vary across samples. For the smaller development dataset we found similar trends of within-sample correlation (IMPSEQ: 0.80, BPCA: 0.80, CF: 0.79, SEQKNN, VAE: 0.78, DAE: 0.76) and between feature sample correlation (IMPSEQ, BPCA, CF, SEQKNN, VAE: 0.95, MICE-NORM, DAE: 0.94). Both correlation comparisons, therefore, indicated that the three self-supervised models were able to model the data well without prior normalization of the data.</p>
      </sec>
      <sec id="Sec7">
        <title>Performance of PIMMS on simulated missing values in real use case</title>
        <p id="Par15">To assess the impact of imputation on a large real-world DIA dataset, we applied PIMMS to 455 blood plasma proteomics samples from a cohort of ALD patients and healthy controls<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. After imputation we again compared how well PIMMS imputed simulated missing values with a share of 25 percent MNAR in the ALD data and found that DAE, TRKNN, CF, RF and VAE achieved similar results of MAE between 0.52-0.55 (Figs. <xref rid="Fig3" ref-type="fig">3</xref>a, <xref rid="Fig4" ref-type="fig">4a–c</xref>, Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>). BPCA which performed well on the development dataset, was worse with an MAE of 2.95 and took more than 10 minutes to complete compared to between 30 seconds up to just above one minute for the best models (Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>, <xref rid="MOESM7" ref-type="media">4</xref>). In the original study RSN was used for imputation on a per sample basis, i.e. using the mean and standard deviation of all protein groups in a sample. This yielded 8 times worse results on the simulated data (Supplementary Data <xref rid="MOESM10" ref-type="media">7</xref>). Having many samples available, we also tested to only simulate missing values on a stratified subset of samples (by kleiner score, see “Methods”) leading to 68 samples and 69 samples assigned to the validation and test split. Sampling simulated missing values only from these led to fewer available intensities for comparison but matched the results when sampling missing values from all available samples (Supplementary Data <xref rid="MOESM11" ref-type="media">8</xref>). The correlation across samples for each protein group was lower for the ALD data in comparison to the heterogeneous measurements of HeLa cell line data for the self-supervised models (median; DAE: 0.60, TRKNN: 0.58, CF: 0.54, VAE: 0.53, RF: 0.52) based on 377 protein groups with at least 3 intensities in test data split (Supplementary Data <xref rid="MOESM10" ref-type="media">7</xref>). The median correlation within samples (Supplementary Data <xref rid="MOESM10" ref-type="media">7</xref>) was around 0.98 for all models keeping the order by MAE. This, thus, matched the overall results from the HeLa data analysis.<fig id="Fig3"><label>Fig. 3</label><caption><title>Performance on simulated missing data with 25 percent MNAR in ALD dataset and effects of imputation on differential analysis.</title><p><bold>a</bold> Performance for protein groups of plasma proteome data using a share of 25 percent MNAR simulated missing values on the full dataset and (<bold>b</bold>) on the 80% dataset using the same configurations (LD: latent dimension, HL: hidden layer dimension). The performance of best five imputation methods for protein groups of plasma proteome data using a share of 25 percent MNAR simulated missing values (red: DAE, dark-green: truncated KNN (TRKNN), green: CF, brown: RF, purple: VAE) <bold>c</bold> q-values, i.e. multiple testing adjusted p-values using Benjamini-Hochberg method, for 17 protein groups which were differentially abundant using the full dataset without imputation, but not for the 80% dataset without imputation. The gray line indicates the five percent FDR cutoff. No imputation (None) shows the result without imputing values. Original <italic>p</italic> values were calculated using an analysis of covariance (ANCOVA), i.e. a regression of the protein intensity along several clinical covariates. <bold>d</bold> q-values for three protein groups which were differentially abundant using the 80% dataset without imputation, but not initially when using all data. <bold>e</bold> Count of false negative (FN) and true positives (TP) per method on the reduced dataset taking the 17 differentially abundant protein groups from (<bold>c</bold>) on the full dataset as ground truth. <bold>f</bold> Same as in (<bold>e</bold>) for three protein groups in (<bold>d</bold>), labeling differentially abundant protein groups in the 80% dataset as false positives (FP) since they did not show up as differentially abundant in the full dataset. The three examples are around the FDR cutoff without imputation. True negatives (TN) are not differentially abundant here (Supplementary Data <xref rid="MOESM8" ref-type="media">9</xref>). No imputation (None) is the reference defining the labels.</p></caption><graphic xlink:href="41467_2024_48711_Fig3_HTML" id="d33e677"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><title>Underlying data distribution and performance in detail on an ALD dataset.</title><p><bold>a</bold> Stacked histogram of validation and test split using a share of 25 percent MNAR. Bins were created using the integer value of the log2 transformed intensities. N is the number of intensities in the split, each corresponds to 5 percent of all available quantifications. <bold>b</bold> Protein groups medians were binned by their integer median value and the boxplot of the proportion of missing values is shown. N: Number of protein groups in bin in parentheses. The box of the boxplots extends from the first quartile to the third quartile of the proportions with their median as a separate line. The whiskers extend no more than one and a half times the interquartile range, ending at the farthest proportion within that interval. Outliers are plotted as separate dots. <bold>c</bold> MAE with 95% bootstrapped confidence interval for protein groups intensities in the test split binned by the integer value of the protein group’s median intensity in the training data split. RF was replaced with QRILC to showcase a MNAR focused method. (N: number of intensities in a bin from test split, 377 protein groups, 7935 intensities, models ordered by overall performance, 1000 draws to compute the confidence interval). <bold>d–g</bold> Comparison of q-values, i.e. multiple testing adjusted p-values using Benjamini-Hochberg method, based on no imputation (measured) and adding imputations using the models for four protein groups. orange: kleiner score &gt; 1, blue: kleiner score &lt;2, crosses in the first column indicate observed values, and dots in other imputation by models. Original <italic>p</italic> values were calculated using an analysis of covariance (ANCOVA), i.e. a regression of the protein intensity along several clinical covariates, either only for the observed intensities or using the imputed intensities in addition to the observed intensities.</p></caption><graphic xlink:href="41467_2024_48711_Fig4_HTML" id="d33e701"/></fig></p>
      </sec>
      <sec id="Sec8">
        <title>Recovering lost differentially abundant protein groups</title>
        <p id="Par16">Then we tested the ability of the self-supervised methods to recover differentially abundant protein groups. To achieve this, we subset the dataset at a share of 25 percent MNAR and only kept 80 percent and compared differentially abundant proteins to using the full dataset. We found that the MAE performance on the simulated missing values with a share of 25 percent MNAR data were nearly the same compared to the full dataset (Fig. <xref rid="Fig3" ref-type="fig">3a, b</xref>, Supplementary Data <xref rid="MOESM6" ref-type="media">3</xref>). This can be explained by the relatively stable measurements of the dataset (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3b</xref>) and indicates that most protein groups were stable across patients. Comparing the results of the differential abundance analysis using the complete dataset compared to the reduced dataset using no imputations, we observed a decrease in the number of differentially abundant protein groups from 226 to 212 (ANCOVA, Benjamini-Hochberg multiple testing correction, <italic>p</italic> value ≤ 0.05). Interestingly, we found that besides 17 protein groups losing their differential abundance, three became differentially abundant by randomly removing data. To assess the effectiveness of various imputation methods, we used no imputation as a benchmark. We found that VAE and TRKNN imputation techniques successfully recovered 15 out of the 17 protein groups that lost their differential abundance (DAE, RF: 14, CF: 13, Median, QRILC: 2) (Fig. <xref rid="Fig3" ref-type="fig">3c, e</xref>, Supplementary Data <xref rid="MOESM12" ref-type="media">9</xref>). Three protein groups became differentially abundant without imputation when intensities were removed in the 80% dataset, which we labeled as false positives to follow our approach (Fig. <xref rid="Fig3" ref-type="fig">3d, f</xref>). However, the three protein groups were nearly statistically significantly abundant in the 100% dataset and therefore adding back patients through imputation to the analysis can be seen as beneficial. This indicated that the PIMMS models could recover lost signals with simulated missing data with a share of 25 MNAR.</p>
      </sec>
      <sec id="Sec9">
        <title>More differentially abundant proteins when using PIMMS</title>
        <p id="Par17">Then we investigated the number of differentially abundant protein groups as well as the ability of the plasma proteome to predict the fibrosis status of 358 individuals for the top five models, RSN, QRILC and median imputation (Supplementary Data <xref rid="MOESM13" ref-type="media">10</xref>). The fibrosis status was based on a liver biopsy, and individuals with all fibrosis scores ranging from zero to four were included. By using these models for imputation of missing data, leaving the available data as it was, we were able to perform the analysis for 377 protein groups (ANCOVA, Benjamini-Hochberg multiple testing correction, <italic>p</italic> value ≤ 0.05) compared to only 313 protein groups when using the RSN imputation approach as originally applied in Niu et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Comparing RSN and VAE we observed that both methods replaced missing values with a distribution shifted towards the lower abundance region. However, the maximum intensity for missing values was higher for the VAE with a value of 21 compared to the RSN with a value of 15 for protein groups (Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). Translating the overall shift in distribution by the VAE to the RSN idea, the intensity distribution was shifted by one standard deviation and the variance was shrunk by 0.7 in comparison to 1.6 standard deviations and 0.3 shrinkage in the ALD-RSN setup. This difference underlies a fundamental difference in the approaches and can be generalized for all other models (Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>). Whereas the RSN always assumes missing values due to low abundance, the VAE and other approaches assign some missing values a higher intensity if other protein groups in the same sample suggest that the missing value occurred rather due to a missed detection than low abundance. QRILC is a more nuanced MNAR method that does not only assign low intensities for missing values and we therefore added it to the comparison, although it shifts the distribution more strongly than RSN (Supplementary Fig. <xref rid="MOESM1" ref-type="media">7</xref>).</p>
        <p id="Par18">When performing differential analysis, we found that 209 of the 313 protein groups originally included were significantly differentially expressed using the RSN approach. However, 212 were significant without any imputation and up to 221 were significant using the other methods (TRKNN: 221, VAE: 220, DAE, CF: 219, RF: 217, QRILC: 216, Median: 211). Adding the newly included 64 protein groups, 258 of 377 in the PIMMS-VAE setup were differentially expressed, an increase by 23% to the originally included and imputed protein groups with RSN (see Methods and Supplementary Data <xref rid="MOESM13" ref-type="media">11</xref>). We found that using the nine approaches, 287 decisions from the differential expression analysis were the same for the 313 shared protein groups. We went on analyzing the 26 differences in detail (Supplementary Data <xref rid="MOESM14" ref-type="media">12</xref>). First, we found that nine of the 26 protein groups were differentially abundant without imputation. For these nine protein groups RSN imputation led to non-significant results, whereas the top five models found these nine to be significant. For the four least significant protein groups without imputation of missing values QRILC imputation made these significantly abundant. For three protein groups with multiple testing adjusted p-values (q-values) just above the 0.05 threshold the self-supervised models and TRKNN imputation led to differential abundance, whereas only Median imputation did not.</p>
        <p id="Par19">On one hand when using QRILC imputations six protein groups <italic>(IGLV9-49, TTN, DEFA1B/DEFA3, PRDX1, ACTA2/ACTG2, SSP2, F7</italic>) were significant which none of the other non-MNAR methods found, with few having very diverging q-values. Few relatively non-prevalent protein groups, i.e. here below 70% prevalence (250/358 and less) of quantified samples, showed a strong difference in differential analysis testing between e.g. the VAE and the RSN imputation. The relatively non-prevalent (222/358) protein group F5H8B0/P08709/P08709-2 associated with the gene <italic>F7</italic> was clearly not significant using the VAE or no imputation, but passed the statistical thresholds when using QRILC imputation and leaned towards significance using RSN. On the other hand, a relatively rarely quantified protein A0A0D9SG88 (gene <italic>CFH</italic>) was significant using VAE imputation, but not RSN or QRILC (Table <xref rid="Tab1" ref-type="table">1</xref>, Supplementary Data <xref rid="MOESM14" ref-type="media">12</xref>). Visualization using swarm plots indicated that the RSN and QRILC imputation for these two protein groups were shifted downwards of the original center (Fig. <xref rid="Fig4" ref-type="fig">4d-e</xref>). In combination with a slight imbalance of targets in the 142 imputed samples, this led to different results. Similarly, for two newly included protein groups P06702 (gene <italic>S100A9</italic>) and Q8WUD1/Q8WUD1-2 (gene <italic>RAB2B</italic>) imputation by a fixed median value or QRILC did not lead to significant results (Fig. <xref rid="Fig4" ref-type="fig">4f-g</xref>, Supplementary Data <xref rid="MOESM14" ref-type="media">12</xref>). It could, therefore, be good scientific practice to check the effect of imputation on differential analysis before a single protein group is selected for further analysis, especially for explicitly random methods such as RSN.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Examples of diverging decisions between imputation methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Protein Group</th><th>F5H8B0<break/>P08709<break/>P08709-2</th><th>A0A0D9SG88</th><th>A0A075B6R2</th><th>P02741</th><th>I3L0A1<break/>J3KPA1<break/>P54108<break/>P54108-2<break/>P54108-3</th><th>P59665<break/>P59666</th></tr></thead><tbody><tr><td>Gene</td><td>F7</td><td>CFH</td><td>IGHV4-4</td><td>CRP</td><td>CRISP3</td><td><p>DEFA1B</p><p>DEFA3</p></td></tr><tr><td>None (ref.)</td><td>0.954</td><td>0.117</td><td>0.000</td><td>0.030</td><td>0.018</td><td>0.365</td></tr><tr><td>DAE</td><td>0.916</td><td>0.022</td><td>0.000</td><td>0.038</td><td>0.001</td><td>0.025</td></tr><tr><td>TRKNN</td><td>0.470</td><td>0.059</td><td>0.000</td><td>0.029</td><td>0.000</td><td>0.082</td></tr><tr><td>VAE</td><td>0.918</td><td>0.038</td><td>0.000</td><td>0.039</td><td>0.000</td><td>0.023</td></tr><tr><td>RF</td><td>0.915</td><td>0.061</td><td>0.000</td><td>0.040</td><td>0.000</td><td>0.061</td></tr><tr><td>CF</td><td>0.841</td><td>0.079</td><td>0.000</td><td>0.031</td><td>0.001</td><td>0.022</td></tr><tr><td>Median</td><td>0.937</td><td>0.113</td><td>0.000</td><td>0.030</td><td>0.051</td><td>0.373</td></tr><tr><td>RSN</td><td>0.082</td><td>0.951</td><td>0.080</td><td>0.052</td><td>0.745</td><td>0.012</td></tr><tr><td>QRILC</td><td>0.002</td><td>0.727</td><td>0.003</td><td>0.062</td><td>0.626</td><td>0.029</td></tr></tbody></table><table-wrap-foot><p>Based on comparison of <italic>q</italic>-values, i.e. multiple testing corrected <italic>p</italic>-values using the Benjamini-Hochberg method, for previously included protein groups. Analysis without imputed values is denoted as None and used as reference (ref.). All diverging decisions are in Supplementary Data <xref rid="MOESM13" ref-type="media">11</xref>,<xref rid="MOESM14" ref-type="media">12</xref>.</p></table-wrap-foot></table-wrap></p>
        <p id="Par20">Finally, expanding the analysis to the 64 additional protein groups included here, we found 16 of them to be significantly differentially abundant without any imputation and 38 using VAE imputations (TRKNN: 44, VAE: 38, RF: 34, CF: 33, DAE: 32, QRILC: 19, Median: 14, see Supplementary Data <xref rid="MOESM13" ref-type="media">11</xref>). Therefore, apart from the difference from the shared protein groups, which yielded 11 more significant hits, imputation using e.g. the VAE allowed us to identify 38 additional protein groups that were not considered for statistical analysis in the original study.</p>
      </sec>
      <sec id="Sec10">
        <title>Robustness of differently abundant protein group identification</title>
        <p id="Par21">Next, we decided to analyze in more detail which protein groups were differentially abundant using the different models and how reliable the differential outcome was per model. We repeated the analysis on the ALD data ten times with 25 percent MNAR simulated missing values, re-training the models on the same training data split. In this scenario median, TRKNN and RSN imputations were guaranteed to yield the same result for all repetitions. Of the 313 originally included protein groups, 27 did not have the same differently abundant outcome for all models on all ten repetitions. Especially for protein groups with few missing values, seeing the same outcome is not unexpected. However, for nine protein groups with two to 29 percent missing values, adding RSN imputations changed the result from being differentially abundant without imputation to being not differentially abundant. The results did not change running DAE, TRKNN, CF, RF and VAE ten times for these nine protein groups. A total of 46 out of the 64 newly included protein groups with a higher percentage of missing values were at least once differentially abundant by using one of the methods. Of these 46 newly included ones, 16 were differentially abundant without imputation, which except by median or QRILC imputation was not changed using the top five model imputations. Additionally, 13 were at least identified seven times as differentially abundant using DAE, TRKNN, CF, RF or VAE imputations (Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>, Supplementary Data <xref rid="MOESM15" ref-type="media">13</xref>). Using CF imputation, we found 17 protein groups to be differentially abundant in all ten repetitions. Deterministic TRKNN imputation in our setup found 28 protein groups to be differential abundant ten times. However, ten of these were only found using TRKNN imputations at least seven times (Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>). We conclude that there is some variability in the analysis due to imputations and that for certain imputed protein groups the choice of the imputation method is crucial.</p>
      </sec>
      <sec id="Sec11">
        <title>Novel protein groups could be biological relevant</title>
        <p id="Par22">We then investigated if these protein groups could be associated with disease using the DISEASES database<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> and found that 39 of the 64 novel proteins had an association entry to fibrosis. Of the 38 at least once newly significant protein groups 30 had an association entry to fibrosis, with four having a confidence score greater than two (Supplementary Data <xref rid="MOESM16" ref-type="media">13</xref>, <xref rid="MOESM17" ref-type="media">14</xref>). For example, the protein P05362 from gene <italic>ICAM1</italic> had the highest disease-association with a score of 3.3. It was found to be significantly dysregulated in the liver data and missing in the plasma data in the original study. Following this reasoning, the second highest scoring protein group was composed of P01033 and Q5H9A7 (gene <italic>TIMP1</italic>). This indicated that the novel protein groups using the PIMMS criteria for inclusion could be biologically relevant.</p>
      </sec>
      <sec id="Sec12">
        <title>The additionally added protein groups were predictive of fibrosis</title>
        <p id="Par23">Finally, in the work by Niu et al., the authors trained machine learning models to predict clinical endpoints such as fibrosis from the plasma protein groups. To assess the impact on the machine learning model, a logistic regression, we used the data from the differential analysis above. We replicated the workflow performed by Niu et al., and evaluated the model using the ALD cohort for individuals with histology-based fibrosis staging data available (<italic>N</italic> = 358). Using minimum redundancy, maximum relevance (MRMR) approach we selected the most predictive set of features of each subset of features on the assigned training samples<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Using median imputation for all available protein groups yielded the best area under the receiver operating curve (AUROC) of 0.90 compared to 0.86–0.90 for DAE, TRKNN, CF, RF, QRILC and VAE on the test samples (Supplementary Fig. <xref rid="MOESM1" ref-type="media">8</xref>). For models trained on only the newly included protein groups the DAE and VAE model imputation led to the best AUROC using only one protein group (<italic>TIMP1</italic>) previously mentioned, whereas the model using five median imputed protein groups was worse (VAE: 0.80, DAE: 0.82, CF: 0.73, RF: 0.79, Median: 0.66, TRKNN: 0.76, QRILC: 0.74). Therefore, this suggests that the additional protein groups were retained with more missing values when using the PIMMS approach compared to the original study’s approach.</p>
      </sec>
    </sec>
    <sec id="Sec13" sec-type="discussion">
      <title>Discussion</title>
      <p id="Par24">Imputation is an essential step for many analysis types in proteomics, which is often done heuristically. Here we tested three models using a more holistic approach to imputation. We showed that CF, DAE and VAE models reached a similar performance on simulated missing values across the entire distribution of the data - including low abundant features. In comparison to most other methods they scaled better to high dimensional data or outperformed fast implementations as scikit-learn based KNN, which was especially beneficial when working with peptides instead of protein groups to avoid implicit imputations<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Further, we investigated the effect of the imputation method on a concrete analysis, using DIA data from 358 liver patients. Here we found that missing values were imputed by the models towards the lower end of the distribution but less pronounced as when using QRILC or RSN imputation which shifts all replacements towards the limit of detection (LOD) in a sample. We believe that this is due to the lowest abundant features limiting the learned data distributions and that some features are not set towards the LOD by the model due to being missing at random. We therefore argue that our holistic model-based imputations are more conservative than e.g. the RSN imputations and that the three self-supervised DL models offer a sensible approach to proteomics imputation while scaling well to high feature dimensions. We found that all methods besides RSN were a better choice for the ALD dataset analyzed.</p>
      <p id="Par25">Simulating 20 percent missing values with a share of 25 percent MNAR we saw the ability to recover signal by the three semi-supervised models. TRKNN and RF recovered lost signal in this application, but median imputation and QRILC were too heuristic to recover most of the signal (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). However, the analysis also revealed that some protein groups will be false positives or negatives as removing some observations can change the outcome. This suggests that imputation using our or other data driven models can recover lost biological signals which is in line with the observation that significant values were not made insignificant when using data driven models (DAE, TRKNN, CF, RF, VAE) in comparison to RSN or QRILC on the ALD cohort (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Finally, we offer a workflow to reproduce the comparison done here for all 30 general imputation methods using any other single tabular data provided by the user.</p>
      <p id="Par26">A limitation of the model-based approach is that the models should only be used for imputation if the samples are related. Therefore, the best imputation strategy will be dependent on the experimental setup. We showed that the models can learn to perform imputation on plasma samples from a diverse set of clinical phenotypes ranging from healthy to liver cirrhosis. However, the data-driven models would not perform well when imputing for instance one liver proteome together with ten plasma proteomes. In such a case the data-driven models would not have any other liver proteomes to learn from. If replicas on a small number of samples are the study design, KNN interpolation can be a good remedy or using a set of tests which among others capture missingness<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. DAE and VAE are not suited to be trained on too few samples, however the exact cutoff will need to be evaluated on a per dataset basis. CF will by the training design not be too dependent on the number of samples if trained sufficiently. We showed that performance is at least competitive with at least 50 samples. In summary, for highly varying features in a complex experimental setting with many differing samples, a holistic model trained with all features and potentially additional related samples can capture dependencies between features - such as the ones implemented in PIMMS.</p>
      <p id="Par27">In general, the modeling approaches here are restricted to the samples in a particular study and all models are fitted for each new dataset. However, transfer of models between datasets can be envisioned although a recent study suggested that this brings no benefits<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. The potential to fine-tune a model trained on one dataset to a new dataset for a fixed set of features is possible without further efforts for autoencoders. For CF one would need to find the closest training samples in the case where samples are separated strictly into train, validation and test set. However, feature embeddings could be transferred and extended easily. Therefore, all models could potentially be envisioned in a clinical setup, where models are re-trained with the latest samples. This could be implemented using similar cohorts, e.g. for the same tissue and similar patients, which is then the basis to build a database of tissue specific models - or by incorporating tissue embeddings as an additional source of information. The difficulty in achieving this will be a stable setup for comparable results without major batch effects due to sample handling or different instruments. How to approach and the potential for data integration from different setups is an unresolved issue. In general, community-curated benchmarks including datasets and detailed metrics should be discussed by creating one or more ProteoBench modules in a community effort<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>.</p>
      <p id="Par28">To ensure reproducibility and further extension we offer an evaluation workflow for simulated MCAR missing values of the entire data distribution and oversampling low abundant intensities (MNAR) instead of only reporting results on our specific datasets. Everything is available and continuously tested on GitHub, including the workflows, which allows for additional methods to be added to our comparisons. This includes comparisons on simulated missing values with varying degrees of MNAR which can be extended to further holistic models. The potential extensibility of the workflow allows for comparison of different ideas on different datasets, including the downstream analysis.</p>
      <p id="Par29">We evaluated imputation on different levels of proteomics features and found that lower-level data was easier to learn due to being less aggregated<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Therefore, it would be great to assess further if machine learning models can be trained on lower-level data as peptides are the most sensible unit and imputation on protein groups level performs one form of implicit imputation at the peptide level<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. One could assess if imputed features on lower-level data can be reaggregated to protein groups, e.g using ideas from Sticker and coworkers<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> or MSStats<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Additionally, the three self-supervised DL models could also be explored for denoising of samples, especially the generative VAE, or by adding diffusion models as they are trained by adding noise to the data<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>.</p>
      <p id="Par30">Finally, an interesting application will be single cell proteomics with hundreds of MS runs. This community in proteomics has not yet developed their own methods to our knowledge, but might not want to fall back to the ones established for discrete count-based single cell RNA data<sup><xref ref-type="bibr" rid="CR48">48</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup> for intensity based label-free quantified proteomics data without further testing. In conclusion we suggest that holistic models such as the ones implemented in PIMMS can improve imputation for proteomics and that our evaluation workflow allows further experimentation leading to more robust imputation.</p>
    </sec>
    <sec id="Sec14">
      <title>Methods</title>
      <sec id="Sec15">
        <title>Description of the HeLa proteomics dataset</title>
        <p id="Par31">The HeLa cell lines were repeatedly measured as maintenance (MNT) and quality control (QC) of the mass spectrometers at Novo Nordisk Foundation Center for Protein Research (NNF CPR) and Max Planck Institute of Biochemistry. The samples were run as QC samples during the measurement of cohorts or as MNT samples after instrument cleaning and calibration using different column lengths and liquid chromatography methods. The cells were lysed by different protocols, which are expected to include digestion using trypsin, but on a per sample basis the exact protocol was not annotated<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. The injection volume ranges from one to seven microliter.</p>
        <p id="Par32">Therefore, our dataset contains repeated measures of similar underlying biological samples acquired using DDA label-free quantification and can be used to explore general questions of applicability of self-supervised learning to proteomics data.</p>
      </sec>
      <sec id="Sec16">
        <title>Description of raw file processing of HeLa proteomics dataset</title>
        <p id="Par33">We used 564 raw files of quality and maintenance runs of HeLa cell lines from a larger set of 7444 quality control and maintenance runs. We processed all of these in a Snakemake<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> workflow as single runs in MaxQuant 1.6.12<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> yielding single abundances for precursor, aggregated peptide and protein group intensities using LFQ. As FASTA file the UNIPROT human reference proteome database 2019_05 release, containing 20,950 canonical and 75,468 additional sequences, was used for the DDA analysis. Contaminants were controlled using the default contaminants fasta shipped with MaxQuant. From the MaxQuant summary folder we then used the <italic>evidence.txt</italic> for precursor quantifications, <italic>peptides.txt</italic> for aggregated peptides and <italic>proteinGroups.txt</italic> for protein groups. The full dataset and detailed pre-processing steps are explained in a Data Descriptor<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
      </sec>
      <sec id="Sec17">
        <title>Feature selection strategy for quantified runs in general comparison workflow</title>
        <p id="Par34">We applied a two-step procedure for feature and sample selection (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). We used a cutoff of 25% feature prevalence across samples to be included into the workflow. Samples were then filtered in a second step by their completeness of the selected features. To be included a sample had to have 50% of the selected features. In order to create train, validation and test splits, a dataset was split in the long-data view, where a row consists of a sample name, feature name and its quantification. We divided 90% of the data into training data, 5% into validation and 5% into the test split, including per default 75% MCAR and 25% MNAR simulated missing values. This ensured that the validation and test data were representative of the entire data, while enough low abundant intensities were available for evaluation of features from the lower range of intensities. The validation cohort was only used for early stopping and the performance on the validation and test data was therefore expected to be similar. On a few hundred sample datasets, the number of sampled quantifications for both validation and test split is quickly in the order of hundred thousand for protein groups and several hundred thousands for peptide-related measurements.</p>
      </sec>
      <sec id="Sec18">
        <title>GALA-ALD dataset</title>
        <p id="Par35">The clinical data consisted of a cohort of patients with liver disease<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. 457 plasma samples were measured in data-independent acquisition (DIA) and processed using Spectronaut v.15.4<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> with the libraries as described in detail by Liu and coworkers<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Peptide quantification was extracted from “PEP.Quantity” - representing the stripped peptide sequence. Data for downstream analysis was selected with the same two-step procedure as described for the HeLa data. 3048 aggregated peptides were available in at least 25% of the samples of a total of 4345 aggregated peptides being present at least once. Protein group quantifications were extracted from “PG.Quantity”, dropping filtered-out values. 377 protein groups were available in at least 25% of the samples of a total of 506 protein groups being present at least once. We used a fibrosis marker (kleiner<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> score ranging from zero to four, <italic>N</italic> = 358) to compare the effects of different imputation methods. In the original ALD study the features were further selected based on QC samples where a maximum coefficient of variation of 0.4 on the non log transformed quantification per feature was used as cutoff for inclusion. This step was omitted in the comparison with the original study results in Fig. <xref rid="Fig4" ref-type="fig">4</xref> as we wanted to have a standardized workflow applicable also to approaches without interspersed QC samples. In numbers this means that we retained 313 protein groups instead of 277 omitting the selection criteria on QC samples. For the differential abundance analysis we had 348 complete clinical samples with both the kleiner score and the clinical control measurements we used.</p>
      </sec>
      <sec id="Sec19">
        <title>Self-supervised DL models</title>
        <p id="Par36">All models used self supervision as their setup, i.e. the data itself is used as a target in a prediction task. CF builds on the idea to combine a sample representation with a feature representation to a target value of interest<sup><xref ref-type="bibr" rid="CR54">54</xref>–<xref ref-type="bibr" rid="CR56">56</xref></sup>. The simplest implementation is to combine embedding vectors of equal length using their scalar product to the desired outcome, here the log intensity value assigned by a proteomics data analysis program. The approach is flexible to the total number of samples and features, and the model was trained only on the non-missing features. The loss function is the mean squared error.</p>
        <p id="Par37">A DAE is at inference time a plain autoencoder. During training its input values were partly masked and needed to be reconstructed. For each mini-batch the error was used to update the model so that the model learned better to reconstruct the data<sup><xref ref-type="bibr" rid="CR57">57</xref>,<xref ref-type="bibr" rid="CR58">58</xref></sup>. The loss was the squared error:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{{{{{\rm{reconstruction}}}}}}}={\sum }_{i}^{{N}_{B}} \, {\sum }_{f}^{{F}_{i}}{\left({I}_{f,i}^{{{{{{\rm{pred}}}}}}}-{I}_{f,i}^{{{{{{\rm{obs}}}}}}}\right)}^{2}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">reconstruction</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mspace width="0.25em"/><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pred</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">obs</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><graphic xlink:href="41467_2024_48711_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{B}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq1.gif"/></alternatives></inline-formula> is the number of samples in a batch <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B$$\end{document}</tex-math><mml:math id="M6"><mml:mi>B</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq2.gif"/></alternatives></inline-formula>, <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{i}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq3.gif"/></alternatives></inline-formula> is the number of features not missing in a sample <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M10"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${I}_{f,i}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq5.gif"/></alternatives></inline-formula> is the predicted and observed label-free quantification intensity value <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I$$\end{document}</tex-math><mml:math id="M14"><mml:mi>I</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq6.gif"/></alternatives></inline-formula> of feature <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f$$\end{document}</tex-math><mml:math id="M16"><mml:mi>f</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq7.gif"/></alternatives></inline-formula> in sample <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M18"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq8.gif"/></alternatives></inline-formula>. Missing features in a sample, which were not missing due to the training procedure of masking intensity values, were not used to calculate the loss. VAE introduces a different objective and models the latent space explicitly, here and as most often done as a standard normal distribution<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR60">60</xref></sup>. The latent space of a VAE has two components that are used for the first part of the loss function, the regularization loss:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{{{{{\rm{regularization}}}}}}}={\sum }_{i}^{{I}_{B}}{\sum }_{l}^{L}\max \left(0,0.5*\left\{{\mu }_{l,i}^{z}+{e}^{{\upsilon }_{l,i}^{z}}-1-{\upsilon }_{l,i}^{z}\right\}\right)$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">regularization</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mi>max</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>*</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2024_48711_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mu }_{l,i}^{z}$$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq9.gif"/></alternatives></inline-formula> is the mean and <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\upsilon }_{l,i}^{z}$$\end{document}</tex-math><mml:math id="M24"><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq10.gif"/></alternatives></inline-formula> the log variance of dimension <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M26"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq11.gif"/></alternatives></inline-formula> and sample <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M28"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq12.gif"/></alternatives></inline-formula> of the isotropic multivariate Gaussian with <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L$$\end{document}</tex-math><mml:math id="M30"><mml:mi>L</mml:mi></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq13.gif"/></alternatives></inline-formula> dimensions of the encoder output, i.e. the latent representation z. The reconstruction loss was based assuming a normal distribution for the decoder as output<sup><xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR61">61</xref></sup>, leading to<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{{{{{\rm{reconstruction}}}}}}}={\sum }_{i}^{{N}_{B}}{\sum }_{f}^{{F}_{i}}0.5\left\{{{{{{\mathrm{ln}}}}}}(2\pi )+\left[{\left({{{{{\rm{I}}}}}}_{f,i}^{{{{{{\rm{obs}}}}}}}-{\mu }_{f,i}^{I}\right)}^{2}\right]{\cdot e}^{-{\upsilon }_{f,i}^{I}}+{\upsilon }_{f,i}^{I}\right\}$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">reconstruction</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mn>0.5</mml:mn><mml:mfenced close="}" open="{"><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">obs</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mo>⋅</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2024_48711_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq14"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{B}$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq14.gif"/></alternatives></inline-formula>, <inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{i}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${I}_{f,i}^{{obs}}$$\end{document}</tex-math><mml:math id="M38"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq16.gif"/></alternatives></inline-formula> are as before and <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mu }_{f,i}^{I}$$\end{document}</tex-math><mml:math id="M40"><mml:msubsup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq17.gif"/></alternatives></inline-formula> and log variance of <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\upsilon }_{f,i}^{I}$$\end{document}</tex-math><mml:math id="M42"><mml:msubsup><mml:mrow><mml:mi>υ</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2024_48711_Article_IEq18.gif"/></alternatives></inline-formula> are the parameters of the isotropic multivariate Gaussian distribution of the decoder outputs, i.e. of the modeled feature distribution. Training of the VAE was augmented by masking input values as in the denoising autoencoder<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, although this is not strictly necessary due to the stochastic nature of the latent space. For inference, missing values are predicted using both the mean of the encoder and decoder output. The models were developed using a variety of software including numpy (v.1.20)<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>, pandas (v.1.4.)<sup><xref ref-type="bibr" rid="CR64">64</xref>,<xref ref-type="bibr" rid="CR65">65</xref></sup>, pytorch (v.1.10)<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> and fastai (v.2.5)<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>.</p>
      </sec>
      <sec id="Sec20">
        <title>Other imputation approaches</title>
        <p id="Par38">We used other methods which were available either in scikit-learn or R<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup>. R or bioconductor packages were e1071, impute<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, SeqKnn<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>, pcaMethods<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, norm, imputeLCMD<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, VIM<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, rrconNA<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>, mice<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>, missForest<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, GSimp<sup><xref ref-type="bibr" rid="CR73">73</xref></sup> and msImpute<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>), which were included in a previous comparison<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> except the last one. We did not include non-general imputation methods as e.g. provided by MSstats<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> or without reusable software<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>. We used KNN interpolation of replicates based on the HeLa cell line measurements being repeated over time. The only parameter to set was how many neighboring samples should be used as replicates. We used three replicates for the scikit-learn<sup><xref ref-type="bibr" rid="CR75">75</xref></sup> based implementation as this was found to be the best setting by Poulous and coauthors<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, which is also the most widely encountered replication number used in the field and set as default of both packages we used. However, the R based implementation of KNN (SEQKNN, TRKNN, KNNIMPUTE) used ten neighbors in NAguideR<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> and we kept the default. We also used a simple median calculation for each feature across samples. This requires estimating one parameter per feature. For features that did not vary a lot, this strategy should yield robust estimates for missing values. We used a random forest implementation using missForest<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. The implementation works well for datasets on the protein group level, but fails for datasets on the peptide and precursor level as these are roughly ten times higher dimensional. We also included methods which assume MNAR missing values, such as the random shifted normal (RSN) distribution for imputation or QRLIC<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. RSN has as parameters a global mean shift and scaling factor for standard deviation, as well as a mean and standard deviation for each unit of interest, i.e. all quantified features of a sample or for a feature all quantification of that feature across samples. Note, that RSN and other MNAR focused methods assume that measurements are not present as they are below the limit of detection (LOD). Therefore, in our default setup we sample 25 percent of the simulated missing values as MNAR into the validation and test data splits to represent the lower range of intensity values (Supplementary Fig. <xref rid="MOESM1" ref-type="media">9</xref>), aiming to make the methods more comparable. All methods are available through our workflow and at least one method per included R package is tested via a GitHub action. Some methods and packages did however not work being called as in NAguideR for some of our data.</p>
      </sec>
      <sec id="Sec21">
        <title>Hyperparameter search using simulated missing values</title>
        <p id="Par39">In order to find good configurations for the self-supervised models a grid search was performed on three data levels on the development dataset. We sampled simulated missing values completely at random from the dataset, i.e. 5% for validation and 5% for testing. The training procedure and architecture of models was refined using the validation data. The performance of the best performing models on the validation data were then reported using the test data. We found that test performance metrics matched validation metrics up to the second decimal and that many model configurations yielded similar results. Performance was compared between the three self-supervised models to improve performance during model development. In this work all results were reported based on the simulated missing values in the test data split. Different latent representation dimension, namely 10, 25, 50, 75 and 100 dimensions were connected to a varying dimension and composition of hidden layers with a leaky rectified linear activation: (256), (512), (1024), (2056), (128, 64), (256, 128), (512, 256), (512, 512), (512, 256, 128), (1024, 512, 256), (128, 128, 128), (256, 256, 128, 128) - for the encoder and inverted for the decoder. The total number of parameters using these combinations ranged from a couple of ten-thousands in the case of the CF models to tens of millions for the autoencoder architectures. We picked the smallest model in terms of parameters of the top 3 performing ones as their performance was nearly equal on the validation data split. Then we retrained the best models with a share of 25 percent MNAR simulated missing values. Besides the best models on all simulated missing values, we reported results using other plots. The intensities in a split were binned by the median of the feature, e.g. protein group, they originated from based on the training data split. The MAE per bin was then reported (Fig. <xref rid="Fig2" ref-type="fig">2e, f</xref>), which allows for selection of the best models in the intensity range of interest. This is accompanied by a plot showing the proportion of missing values of a feature based on its median value over samples (Fig. <xref rid="Fig2" ref-type="fig">2c, d</xref>). The correlation plots were based on Pearson correlation of predicted intensities and their original values in the test split. The Pearson correlation was calculated for a feature across all predictions of all samples, denoted “per feature correlation”, or for all predictions within one sample, denoted “per sample correlation” (Supplementary Fig. <xref rid="MOESM1" ref-type="media">6</xref>).</p>
      </sec>
      <sec id="Sec22">
        <title>Evaluation, imputation and differential expression in GALA-ALD dataset</title>
        <p id="Par40">We used the same splitting approach of the data as for the development dataset for evaluation with a share of 25 percent MNAR. We evaluated using a dimension of ten for CF’s sample and feature embeddings, and the DAE and VAE latent spaces. The autoencoders were composed of one hidden layer with 64 neurons both for the encoder and decoder, leading to a total number of parameters between 9,174 and 74,462 for the three models. The RSN imputation for the missing values in the original ALD study was done on a per sample basis, i.e. mean and standard deviation for each sample. Using the two-step procedure with defaults as in the original study, this yielded 313 protein groups for comparison (see ALD data description). Using a filtering of 25% for feature prevalence prior to imputation with the VAE (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>) we increased the share of missing values to 14% for the selected 377 protein groups in comparison to roughly 5% for the 313 features using the selection approach as in the original study<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. The differential analysis was done using an analysis of covariance (ANCOVA) procedure using statsmodels (v.0.12) and pingouin (v.0.5)<sup><xref ref-type="bibr" rid="CR76">76</xref>,<xref ref-type="bibr" rid="CR77">77</xref></sup>. We used a linear regression with the original kleiner score<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> as the stratification variable of interest for the patient’s cirrhosis disease stage to predict protein quantifications, controlling for covariates. Therefore, effects for each protein group were based on an ANCOVA controlling for age, BMI, gender, steatosis, and abstinence from alcohol as well as correcting for multiple testing as done in the original study. The multiple comparison corrections (q-values) were based on 313 protein groups in the original data imputed using RSN, and on 377 protein groups retained here. Correction for multiple testing correction was done using Benjamini-Hochberg’s correction<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>. The q-values of each DA were then compared for the overlapping 313 protein groups (Supplementary Data <xref rid="MOESM13" ref-type="media">10</xref>–<xref rid="MOESM15" ref-type="media">12</xref>).</p>
      </sec>
      <sec id="Sec23">
        <title>Machine learning in GALA-ALD dataset</title>
        <p id="Par41">In order to assess the predictive performance of newly retained features, we evaluated a logistic regression using different feature sets for the binary target of a fibrosis score greater than one (F2 endpoint in the original study, False: kleiner &lt;2, True, kleiner ≥2). The feature sets were: First, the features retained using the selection approach with settings as in the ALD study; second, all features available when using PIMMS selection approach; third, and the difference between both feature sets termed “new feat”. We used maximum relevance, minimum redundancy using the F-test based implementation, in detail the F-test correlation quotient (FCQ)<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR79">79</xref></sup> to select a set of features to be used in the logistic regression. Using cross validation we selected the best set of up to 15 features for each of the three sub datasets. Then, the model was retrained on a final 80-20 percent training-testing data split of samples for each subdataset. Areas under the curve (AUC) for the receiver operation (ROC) and precision recall (PRC) curves were compared between these three sub datasets. The shown graphs and reported metrics were calculated on the test split<sup><xref ref-type="bibr" rid="CR75">75</xref>,<xref ref-type="bibr" rid="CR76">76</xref></sup>.</p>
      </sec>
      <sec id="Sec24">
        <title>Reporting summary</title>
        <p id="Par42">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
      </sec>
    </sec>
    <sec sec-type="supplementary-material">
      <sec id="Sec25">
        <title>Supplementary information</title>
        <p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2024_48711_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41467_2024_48711_MOESM2_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41467_2024_48711_MOESM3_ESM.pdf"><caption><p>Peer Review File</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="41467_2024_48711_MOESM4_ESM.xlsx"><caption><p>Supplementary Data 1</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="41467_2024_48711_MOESM5_ESM.xlsx"><caption><p>Supplementary Data 2</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM6"><media xlink:href="41467_2024_48711_MOESM6_ESM.xlsx"><caption><p>Supplementary Data 3</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM7"><media xlink:href="41467_2024_48711_MOESM7_ESM.xlsx"><caption><p>Supplementary Data 4</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM8"><media xlink:href="41467_2024_48711_MOESM8_ESM.xlsx"><caption><p>Supplementary Data 5</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM9"><media xlink:href="41467_2024_48711_MOESM9_ESM.xlsx"><caption><p>Supplementary Data 6</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM10"><media xlink:href="41467_2024_48711_MOESM10_ESM.xlsx"><caption><p>Supplementary Data 7</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM11"><media xlink:href="41467_2024_48711_MOESM11_ESM.xlsx"><caption><p>Supplementary Data 8</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM12"><media xlink:href="41467_2024_48711_MOESM12_ESM.xlsx"><caption><p>Supplementary Data 9</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM13"><media xlink:href="41467_2024_48711_MOESM13_ESM.xlsx"><caption><p>Supplementary Data 10</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM14"><media xlink:href="41467_2024_48711_MOESM14_ESM.xlsx"><caption><p>Supplementary Data 11</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM15"><media xlink:href="41467_2024_48711_MOESM15_ESM.xlsx"><caption><p>Supplementary Data 12</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM16"><media xlink:href="41467_2024_48711_MOESM16_ESM.xlsx"><caption><p>Supplementary Data 13</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM17"><media xlink:href="41467_2024_48711_MOESM17_ESM.xlsx"><caption><p>Supplementary Data 14</p></caption></media></supplementary-material>
</p>
      </sec>
      <sec id="Sec26">
        <title>Source data</title>
        <p>
<supplementary-material content-type="local-data" id="MOESM18"><media xlink:href="41467_2024_48711_MOESM18_ESM.xlsx"><caption><p>Source Data</p></caption></media></supplementary-material>
</p>
      </sec>
    </sec>
  </body>
  <back>
    <fn-group>
      <fn>
        <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
      </fn>
    </fn-group>
    <sec>
      <title>Supplementary information</title>
      <p>The online version contains supplementary material available at 10.1038/s41467-024-48711-5.</p>
    </sec>
    <ack>
      <title>Acknowledgements</title>
      <p>We would like to acknowledge Rebeca Quinones, Mario Oroshi and John Damm Sørensen for their huge effort to retrieve all quality and maintenance HeLa raw files for the creation of our development dataset. Furthermore, we would like to thank Jeppe Madsen and Martin Rykær for their time and expertise. Lastly, we would like to thank the proteomics groups that have kindly shared their maintenance and QC HeLa files, both at Novo Nordisk Foundation Center for Protein Research and at the Proteomics and Signal Transduction department of the MaxPlanck institute of biochemistry. H.W. was supported by the Novo Nordisk Foundation (grant NNF19SA0035440). H.W., A.B.N., L.N., M.L-P., M.M, L.J.J. and S.R. were supported by the Novo Nordisk Foundation grant NNF14CC0001 and S.R. from the Novo Nordisk Foundation grants NNF21SA0072102 and NNF23SA0084103.</p>
    </ack>
    <notes notes-type="author-contribution">
      <title>Author contributions</title>
      <p>S.R. and A.B.N. initiated the study and guided the analysis. M.M. and A.B.N. led the efforts in collecting the data. H.W. assembled the data, performed the analyses and wrote the software choosing the modeling approaches and refining the idea. M.L.P, L.N. and L.J.J. provided guidance and input for the analysis. H.W. and S.R. wrote the manuscript with contributions from all coauthors. All authors read and approved the final version of the manuscript.</p>
    </notes>
    <notes notes-type="peer-review">
      <title>Peer review</title>
      <sec id="FPar1">
        <title>Peer review information</title>
        <p id="Par43"><italic>Nature Communications</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. A peer review file is available.</p>
      </sec>
    </notes>
    <notes notes-type="data-availability">
      <title>Data availability</title>
      <p>The mass spectrometry proteomics data have been deposited to the ProteomeXchange Consortium via the PRIDE<sup><xref ref-type="bibr" rid="CR80">80</xref></sup> partner repository with the dataset identifier <ext-link ext-link-type="uri" xlink:href="http://proteomecentral.proteomexchange.org/cgi/GetDataset?ID=PXD042233">PXD042233</ext-link>. A manuscript<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> describing these data has been published in Scientific Data and is available at 10.1038/s41597-024-02922-z. The clinical data is not freely available, but can be requested as specified by Niu et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>: “The full proteomics datasets and histologic scoring generated and/or analyzed (…) are available (…) upon request, to Odense Patient Data Exploratory Network (open@rsyd.dk) with reference to project ID OP_040. Permission to access and analyze data can be obtained following approval from the Danish Data Protection Agency and the ethics committee for the Region of Southern Denmark.” Source data for the main figures are provided with this paper. <xref ref-type="sec" rid="Sec26">Source data</xref> are provided with this paper.</p>
    </notes>
    <notes notes-type="data-availability">
      <title>Code availability</title>
      <p>The PIMMS package and all analysis scripts are available on PyPI and at github.com/RasmussenLab/pimms<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>. The differential analysis and machine learning procedure used on the ALD data is available on PyPI and GitHub at github.com/RasmussenLab/njab.</p>
    </notes>
    <notes id="FPar2" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par44">The authors declare no competing interests.</p>
    </notes>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aebersold</surname><given-names>R</given-names></name><name><surname>Mann</surname><given-names>M</given-names></name></person-group><article-title>Mass-spectrometric exploration of proteome structure and function</article-title><source>Nature</source><year>2016</year><volume>537</volume><fpage>347</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature19949</pub-id><pub-id pub-id-type="pmid">27629641</pub-id>
</element-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niu</surname><given-names>L</given-names></name><etal/></person-group><article-title>Noninvasive proteomic biomarkers for alcohol-related liver disease</article-title><source>Nat. Med.</source><year>2022</year><volume>28</volume><fpage>1277</fpage><lpage>1287</lpage><pub-id pub-id-type="doi">10.1038/s41591-022-01850-y</pub-id><pub-id pub-id-type="pmid">35654907</pub-id>
</element-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francavilla</surname><given-names>C</given-names></name><etal/></person-group><article-title>Phosphoproteomics of primary cells reveals druggable kinase signatures in ovarian cancer</article-title><source>Cell Rep.</source><year>2017</year><volume>18</volume><fpage>3242</fpage><lpage>3256</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.03.015</pub-id><pub-id pub-id-type="pmid">28355574</pub-id>
</element-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bader</surname><given-names>JM</given-names></name><etal/></person-group><article-title>Proteome profiling in cerebrospinal fluid reveals novel biomarkers of Alzheimer’s disease</article-title><source>Mol. Syst. Biol.</source><year>2020</year><volume>16</volume><fpage>e9356</fpage><pub-id pub-id-type="doi">10.15252/msb.20199356</pub-id><pub-id pub-id-type="pmid">32485097</pub-id>
</element-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoof</surname><given-names>EM</given-names></name><etal/></person-group><article-title>Quantitative single-cell proteomics as a tool to characterize cellular hierarchies</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>3341</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-23667-y</pub-id><pub-id pub-id-type="pmid">34099695</pub-id>
</element-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunner</surname><given-names>A-D</given-names></name><etal/></person-group><article-title>Ultra-high sensitivity mass spectrometry quantifies single-cell proteome changes upon perturbation</article-title><source>Mol. Syst. Biol.</source><year>2022</year><volume>18</volume><fpage>e10798</fpage><pub-id pub-id-type="doi">10.15252/msb.202110798</pub-id><pub-id pub-id-type="pmid">35226415</pub-id>
</element-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mund</surname><given-names>A</given-names></name><etal/></person-group><article-title>Deep Visual Proteomics defines single-cell identity and heterogeneity</article-title><source>Nat. Biotechnol.</source><year>2022</year><volume>40</volume><fpage>1231</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1038/s41587-022-01302-5</pub-id><pub-id pub-id-type="pmid">35590073</pub-id>
</element-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazar</surname><given-names>C</given-names></name><name><surname>Gatto</surname><given-names>L</given-names></name><name><surname>Ferro</surname><given-names>M</given-names></name><name><surname>Bruley</surname><given-names>C</given-names></name><name><surname>Burger</surname><given-names>T</given-names></name></person-group><article-title>Accounting for the multiple natures of missing values in label-free quantitative proteomics data sets to compare imputation strategies</article-title><source>J. Proteome Res.</source><year>2016</year><volume>15</volume><fpage>1116</fpage><lpage>1125</lpage><pub-id pub-id-type="doi">10.1021/acs.jproteome.5b00981</pub-id><pub-id pub-id-type="pmid">26906401</pub-id>
</element-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>F</given-names></name><etal/></person-group><article-title>Parallel accumulation-serial fragmentation (PASEF): Multiplying sequencing speed and sensitivity by synchronized scans in a trapped ion mobility device</article-title><source>J. Proteome Res.</source><year>2015</year><volume>14</volume><fpage>5378</fpage><lpage>5387</lpage><pub-id pub-id-type="doi">10.1021/acs.jproteome.5b00932</pub-id><pub-id pub-id-type="pmid">26538118</pub-id>
</element-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>F</given-names></name><name><surname>Geyer</surname><given-names>PE</given-names></name><name><surname>Virreira Winter</surname><given-names>S</given-names></name><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Mann</surname><given-names>M</given-names></name></person-group><article-title>BoxCar acquisition method enables single-shot proteomics at a depth of 10,000 proteins in 100 minutes</article-title><source>Nat. Methods</source><year>2018</year><volume>15</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0003-5</pub-id><pub-id pub-id-type="pmid">29735998</pub-id>
</element-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meier</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>MA</given-names>
            </name>
            <name>
              <surname>Mann</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Trapped ion mobility spectrometry and parallel accumulation-serial fragmentation in proteomics</article-title>
          <source>Mol. Cell. Proteom.</source>
          <year>2021</year>
          <volume>20</volume>
          <fpage>100138</fpage>
          <pub-id pub-id-type="doi">10.1016/j.mcpro.2021.100138</pub-id>
        </element-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demichev</surname><given-names>V</given-names></name><name><surname>Messner</surname><given-names>CB</given-names></name><name><surname>Vernardis</surname><given-names>SI</given-names></name><name><surname>Lilley</surname><given-names>KS</given-names></name><name><surname>Ralser</surname><given-names>M</given-names></name></person-group><article-title>DIA-NN: neural networks and interference correction enable deep proteome coverage in high throughput</article-title><source>Nat. Methods</source><year>2020</year><volume>17</volume><fpage>41</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0638-x</pub-id><pub-id pub-id-type="pmid">31768060</pub-id>
</element-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb-Robertson</surname><given-names>B-JM</given-names></name><etal/></person-group><article-title>Review, evaluation, and discussion of the challenges of missing value imputation for mass spectrometry-based label-free global proteomics</article-title><source>J. Proteome Res.</source><year>2015</year><volume>14</volume><fpage>1993</fpage><lpage>2001</lpage><pub-id pub-id-type="doi">10.1021/pr501138h</pub-id><pub-id pub-id-type="pmid">25855118</pub-id>
</element-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><etal/></person-group><article-title>NAguideR: performing and prioritizing missing value imputations for consistent bottom-up proteomic analyses</article-title><source>Nucleic Acids Res.</source><year>2020</year><volume>48</volume><fpage>e83</fpage><pub-id pub-id-type="doi">10.1093/nar/gkaa498</pub-id><pub-id pub-id-type="pmid">32526036</pub-id>
</element-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Berg</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>McConnell</surname>
              <given-names>EW</given-names>
            </name>
            <name>
              <surname>Hicks</surname>
              <given-names>LM</given-names>
            </name>
            <name>
              <surname>Popescu</surname>
              <given-names>SC</given-names>
            </name>
            <name>
              <surname>Popescu</surname>
              <given-names>GV</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of linear models and missing value imputation for the analysis of peptide-centric proteomics</article-title>
          <source>BMC Bioinforma.</source>
          <year>2019</year>
          <volume>20</volume>
          <fpage>102</fpage>
          <pub-id pub-id-type="doi">10.1186/s12859-019-2619-6</pub-id>
        </element-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Välikangas</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Suomi</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Elo</surname>
              <given-names>LL</given-names>
            </name>
          </person-group>
          <article-title>A comprehensive evaluation of popular proteomics software workflows for label-free proteome quantification and imputation</article-title>
          <source>Brief. Bioinform.</source>
          <year>2017</year>
          <volume>19</volume>
          <fpage>1344</fpage>
          <lpage>1355</lpage>
        </element-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Čuklina</surname><given-names>J</given-names></name><etal/></person-group><article-title>Diagnostics and correction of batch effects in large-scale proteomic studies: A tutorial</article-title><source>Mol. Syst. Biol.</source><year>2021</year><volume>17</volume><fpage>e10240</fpage><pub-id pub-id-type="doi">10.15252/msb.202110240</pub-id><pub-id pub-id-type="pmid">34432947</pub-id>
</element-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <mixed-citation publication-type="other">Liu, M. &amp; Dongre, A. Proper imputation of missing values in proteomics datasets for differential expression analysis. <italic>Brief. Bioinform</italic>. <bold>22</bold>, bbaa112 (2021).</mixed-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <mixed-citation publication-type="other">Harris, L., Fondrie, W. E., Oh, S. &amp; Noble, W. S. Evaluating Proteomics Imputation Methods with Improved Criteria. <italic>J. Proteome Res.</italic>10.1021/acs.jproteome.3c00205 (2023).</mixed-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wieczorek</surname><given-names>S</given-names></name><etal/></person-group><article-title>DAPAR &amp; ProStaR: software to perform statistical analyses in quantitative discovery proteomics</article-title><source>Bioinformatics</source><year>2017</year><volume>33</volume><fpage>135</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btw580</pub-id><pub-id pub-id-type="pmid">27605098</pub-id>
</element-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <mixed-citation publication-type="other">Lazar, C. imputeLCMD: a collection of methods for left-censored missing data imputation. <italic>R package, version</italic><bold>2</bold>, (2015).</mixed-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <mixed-citation publication-type="other">Etourneau, L., Fancello, L., Wieczorek, S., Varoquaux, N. &amp; Burger, T. A new take on missing value imputation for bottom-up label-free LC-MS/MS proteomics. <italic>bioRxiv</italic> 2023.11.09.566355. 10.1101/2023.11.09.566355 (2023).</mixed-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwämmle</surname>
              <given-names>V</given-names>
            </name>
            <name>
              <surname>Hagensen</surname>
              <given-names>CE</given-names>
            </name>
            <name>
              <surname>Rogowska-Wrzesinska</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Jensen</surname>
              <given-names>ON</given-names>
            </name>
          </person-group>
          <article-title>PolySTest: Robust statistical testing of proteomics data with missing values improves detection of biologically relevant features</article-title>
          <source>Mol. Cell. Proteom.</source>
          <year>2020</year>
          <volume>19</volume>
          <fpage>1396</fpage>
          <lpage>1408</lpage>
          <pub-id pub-id-type="doi">10.1074/mcp.RA119.001777</pub-id>
        </element-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyanova</surname><given-names>S</given-names></name><name><surname>Temu</surname><given-names>T</given-names></name><name><surname>Cox</surname><given-names>J</given-names></name></person-group><article-title>The MaxQuant computational platform for mass spectrometry-based shotgun proteomics</article-title><source>Nat. Protoc.</source><year>2016</year><volume>11</volume><fpage>2301</fpage><lpage>2319</lpage><pub-id pub-id-type="doi">10.1038/nprot.2016.136</pub-id><pub-id pub-id-type="pmid">27809316</pub-id>
</element-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouyssié</surname><given-names>D</given-names></name><etal/></person-group><article-title>Proline: an efficient and user-friendly software suite for large-scale proteomics</article-title><source>Bioinformatics</source><year>2020</year><volume>36</volume><fpage>3148</fpage><lpage>3155</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa118</pub-id><pub-id pub-id-type="pmid">32096818</pub-id>
</element-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poulos</surname><given-names>RC</given-names></name><etal/></person-group><article-title>Strategies to enable large-scale proteomics for reproducible research</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>3793</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-17641-3</pub-id><pub-id pub-id-type="pmid">32732981</pub-id>
</element-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nissen</surname><given-names>JN</given-names></name><etal/></person-group><article-title>Improved metagenome binning and assembly using deep variational autoencoders</article-title><source>Nat. Biotechnol.</source><year>2021</year><volume>39</volume><fpage>555</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1038/s41587-020-00777-4</pub-id><pub-id pub-id-type="pmid">33398153</pub-id>
</element-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <mixed-citation publication-type="other">Frazer, J. et al. Disease variant prediction with deep generative models of evolutionary data. <italic>Nature</italic> 1–5. 10.1038/s41586-021-04043-8 (2021).</mixed-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <mixed-citation publication-type="other">Buergel, T. et al. Metabolomic profiles predict individual multidisease outcomes. <italic>Nat. Med</italic>. 10.1038/s41591-022-01980-3 (2022).</mixed-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>M</given-names></name><name><surname>Kumar</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>W-F</given-names></name><name><surname>Strauss</surname><given-names>MT</given-names></name></person-group><article-title>Artificial intelligence for proteomics and biomarker discovery</article-title><source>Cell Syst.</source><year>2021</year><volume>12</volume><fpage>759</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2021.06.006</pub-id><pub-id pub-id-type="pmid">34411543</pub-id>
</element-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwmeester</surname><given-names>R</given-names></name><name><surname>Gabriels</surname><given-names>R</given-names></name><name><surname>Van Den Bossche</surname><given-names>T</given-names></name><name><surname>Martens</surname><given-names>L</given-names></name><name><surname>Degroeve</surname><given-names>S</given-names></name></person-group><article-title>The age of data-driven proteomics: How machine learning enables novel workflows</article-title><source>Proteomics</source><year>2020</year><volume>20</volume><fpage>e1900351</fpage><pub-id pub-id-type="doi">10.1002/pmic.201900351</pub-id><pub-id pub-id-type="pmid">32267083</pub-id>
</element-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>B</given-names></name><etal/></person-group><article-title>Deep learning in proteomics</article-title><source>Proteomics</source><year>2020</year><volume>20</volume><fpage>e1900335</fpage><pub-id pub-id-type="doi">10.1002/pmic.201900335</pub-id><pub-id pub-id-type="pmid">32939979</pub-id>
</element-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwmeester</surname><given-names>R</given-names></name><name><surname>Gabriels</surname><given-names>R</given-names></name><name><surname>Hulstaert</surname><given-names>N</given-names></name><name><surname>Martens</surname><given-names>L</given-names></name><name><surname>Degroeve</surname><given-names>S</given-names></name></person-group><article-title>DeepLC can predict retention times for peptides that carry as-yet unseen modifications</article-title><source>Nat. Methods</source><year>2021</year><volume>18</volume><fpage>1363</fpage><lpage>1369</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01301-5</pub-id><pub-id pub-id-type="pmid">34711972</pub-id>
</element-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilhelm</surname><given-names>M</given-names></name><etal/></person-group><article-title>Deep learning boosts sensitivity of mass spectrometry-based immunopeptidomics</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>3346</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-23713-9</pub-id><pub-id pub-id-type="pmid">34099720</pub-id>
</element-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webel</surname><given-names>H</given-names></name><name><surname>Perez-Riverol</surname><given-names>Y</given-names></name><name><surname>Nielsen</surname><given-names>AB</given-names></name><name><surname>Rasmussen</surname><given-names>S</given-names></name></person-group><article-title>Mass spectrometry-based proteomics data from thousands of HeLa control samples</article-title><source>Sci. Data</source><year>2024</year><volume>11</volume><fpage>112</fpage><pub-id pub-id-type="doi">10.1038/s41597-024-02922-z</pub-id><pub-id pub-id-type="pmid">38263211</pub-id>
</element-citation>
      </ref>
      <ref id="CR36">
        <label>36.</label>
        <mixed-citation publication-type="other">Trevor Hastie, Robert Tibshirani, Balasubramanian Narasimhan, Gilbert Chu. <italic>Impute</italic>. (Bioconductor, 2017). 10.18129/B9.BIOC.IMPUTE.</mixed-citation>
      </ref>
      <ref id="CR37">
        <label>37.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troyanskaya</surname><given-names>O</given-names></name><etal/></person-group><article-title>Missing value estimation methods for DNA microarrays</article-title><source>Bioinformatics</source><year>2001</year><volume>17</volume><fpage>520</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/17.6.520</pub-id><pub-id pub-id-type="pmid">11395428</pub-id>
</element-citation>
      </ref>
      <ref id="CR38">
        <label>38.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stekhoven</surname><given-names>DJ</given-names></name><name><surname>Bühlmann</surname><given-names>P</given-names></name></person-group><article-title>MissForest–non-parametric missing value imputation for mixed-type data</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>112</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btr597</pub-id><pub-id pub-id-type="pmid">22039212</pub-id>
</element-citation>
      </ref>
      <ref id="CR39">
        <label>39.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verboven</surname><given-names>S</given-names></name><name><surname>Branden</surname><given-names>KV</given-names></name><name><surname>Goos</surname><given-names>P</given-names></name></person-group><article-title>Sequential imputation for missing values</article-title><source>Comput. Biol. Chem.</source><year>2007</year><volume>31</volume><fpage>320</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/j.compbiolchem.2007.07.001</pub-id><pub-id pub-id-type="pmid">17920334</pub-id>
</element-citation>
      </ref>
      <ref id="CR40">
        <label>40.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oba</surname><given-names>S</given-names></name><etal/></person-group><article-title>A Bayesian missing value estimation method for gene expression profile data</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>2088</fpage><lpage>2096</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btg287</pub-id><pub-id pub-id-type="pmid">14594714</pub-id>
</element-citation>
      </ref>
      <ref id="CR41">
        <label>41.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pletscher-Frankild</surname><given-names>S</given-names></name><name><surname>Pallejà</surname><given-names>A</given-names></name><name><surname>Tsafou</surname><given-names>K</given-names></name><name><surname>Binder</surname><given-names>JX</given-names></name><name><surname>Jensen</surname><given-names>LJ</given-names></name></person-group><article-title>DISEASES: Text mining and data integration of disease-gene associations</article-title><source>Methods</source><year>2015</year><volume>74</volume><fpage>83</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.ymeth.2014.11.020</pub-id><pub-id pub-id-type="pmid">25484339</pub-id>
</element-citation>
      </ref>
      <ref id="CR42">
        <label>42.</label>
        <mixed-citation publication-type="other">Zhao, Z., Anand, R. &amp; Wang, M. Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform. <italic>arXiv [stat.ML]</italic>10.48550/arXiv.1908.05376 (2019)</mixed-citation>
      </ref>
      <ref id="CR43">
        <label>43.</label>
        <mixed-citation publication-type="other">Rehfeldt, T. G. et al. Variability analysis of LC-MS experimental factors and their impact on machine learning. <italic>Gigascience</italic><bold>12</bold>, giad096 (2023).</mixed-citation>
      </ref>
      <ref id="CR44">
        <label>44.</label>
        <mixed-citation publication-type="other">EuBIC. <italic>ProteoBench</italic>. (Github, 2024).</mixed-citation>
      </ref>
      <ref id="CR45">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sticker</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Goeminne</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Martens</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Clement</surname>
              <given-names>L</given-names>
            </name>
          </person-group>
          <article-title>Robust summarization and inference in proteome-wide label-free quantification</article-title>
          <source>Mol. Cell. Proteom.</source>
          <year>2020</year>
          <volume>19</volume>
          <fpage>1209</fpage>
          <lpage>1219</lpage>
          <pub-id pub-id-type="doi">10.1074/mcp.RA119.001624</pub-id>
        </element-citation>
      </ref>
      <ref id="CR46">
        <label>46.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohler</surname><given-names>D</given-names></name><etal/></person-group><article-title>MSstats version 4.0: Statistical analyses of quantitative mass spectrometry-based proteomic experiments with chromatography-based quantification at scale</article-title><source>J. Proteome Res.</source><year>2023</year><volume>22</volume><fpage>1466</fpage><lpage>1482</lpage><pub-id pub-id-type="doi">10.1021/acs.jproteome.2c00834</pub-id><pub-id pub-id-type="pmid">37018319</pub-id>
</element-citation>
      </ref>
      <ref id="CR47">
        <label>47.</label>
        <mixed-citation publication-type="other">Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &amp; Ommer, B. Proceedings of the IEEE/CVF Conference on Computer Vision &amp; Pattern Recognition (CVPR). <italic>arXiv [cs.CV]</italic> 10684–10695 10.48550/arXiv.2112.10752 (2022).</mixed-citation>
      </ref>
      <ref id="CR48">
        <label>48.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Dijk</surname><given-names>D</given-names></name><etal/></person-group><article-title>Recovering gene interactions from single-cell data using data diffusion</article-title><source>Cell</source><year>2018</year><volume>174</volume><fpage>716</fpage><lpage>729.e27</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.05.061</pub-id><pub-id pub-id-type="pmid">29961576</pub-id>
</element-citation>
      </ref>
      <ref id="CR49">
        <label>49.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>FA</given-names></name><name><surname>Angerer</surname><given-names>P</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>SCANPY: Large-scale single-cell gene expression data analysis</article-title><source>Genome Biol.</source><year>2018</year><volume>19</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1186/s13059-017-1382-0</pub-id><pub-id pub-id-type="pmid">29409532</pub-id>
</element-citation>
      </ref>
      <ref id="CR50">
        <label>50.</label>
        <mixed-citation publication-type="other">Webel, H. HeLa quality control sample preparation for MS-based proteomics. <italic>Protocol Exchange</italic>. 10.21203/rs.3.pex-2155/v1 (2023).</mixed-citation>
      </ref>
      <ref id="CR51">
        <label>51.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mölder</surname><given-names>F</given-names></name><etal/></person-group><article-title>Sustainable data analysis with Snakemake</article-title><source>F1000Res.</source><year>2021</year><volume>10</volume><fpage>33</fpage><pub-id pub-id-type="doi">10.12688/f1000research.29032.2</pub-id><pub-id pub-id-type="pmid">34035898</pub-id>
</element-citation>
      </ref>
      <ref id="CR52">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bruderer</surname>
              <given-names>R</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Extending the limits of quantitative proteome profiling with data-independent acquisition and application to acetaminophen-treated three-dimensional liver microtissues</article-title>
          <source>Mol. Cell. Proteom.</source>
          <year>2015</year>
          <volume>14</volume>
          <fpage>1400</fpage>
          <lpage>1410</lpage>
          <pub-id pub-id-type="doi">10.1074/mcp.M114.044305</pub-id>
        </element-citation>
      </ref>
      <ref id="CR53">
        <label>53.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>DE</given-names></name><etal/></person-group><article-title>Design and validation of a histological scoring system for nonalcoholic fatty liver disease</article-title><source>Hepatology</source><year>2005</year><volume>41</volume><fpage>1313</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1002/hep.20701</pub-id><pub-id pub-id-type="pmid">15915461</pub-id>
</element-citation>
      </ref>
      <ref id="CR54">
        <label>54.</label>
        <mixed-citation publication-type="other">He, X. et al. Neural Collaborative Filtering. <italic>arXiv</italic>. 10.1145/3038912.3052569 (2017).</mixed-citation>
      </ref>
      <ref id="CR55">
        <label>55.</label>
        <mixed-citation publication-type="other">Howard, J. &amp; Gugger, S. fastai: A Layered API for deep learning. <italic>Information</italic><bold>11</bold>, (2020).</mixed-citation>
      </ref>
      <ref id="CR56">
        <label>56.</label>
        <mixed-citation publication-type="other">Howard, J. &amp; Gugger, S. <italic>Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD</italic>. 582 (O’Reilly, 2020).</mixed-citation>
      </ref>
      <ref id="CR57">
        <label>57.</label>
        <mixed-citation publication-type="other">Vincent, P., Larochelle, H., Bengio, Y. &amp; Manzagol, P.-A. <italic>Extracting and Composing Robust Features with Denoising Autoencoders</italic>. (2008).</mixed-citation>
      </ref>
      <ref id="CR58">
        <label>58.</label>
        <mixed-citation publication-type="other">Ca, P. V., Edu, L. T., Lajoie, I., Ca, Y. B. &amp; Ca, P.-A. M. <italic>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</italic>. vol. 11 3371–3408. <ext-link ext-link-type="uri" xlink:href="http://jmlr.org/papers/v11/vincent10a.html">http://jmlr.org/papers/v11/vincent10a.html</ext-link> (2010).</mixed-citation>
      </ref>
      <ref id="CR59">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kingma</surname>
              <given-names>DP</given-names>
            </name>
            <name>
              <surname>Welling</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>An introduction to variational autoencoders</article-title>
          <source>Found. Trends® Mach. Learn.</source>
          <year>2019</year>
          <volume>12</volume>
          <fpage>307</fpage>
          <lpage>392</lpage>
          <pub-id pub-id-type="doi">10.1561/2200000056</pub-id>
        </element-citation>
      </ref>
      <ref id="CR60">
        <label>60.</label>
        <mixed-citation publication-type="other">Yu, R. A Tutorial on VAEs: From Bayes’ Rule to Lossless Compression. <italic>arXiv</italic> (2020).</mixed-citation>
      </ref>
      <ref id="CR61">
        <label>61.</label>
        <mixed-citation publication-type="other">Kingma, D. P. &amp; Welling, M. Auto-encoding variational bayes. in <italic>2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings</italic> (International Conference on Learning Representations, ICLR, 2014).</mixed-citation>
      </ref>
      <ref id="CR62">
        <label>62.</label>
        <mixed-citation publication-type="other">Im, D. J., Ahn, S., Memisevic, R. &amp; Bengio, Y. Denoising Criterion for Variational Auto-Encoding Framework. <italic>arXiv</italic> 2059–2065 (2015).</mixed-citation>
      </ref>
      <ref id="CR63">
        <label>63.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname></name><name><surname>R</surname><given-names>C</given-names></name><etal/></person-group><article-title>Array programming with NumPy</article-title><source>Nature</source><year>2020</year><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id>
</element-citation>
      </ref>
      <ref id="CR64">
        <label>64.</label>
        <mixed-citation publication-type="other">The pandas development team. <italic>Pandas-Dev/pandas: Pandas</italic>. 10.5281/zenodo.7093122 (2022).</mixed-citation>
      </ref>
      <ref id="CR65">
        <label>65.</label>
        <mixed-citation publication-type="other">Mc Kinney, W. Data Structures for Statistical Computing in Python. in <italic>Proceedings of the 9th Python in Science Conference</italic> (eds. van der Walt, S. &amp; Millman, J.) 56–61 (2010).</mixed-citation>
      </ref>
      <ref id="CR66">
        <label>66.</label>
        <mixed-citation publication-type="other">Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. <italic>Adv. Neural Inf. Process. Syst</italic>. <bold>32</bold>, (2019).</mixed-citation>
      </ref>
      <ref id="CR67">
        <label>67.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hediyeh-Zadeh</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Webb</surname>
              <given-names>AI</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>MJ</given-names>
            </name>
          </person-group>
          <article-title>MsImpute: Estimation of Missing Peptide Intensity Data in Label-Free Quantitative Mass Spectrometry</article-title>
          <source>Mol. Cell. Proteom.</source>
          <year>2023</year>
          <volume>22</volume>
          <fpage>100558</fpage>
          <pub-id pub-id-type="doi">10.1016/j.mcpro.2023.100558</pub-id>
        </element-citation>
      </ref>
      <ref id="CR68">
        <label>68.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>K-Y</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>B-J</given-names>
            </name>
            <name>
              <surname>Yi</surname>
              <given-names>G-S</given-names>
            </name>
          </person-group>
          <article-title>Reuse of imputed data in microarray analysis increases imputation efficiency</article-title>
          <source>BMC Bioinforma.</source>
          <year>2004</year>
          <volume>5</volume>
          <fpage>160</fpage>
          <pub-id pub-id-type="doi">10.1186/1471-2105-5-160</pub-id>
        </element-citation>
      </ref>
      <ref id="CR69">
        <label>69.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stacklies</surname><given-names>W</given-names></name><name><surname>Redestig</surname><given-names>H</given-names></name><name><surname>Scholz</surname><given-names>M</given-names></name><name><surname>Walther</surname><given-names>D</given-names></name><name><surname>Selbig</surname><given-names>J</given-names></name></person-group><article-title>pcaMethods–a bioconductor package providing PCA methods for incomplete data</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>1164</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm069</pub-id><pub-id pub-id-type="pmid">17344241</pub-id>
</element-citation>
      </ref>
      <ref id="CR70">
        <label>70.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kowarik</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Templ</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Imputation with the R Package VIM</article-title>
          <source>J. Stat. Softw.</source>
          <year>2016</year>
          <volume>74</volume>
          <fpage>1</fpage>
          <lpage>16</lpage>
          <pub-id pub-id-type="doi">10.18637/jss.v074.i07</pub-id>
        </element-citation>
      </ref>
      <ref id="CR71">
        <label>71.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Todorov</surname>
              <given-names>V</given-names>
            </name>
            <name>
              <surname>Templ</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Filzmoser</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Detection of multivariate outliers in business survey data with incomplete information</article-title>
          <source>Adv. Data Anal. Classif.</source>
          <year>2011</year>
          <volume>5</volume>
          <fpage>37</fpage>
          <lpage>56</lpage>
          <pub-id pub-id-type="doi">10.1007/s11634-010-0075-2</pub-id>
        </element-citation>
      </ref>
      <ref id="CR72">
        <label>72.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>van Buuren</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Groothuis-Oudshoorn</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>mice: Multivariate Imputation by Chained Equations in R</article-title>
          <source>J. Stat. Softw.</source>
          <year>2011</year>
          <volume>45</volume>
          <fpage>1</fpage>
          <lpage>67</lpage>
          <pub-id pub-id-type="doi">10.18637/jss.v045.i03</pub-id>
        </element-citation>
      </ref>
      <ref id="CR73">
        <label>73.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>R</given-names></name><etal/></person-group><article-title>GSimp: A Gibbs sampler based left-censored missing value imputation approach for metabolomics studies</article-title><source>PLoS Comput. Biol.</source><year>2018</year><volume>14</volume><fpage>e1005973</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005973</pub-id><pub-id pub-id-type="pmid">29385130</pub-id>
</element-citation>
      </ref>
      <ref id="CR74">
        <label>74.</label>
        <mixed-citation publication-type="other">Kong, W. et al. ProJect: a powerful mixed-model missing value imputation method. <italic>Brief. Bioinform</italic>. 10.1093/bib/bbad233 (2023).</mixed-citation>
      </ref>
      <ref id="CR75">
        <label>75.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pedregosa</surname>
              <given-names>F</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Scikit-learn: Machine learning in python</article-title>
          <source>J. Mach. Learn. Res.</source>
          <year>2011</year>
          <volume>12</volume>
          <fpage>2825</fpage>
          <lpage>2830</lpage>
        </element-citation>
      </ref>
      <ref id="CR76">
        <label>76.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vallat</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <article-title>Pingouin: statistics in python</article-title>
          <source>J. Open Source Softw.</source>
          <year>2018</year>
          <volume>3</volume>
          <fpage>1026</fpage>
          <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>
        </element-citation>
      </ref>
      <ref id="CR77">
        <label>77.</label>
        <mixed-citation publication-type="other">Seabold, S. &amp; Perktold, J. Statsmodels: Econometric and statistical modeling with Python. in <italic>Proceedings of the 9th Python in Science Conference</italic> vol. 57 10–25080 (Austin, TX, 2010).</mixed-citation>
      </ref>
      <ref id="CR78">
        <label>78.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Benjamini</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Hochberg</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>Controlling the false discovery rate: A practical and powerful approach to multiple testing</article-title>
          <source>J. R. Stat. Soc. Ser. B Stat. Methodol.</source>
          <year>1995</year>
          <volume>57</volume>
          <fpage>289</fpage>
          <lpage>300</lpage>
          <pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id>
        </element-citation>
      </ref>
      <ref id="CR79">
        <label>79.</label>
        <mixed-citation publication-type="other">Mazzanti, S. <italic>Mrmr-Selection</italic>. (2022).</mixed-citation>
      </ref>
      <ref id="CR80">
        <label>80.</label>
        <element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez-Riverol</surname><given-names>Y</given-names></name><etal/></person-group><article-title>The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences</article-title><source>Nucleic Acids Res</source><year>2022</year><volume>50</volume><fpage>D543</fpage><lpage>D552</lpage><pub-id pub-id-type="doi">10.1093/nar/gkab1038</pub-id><pub-id pub-id-type="pmid">34723319</pub-id>
</element-citation>
      </ref>
      <ref id="CR81">
        <label>81.</label>
        <mixed-citation publication-type="other">Webel, H. et al. <italic>Mass Spectrometry-Based Proteomics Imputation Using Self-Supervised Deep Learning</italic>. PIMMS, 10.5281/zenodo.10854544 (2023).</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
</pmc-articleset>
