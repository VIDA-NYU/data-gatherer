{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ffc8cfa8103ed2",
   "metadata": {},
   "source": [
    "# Vector Retrieval Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:06.790281Z",
     "start_time": "2025-06-30T21:37:53.494753Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_gatherer.data_gatherer import DataGatherer\n",
    "from data_gatherer.parser.xml_parser import XMLParser\n",
    "from data_gatherer.parser.html_parser import HTMLParser\n",
    "from data_gatherer.logger_setup import setup_logging\n",
    "from data_gatherer.retriever.embeddings_retriever import EmbeddingsRetriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from lxml import etree\n",
    "import dspy\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logging('vector_retrieval_experiment', './logs/vector_retrieval_experiment.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting vector retrieval experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbcf8555f0cedf",
   "metadata": {},
   "source": [
    "## 1. Load corpus and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccefdcf4200617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:09.529072Z",
     "start_time": "2025-06-30T21:38:06.793253Z"
    }
   },
   "outputs": [],
   "source": [
    "input_corpus = pd.read_parquet('scripts/exp_input/Local_fetched_data.parquet')  # or load HTML and extract text\n",
    "ground_truth = pd.read_parquet('scripts/output/gold/dataset_citation_records_Table.parquet')  # adjust as needed\n",
    "\n",
    "# Add a warning about input data:\n",
    "logger.info(f\"Corpus shape: {str(input_corpus.shape)}\")\n",
    "logger.info(f\"Ground truth shape: {str(ground_truth.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea03a752c2d86a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:10.883677Z",
     "start_time": "2025-06-30T21:38:09.530712Z"
    }
   },
   "outputs": [],
   "source": [
    "ground_truth['pmc_id'] = ground_truth['citing_publication_link'].str.extract(r'(PMC\\d+)', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cf03e316bf14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:10.917994Z",
     "start_time": "2025-06-30T21:38:10.886171Z"
    }
   },
   "outputs": [],
   "source": [
    "input_corpus.head()  # Check the structure of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d0d7e0a4c4c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:10.931301Z",
     "start_time": "2025-06-30T21:38:10.920368Z"
    }
   },
   "outputs": [],
   "source": [
    "ground_truth.head()  # Check the structure of the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced query using hackathon context trigger keywords\n",
    "query_ontology_aware = \"\"\"Data Availability Statement or mentions of dataset repositories/portals, identifiers, or accession codes, including PRIDE, ProteomeXchange, MassIVE, iProX, JPOST, Proteomic Data Commons (PDC), Genomic Data Commons (GDC), Cancer Imaging Archive (TCIA), Imaging Data Commons (IDC), Gene Expression Omnibus (GEO), ArrayExpress, dbGaP, Sequence Read Archive (SRA), Protein Data Bank (PDB), Mendeley Data, Synapse, European Genome-Phenome Archive (EGA), BIGD, and ProteomeCentral. \n",
    "Also include dataset identifiers or links such as PXD, MSV, GSE, GSM, GPL, GDS, phs, syn, PDC, PRJNA, DOI, or accession code. \n",
    "Look for phrases like deposited in, available at, submitted to, uploaded to, archived in, hosted by, retrieved from, accessible via, or publicly available. \n",
    "Capture statements indicating datasets, repositories, or data access locations.\n",
    "\"\"\"\n",
    "query_augmented = \"\"\"Dataset or data repository information including: deposited in, uploaded to, archived at, available at, stored on, hosted by, accessible via, retrieved from, provided by, experimental data, raw data, public repository, data archive, data portal, accession code\"\"\"\n",
    "query_base = \"Available data, accession code, data repository, deposited data\"\n",
    "\n",
    "query_base_adj = \"Data Availability Statement, Methods with dataset mention(s), Deposited data, Data Accession, Data Provenance, Downloaded data.\"\n",
    "\n",
    "query = query_ontology_aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml_parser = XMLParser('open_bio_data_repos.json', logger, llm_name='gemini-2.0-flash', use_portkey=True)\n",
    "#html_parser = HTMLParser('open_bio_data_repos.json', logger, llm_name='gemini-2.0-flash', use_portkey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25edfd10ad51b367",
   "metadata": {},
   "source": [
    "Note: some files are being skipped because of ground truth incompleteness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2c9dba5bb24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:55.369543Z",
     "start_time": "2025-06-30T21:38:11.167780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Systematic evaluation of different models and top-k values (OPTIMIZED)\n",
    "\n",
    "# Define models to test\n",
    "models_to_test = [\n",
    "    # Base models\n",
    "    #############################################'sentence-transformers/all-MiniLM-L6-v2', \n",
    "    #'sentence-transformers/all-mpnet-base-v2',\n",
    "    #'sentence-transformers/all-MiniLM-L12-v2',\n",
    "    #'sentence-transformers/sentence-t5-base',\n",
    "\n",
    "    # BioMed\n",
    "    #############################################\"sentence-transformers/embeddinggemma-300m-medical\"\n",
    "    #\"neuml/pubmedbert-base-embeddings\",\n",
    "    #'sentence-transformers/allenai-specter',\n",
    "    #'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext',\n",
    "\n",
    "    # MSMARCO passage ranking models\n",
    "    #############################################'sentence-transformers/msmarco-distilbert-base-v4',\n",
    "    #'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    #'sentence-transformers/msmarco-distilbert-dot-v5',\n",
    "    #'sentence-transformers/msmarco-distilbert-base-tas-b',\n",
    "\n",
    "    # Paraphrase models\n",
    "    #############################################'sentence-transformers/paraphrase-MiniLM-L3-v2',\n",
    "\n",
    "    # Semantic Search\n",
    "    #'sentence-transformers/multi-qa-mpnet-base-cos-v1',\n",
    "    #'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
    "    #############################################'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "]\n",
    "\n",
    "# Define top-k values to test\n",
    "topk_values = [1, 3, 5, 9]\n",
    "max_k = max(topk_values)  # We'll retrieve this many and slice for smaller k values\n",
    "\n",
    "# Store results - Initialize properly for all queries\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "logger.info(f\"Starting OPTIMIZED systematic evaluation with enhanced query\")\n",
    "logger.info(f\"Testing {len(models_to_test)} models with top-k values {topk_values}\")\n",
    "logger.info(f\"Optimization: Single model load per model, reuse for all publications\")\n",
    "\n",
    "queries = ['query_ontology_aware', 'query_augmented', 'query_base', 'query_base_adj']\n",
    "\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    # Clean up previous embeddings\n",
    "    if os.path.exists(\"corpus_embeddings.npy\"):\n",
    "        os.remove(\"corpus_embeddings.npy\")\n",
    "\n",
    "    start_time = time.time()  \n",
    "    cnt = 0\n",
    "\n",
    "    #html_parser.embeddings_retriever.embed_query(query)\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Testing model: {model_name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    html_parser = HTMLParser('open_bio_data_repos.json', logger, llm_name='gemini-2.0-flash', use_portkey=True, embeddings_model_name=model_name)\n",
    "    xml_parser = XMLParser('open_bio_data_repos.json', logger, llm_name='gemini-2.0-flash', use_portkey=True, embeddings_model_name=model_name)\n",
    "\n",
    "    queries_recall = {(q_name, topk): 0 for q_name in queries for topk in topk_values}\n",
    "    \n",
    "    for i, publication in input_corpus.iterrows():\n",
    "        if i == 10:  # Limit to first 10 publications for faster testing\n",
    "            break\n",
    "        logger.info(f\"Publication: {publication['publication']}\")\n",
    "            \n",
    "        gt = ground_truth[ground_truth['pmc_id'].str.lower() == publication['publication'].lower()]\n",
    "        idnts = gt['identifier'].tolist()\n",
    "\n",
    "        logger.info(f\"Identifiers in ground truth: {idnts}\")\n",
    "            \n",
    "        if publication['format'] == 'xml':\n",
    "            sections = xml_parser.extract_sections_from_xml(etree.fromstring(publication['raw_cont'].encode('utf-8')))\n",
    "            sections = xml_parser.from_sections_to_corpus(sections)\n",
    "            parser = xml_parser\n",
    "        elif publication['format'] == 'html':\n",
    "            clean_html = html_parser.normalize_HTML(publication['raw_cont'])\n",
    "            sections = html_parser.extract_sections_from_html(clean_html)\n",
    "            sections = html_parser.from_sections_to_corpus(sections)\n",
    "            parser = html_parser\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"Unsupported format {publication['format']} for publication {publication['publication']}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "         # Check if identifiers are in content\n",
    "        idnts_in_cont = []\n",
    "        for idnt in idnts:\n",
    "            if idnt in publication['raw_cont']:\n",
    "                 idnts_in_cont.append(idnt)\n",
    "            \n",
    "        logger.info(f\"Identifiers in content: {idnts_in_cont}\")\n",
    "            \n",
    "        if not idnts_in_cont:\n",
    "            continue\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        # Prepare corpus\n",
    "        corpus = []\n",
    "        for section in sections:\n",
    "            corpus.append({\n",
    "                'sec_txt': 'Section Title: ' + section['section_title'] + \n",
    "                        '. Content: ' + section['sec_txt']\n",
    "            })\n",
    "            \n",
    "        logger.info(f\"Corpus:\\n{str.join('\\n',[item['sec_txt'] for item in corpus])}\")\n",
    "\n",
    "        try:\n",
    "            parser.embeddings_retriever.embed_corpus(corpus, batch_size=128)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error embedding corpus for publication {publication['publication']}: {e}\")\n",
    "    \n",
    "        for q_i,query in enumerate([query_ontology_aware, query_augmented, query_base, query_base_adj]):\n",
    "            q_name = queries[q_i]\n",
    "            logger.info(f\"Query {q_i} {q_name}: {query}\")\n",
    "            \n",
    "            html_parser.embeddings_retriever.embed_query(query)\n",
    "            xml_parser.embeddings_retriever.embed_query(query)\n",
    "            \n",
    "            # OPTIMIZATION: Only embed corpus (model already loaded)\n",
    "            try:\n",
    "                #parser.embeddings_retriever.embed_corpus(corpus, batch_size=128)\n",
    "                \n",
    "                # OPTIMIZATION: Single retrieval with max_k, then slice for different k values\n",
    "                full_result = parser.embeddings_retriever.search(query=None, k=max_k)\n",
    "\n",
    "                for full_result_item in full_result:\n",
    "                    logger.info(f\"L2 Norm {full_result_item['L2_distance']} --> {full_result_item['text'][:150]}\")\n",
    "\n",
    "                # Evaluate for all top-k values using the same retrieval result\n",
    "                for topk_docs_to_retrieve in topk_values:\n",
    "                    logger.info(f\"Evaluating with top-k = {topk_docs_to_retrieve}\")\n",
    "                    \n",
    "                    # Slice results for current k value\n",
    "                    result = full_result[:topk_docs_to_retrieve]\n",
    "                    \n",
    "                    # Combine all retrieved text\n",
    "                    iterres = '. '.join([r['text'] for r in result])\n",
    "                    \n",
    "                    # Check matches\n",
    "                    matches = set()\n",
    "                    not_matched = set()\n",
    "                    for j, row in gt.iterrows():\n",
    "                        if row['identifier'].lower() in iterres.lower():\n",
    "                            queries_recall[q_name,topk_docs_to_retrieve] += 1/len(idnts_in_cont)\n",
    "                            matches.add(row['identifier'])\n",
    "                    \n",
    "                    not_matched = set(idnts_in_cont) - matches\n",
    "                    \n",
    "                    logger.info(f\"Publication {publication['publication']}, Top-k {topk_docs_to_retrieve}: Found {len(matches)} matches out of {len(idnts_in_cont)} ground truth\")\n",
    "                    \n",
    "                    logger.info(f\"Missed citations: {not_matched}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing publication {i+1} with model {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "            # Calculate final recalls and store results for all top-k values\n",
    "            \n",
    "    for topk_docs_to_retrieve in topk_values:\n",
    "        for q_i, q_name in enumerate(queries):\n",
    "            final_recall = queries_recall[q_name,topk_docs_to_retrieve]/cnt if cnt > 0 else 0\n",
    "\n",
    "            # Store results - Create a unique key combining model and query\n",
    "            result_key = f\"{model_name}_{q_name}\"\n",
    "            results[result_key][topk_docs_to_retrieve] = {\n",
    "                'recall': final_recall,\n",
    "                'processed_docs': cnt,\n",
    "                'query': q_name,\n",
    "                'model': model_name\n",
    "            }\n",
    "                \n",
    "            logger.info(f\"Model: {model_name}, Top-k: {topk_docs_to_retrieve}, Recall: {final_recall:.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time    \n",
    "    logger.info(f\"Total time for model {model_name}: {elapsed_time:.2f}s\")\n",
    "\n",
    "logger.info(f\"\\n{'='*60}\")\n",
    "logger.info(\"OPTIMIZED evaluation completed!\")\n",
    "logger.info(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VECTOR RETRIEVAL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group results by query type - Updated logic for new structure\n",
    "query_results = {'ontology_aware': {}, 'augmented': {}, 'base': {}}\n",
    "\n",
    "for result_key, model_results in results.items():\n",
    "    # Extract model name and query name from the combined key\n",
    "    if '_query_' in result_key:\n",
    "        model_name, query_name = result_key.split('_query_', 1)\n",
    "    else:\n",
    "        # Fallback: extract from stored data\n",
    "        first_result = list(model_results.values())[0]\n",
    "        model_name = first_result['model']\n",
    "        query_name = first_result['query']\n",
    "    \n",
    "    if query_name not in query_results:\n",
    "        query_results[query_name] = {}\n",
    "    if model_name not in query_results[query_name]:\n",
    "        query_results[query_name][model_name] = {}\n",
    "    \n",
    "    for topk, metrics in model_results.items():\n",
    "        query_results[query_name][model_name][topk] = metrics\n",
    "\n",
    "print(f\"Found query types: {list(query_results.keys())}\")\n",
    "print(f\"Results structure check:\")\n",
    "for q_type, q_data in query_results.items():\n",
    "    print(f\"  {q_type}: {len(q_data)} models\")\n",
    "\n",
    "# Create and save tables for each query type\n",
    "all_tables = {}\n",
    "\n",
    "for query_name, query_data in query_results.items():\n",
    "    if not query_data:  # Skip empty query results\n",
    "        print(f\"Skipping empty query: {query_name}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS FOR {query_name.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create table data\n",
    "    table_data = []\n",
    "    for model_name, model_results in query_data.items():\n",
    "        row = {'Model': model_name.split('/')[-1]}  # Just the model name without org\n",
    "        \n",
    "        # Add recall for each top-k value\n",
    "        for topk in sorted(model_results.keys()):\n",
    "            row[f'Top-K {topk}'] = f\"{model_results[topk]['recall']:.4f}\"\n",
    "        \n",
    "        # Add processing info\n",
    "        first_result = list(model_results.values())[0]\n",
    "        row['Processed Docs'] = first_result['processed_docs']\n",
    "        \n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame and display\n",
    "    df = pd.DataFrame(table_data)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Find best model for this query\n",
    "    best_recall = 0\n",
    "    best_config = None\n",
    "    for model_name, model_results in query_data.items():\n",
    "        for topk, metrics in model_results.items():\n",
    "            if metrics['recall'] > best_recall:\n",
    "                best_recall = metrics['recall']\n",
    "                best_config = (model_name.split('/')[-1], topk)\n",
    "    \n",
    "    if best_config:\n",
    "        print(f\"\\n🏆 Best for {query_name}: {best_recall:.4f} - {best_config[0]} (Top-K: {best_config[1]})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL QUERIES EVALUATION COMPLETE!\")\n",
    "print(\"Tables saved to scripts/output/\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Debug: Show actual results structure\n",
    "print(f\"\\nDEBUG - Raw results structure:\")\n",
    "for result_key, model_results in results.items():\n",
    "    print(f\"Result Key: {result_key}\")\n",
    "    for topk, metrics in model_results.items():\n",
    "        print(f\"  Top-K {topk}: query={metrics['query']}, model={metrics['model']}, recall={metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a251f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96638f4fa1c2308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T21:38:55.407709Z",
     "start_time": "2025-06-30T21:38:55.379434Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Define your trainset (queries and expected retrievals)\n",
    "trainset = [\n",
    "    dspy.Example(\n",
    "        question=\"Data is available with accession code ABC0123 in Repository XYZ\",            \n",
    "        references=[\"Data Availability Statement\\nThe datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found below: NCBI GEO repository,\\nGSE123128\\n.\"]\n",
    "        ),\n",
    "    dspy.Example(\n",
    "        question=\"Data is available with accession code ABC0123 in Repository XYZ\",            \n",
    "        references=['Data Availability Statement\\nRaw sequencing data from this study have been deposited in the GEO database with the accession number\\nGSE171155\\n. The mass spectrometry proteomics data have been deposited to the ProteomeXchange Consortium via the PRIDE [1] partner repository with the data set identifier PXD024161 and 10.6019/PXD024161.']\n",
    "        ),\n",
    "    dspy.Example(\n",
    "        question=\"Data is available with accession code ABC0123 in Repository XYZ\",            \n",
    "        references=['Associated Data\\nThis section collects any data citations, data availability statements, or supplementary materials included in this article.\\nSupplementary Materials\\nDocument S1. Figures\\xa0S1–S6 and Tables\\xa0S1–S6\\nmmc1.pdf\\n(2.5MB, pdf)\\nDocument S2. Article plus supplemental information\\nmmc2.pdf\\n(9.1MB, pdf)\\nData Availability Statement\\n•\\nThe next-generation DNA sequencing dataset generated during this study is available at the National Genomics Data Center: HRA003231 (URL:\\nhttps://ngdc.cncb.ac.cn\\n). The mass spectrometry proteomics data reported in this paper have been deposited to the ProteomeXchange Consortium: PXD037076(\\nhttp://proteomecentral.proteomexchange.org\\n) via iProx partner repository\\n61\\n.\\n•\\nThis paper does not report the original code.\\n•\\nAny additional information required to reanalyze the data reported in this work paper is available from the\\nlead contact\\nupon request.']\n",
    "        ),\n",
    "    dspy.Example(\n",
    "        question=\"Data is available with accession code ABC0123 in Repository XYZ\",            \n",
    "        references=['Data and Code Availability\\nRNA-seq data generated in this study are available at NCBI GEO database with the accession number\\nGSE151029\\n. The 53BP1 mass spectrometry data have been deposited to the ProteomeXchange Consortium via the PRIDE partner repository with the dataset identifier PXD020090. The accession number for the FOXK1 and FOXK2 MS data reported in this paper is PRIDE: PXD001383']\n",
    "        ),\n",
    "    dspy.Example(\n",
    "        question=\"Data is available with accession code ABC0123 in Repository XYZ\",            \n",
    "        references=[\"METHODS\\nConstruction of Plasmids\\nThe protein-coding regions of the NST3 gene were amplified from the Arabidopsis thaliana cDNA library with appropriate primers (see Supplemental Table 2 online). The 5′ upstream region of 3027 bp, which extended from the site of initiation of translation of the NST3 gene, was used for preparation of the ProNST3:GUS, ProNST3:NST3, and ProNST3:NST3SRDX gene constructs. These genes and 35S:NST3 were constructed from modified vectors derived from pGreenII0029 (Hellens et al., 2000) and p35SSRDXG (Mitsuda et al., 2006). For complementation analysis, we used genomic fragments including NST1 (9580 bp) and NST3 (5199 bp), which contained 6523 and 3069 bp of the respective promoter regions. The region corresponding to the transgene of each vector, with the exception of the pGreen-based vectors, was transferred to the pBCKH plant expression vector (Mitsuda et al., 2006) using the Gateway system (Invitrogen).\\n\\nConditions for Plant Growth and Transformation\\nArabidopsis plants were grown in soil at 22°C with 16 h (long-day condition) or 8 h (short-day condition) of light daily. Unless otherwise stated, plants were grown under the long-day condition. For transformation, a T-DNA vector carrying the appropriate construct was introduced into Agrobacterium tumefaciens strain GV3101 by electroporation, and the resultant Agrobacterium was infiltrated into Arabidopsis using the floral dip method (Clough and Bent, 1998).\\n\\n\\nAssessment of the Mechanical Strength of Inflorescence Stems\\nWe used the bottom 5 cm of inflorescence stems taller than 25 cm for measurement of Young's modulus according to a previously described method (Kojima and Yamamoto, 2004).\\n\\nExamination of the Crystal State of Cellulose Microfibrils of Inflorescence Stems\\nThe bottom region of the inflorescence stems, as described above, was used for x-ray diffraction analysis according to a previously described method (Abe and Yamamoto, 2005). Nickel-filtered Cu Kα radiation (wavelength, 0.154 nm) at 30 kV and 35 mA was used with the reflection technique.\\n\\nIsolation of RNA, Microarray Experiments, and Analysis\\nTotal RNA was isolated with Trizol as described previously (Fukuda et al., 1991) from the bottom 4 cm of the inflorescence stems of three independent plants grown under the short-day condition and with a height of between 13 and 17 cm. Microarray analyses were performed with the Arabidopsis 2 Oligo Microarray (Agilent Technologies). All microarray experiments and the analysis of data were performed as described previously (Mitsuda et al., 2005) with the exceptions summarized below. P values for differences between nst1-1 nst3-1 and wild-type plants were calculated by Welch's t test, based on a two-tailed distribution (n = 3). To minimize type-I family-wise errors in multiple and simultaneous statistical tests, we adopted a strategy for suppression of false positives. We calculated a Q-value to estimate the false discovery rate from the P value described above using QVALUE software (Storey and Tibshirani, 2003) with the default setting. We considered genes with a Q-value of <0.1 to be genes expressed at different levels in nst1-1 nst3-1 and wild-type plants. Comprehensive gene group analysis using Fisher's exact test was performed with the R program package (http://www.r-project.org/). Quantitative RT-PCR was performed as described previously (Mitsuda et al., 2005). For the analysis of NST transcripts in the mutant lines, RT-PCR was performed with appropriate primers (see Supplemental Table 2 online).\\n\\nLight and Fluorescence Microscopy\\nFor observations of lignin autofluorescence, we used a filter with the following specifications: glass, 365; dichroic mirror, 395; long-pass, 400. To observe ectopic secondary wall thickening, we cleared tissues by incubating them overnight in 70% lactic acid at 50°C. To prepare 70- to 150-μm sections of inflorescence stems and hypocotyls, we embedded the tissues in 3% agar then sectioned them on a vibrating microtome (HM-650V; Microm). Assays of GUS activity were performed with T1 or T2 transgenic plants. Plant tissues were fixed briefly, in some cases, in solution containing 0.3% formalin, 0.2% MES, pH 5.8, and 0.3 M mannitol before incubation in 100 mM sodium phosphate buffer, pH 7.0, containing 0.1% Triton X-100, 1 mM 5-bromo-4-chloro-3-indolyl-β-d-glucuronide, and 0.5 mM potassium ferricyanide at 37°C for up to 12 h. Stained stems and hypocotyls were embedded in 3% agar and sectioned. All observations by light and fluorescence microscopy were made with the Axioskop2 plus system (Carl Zeiss).\\n\\nUltrastructural Observation by Transmission Electron Microscopy\\nShort pieces of inflorescence stems were fixed in 30 mM HEPES buffer containing 2% paraformaldehyde and 2% glutaraldehyde then fixed in HEPES buffer containing 2% osmium tetroxide. Fixed tissues were embedded in Q651 resin (Nissin EM). Sections of 80 to 90 nm thick were post-stained with uranyl acetate and lead citrate and observed by a JEM1200EX transmission electron microscope (JEOL) at an accelerating voltage of 80 kV.\\n\\nIdentification of NST Homologs in Poplar\\nPoplar NAC genes resembling the Arabidopsis NST genes were collected using the Advanced Search tool of the Joint Genome Initiative poplar database (http://genome.jgi-psf.org/Poptr1/Poptr1.home.html) with the command, “find by homology to related protein with E-value <1.0e-20”; the database for Populus trichocarpa; and the query “At2g46770.” The 62 extracted sequences and amino acid sequences of subfamily IIb of NAC transcription factors of Arabidopsis, as defined in a previous study (Mitsuda et al., 2005), were aligned using the ClustalW program with default settings (Chenna et al., 2003). The amino acid sequences corresponding to conserved NAC domains were extracted and realigned. A phylogenetic tree was built by neighboring-joining method using ClustalW with default settings (an alignment and the sequences are shown in Supplemental Table 3 online). Bootstrap values were calculated from 100 trials. The subtree including the NST and VND genes is shown in Figure 7.\\n\\nAccession Numbers and Data Deposition\\nNST1 and NST3 reported in this study correspond to the Arabidopsis Genome Initiative locus identifiers At2g46770 and At1g32770, respectively. Microarray data performed in this study can be found in the National Center for Biotechnology Information Gene Expression Omnibus data library under accession number GSE5187.\\n\"]\n",
    "        ),\n",
    "    dspy.Example(\n",
    "        question=\"Data is available with accession code ABC0123 in Repository XYZ\",            \n",
    "        references=[\"Materials and methods\\nMaterials\\nDilution series\\nIllumina HumanCNV370-Duo BeadChip Infinium SNP data for dilution series of 12 mixtures of cancer cell line (HCC1395) mixed with its paired normal cell line (HCC1395BL) were downloaded from the NCBI Gene Expression Omnibus accession [GEO:GSE11976]. We excluded chromosome 6 and 16 from analysis due to copy genomic aberrations present in the normal cell line HCC1395BL.\\n\\nCancer cell lines\\nIllumina HumanHap300 data for the promyelocytic leukemia cancer cell HL-60 and colon cancer cell line HT-29 were obtained from Illumina, and Human-610 Quad SNP genotyping data for the colon cancer cell lines SW403, SW480, SW620, SW837, SW1417 and LIM1863 were generated at the Ludwig Institute of Cancer Research using standard processing protocols. The genotyping data for breast cancer cell lines MDA-175 and MDA-468 were downloaded from the NCBI Gene Expression Omnibus accession [GEO:GSE18799] [23].\\n\\nPrimary breast tumors\\nThree breast tumors (cases 114, 601 and 3,364) that had not received non-neoadjuvant therapy were analyzed in detail using material derived from microdissection. For each case, material containing pure tumor and pure stroma cells respectively was microdissected and compared to data obtained from surgically obtained material from the same tumors. Case 114 was of Luminal B type (23 mm tumor, moderately differentiated infiltrating ductal carcinoma with an extensive in-situ component. Node +ve, ER +ve (6.8 fm/mg protein), EGFR -ve (7.8 fm/mg protein)). Case 601 (20 mm 30 mm tumor, grade 3 with intraductal in-situ ca. and in filtrating ductal carcinoma, node +ve, ER -ve (1.5 fm/mg protein), Her2 +ve (histoscore of 3), EGFR +ve (histoscore of 208)) was classified as ERBB2 positive based on expression microarray data with a fractional rank of 0.982, Case 3,364 was 25 mm grade 3 infiltrating ductal carcinoma, ER positive (8 fm/mg protein), PR positive (histoscore 8/8), Her2 positive (histoscore 3+, one of ten axillary nodes +ve). For each case, DNA was extracted from microdissected stroma and tumor, as well as the original non-dissected sample and analyzed using Illumina Human-610 Quad SNP arrays applying standard protocols.\\n\\nData processing\\nGenome Alteration Print was downloaded [43] and used to analyze all datasets using default settings and the highest ranked copy number and LOH predictions used for comparisons. However, for the cancer cell line dilution series, we re-used the results that had previously been generated by [23] and made available on the aforementioned website.\\n\\nGenoCN v1.06 was downloaded [44] and used with default settings and stromal contamination settings on for all datasets generated using Illumina Infonaut II SNP arrays. Adjusted GenoCN parameters for the Log R Ratio levels were used for Infonaut HD SNP array processing and in these instances we used the same levels that we specified for OncoSNP. The copy number and LOH predictions from the Viterbi sequence were used for comparisons.\\n\\nOncoSNP was run on all datasets using 15 EM iterations and with both stromal and intra-tumor heterogeneity options. In all cases, the ploidy prediction with the highest maximum likelihood was chosen and the Viterbi sequence of tumor states used for comparisons. We filtered detected aberrations using a Log Bayes Factor of 30.\\n\\nStatistical model\\nA complete description of our statistical model is provided in Supplementary Information in Additional file 1.\\n\\nLet xi denote the tumor state at the i-th probe location and (xi, n, xi, t) denote the associated normal and tumor copy numbers. Furthermore, let zi = (zi, n,zi, t) denote the B allele count for the normal and tumor genotype respectively. The combinations (zi, n, (xi, n) and (zi, t, xi, t) fully define the normal and tumor genotypes respectively. The tumor state at each probe denotes the allowable combinations of normal-tumor genotypes at that location as shown in Table 1.\\n\\nLet π0 denote the normal DNA fraction of the tumor sample due to stromal contamination and 𝜋={𝜋𝑖}𝑛\\n𝑖=1 denote the proportion of tumor cells having the normal genotype at each probe. The data 𝑦={𝑦𝑖}𝑛\\n𝑖=1 consists of a set of two-dimensional vectors yi = [ri, bi]' whose elements correspond to the Log R Ratio and B allele frequency respectively.\\n\\nGiven (x, z, π, π0) the data is assumed to be distributed according to a (K + 1)-component mixture of Student t-distributions, where ki indicates the mixture component assignment of the i-th data point,\\n\\n𝑦𝑖|𝑥𝑖,𝑧𝑖,𝑘𝑖,𝑚,𝛿, 𝛴={ \\n𝑆⁢𝑡(𝑚⁡(𝑥𝑖,𝑧𝑖)+𝛿(𝑙𝑙)\\n𝑘𝑙,∑(𝑙𝑖)\\n𝑘𝑖,𝜈),\t𝑘≠0,\\n𝑈𝑟⁡(𝑟min,𝑟max)×U𝑏⁢(0,1),\t𝑘=0,\\n \\n(1)\\nwhere 𝑆⁢𝑡⁡(𝛿(𝑙)\\n𝑘,𝛴(𝑙)\\n𝑘,𝑣) is the probability density function of the Student t-distribution with mean 𝛿(𝑙)\\n𝑘 and covariance matrix 𝛴(𝑙)\\n𝑘 associated with the k-th mixture component and the l-th genotype class and v degrees of freedom. The 0-th component is an outlier class which assumes uniformly distributed data over a specified range.\\n\\nThe elements of the mean vectors m(xi, zi) = [mr(xi), mb(zi, xi)]' are given by the following:\\n\\n𝑚𝑟⁡(𝑥𝑖)=(𝜋𝑖⁢(1−𝜋0)+𝜋0)⁢\\n̅\\n𝑟\\n𝑥𝑖,𝑛+(1−𝜋𝑖)⁢(1−𝜋0)⁢\\n̅\\n𝑟\\n𝑥𝑥𝑖,⁢𝑡+𝛽0+𝛽1⁢𝑔𝑖,\\n(2)\\nwhere gi is the local GC content at the i-th probe location and\\n\\n𝑚𝑏⁡(𝑧𝑖,𝑥𝑖)=\\n(𝜋𝑖⁢(1−𝜋0)+𝜋0)⁢𝑧𝑖,𝑛+(1−𝜋𝑖)⁢(1−𝜋0)⁢𝑧𝑖,𝑡\\n(𝜋𝑖⁢(1−𝜋0)+𝜋0)⁢𝑥𝑖,𝑛+(1−𝜋𝑖)⁢(1−𝜋0)⁢𝑥𝑖,𝑡\\n \\n.\\n(3)\\nPrior distributions\\nThe prior distribution on the mixture weights is given by a Dirichlet distribution:\\n\\n𝑤(𝑙)|𝛼~𝐷⁢𝑖⁢𝑟⁡(𝛼),\\n(4)\\nwhere α is a concentration parameter which in the numerical results we used α = 1 to give a at prior on the mixture weights.\\n\\nThe prior distributions on the mixture centers and covariance matrices are given by standard conjugate Normal-Inverse Wishart distributions:\\n\\n𝛿(𝑙)\\n𝑘|𝜏, 𝛴(𝑙)\\n𝑘~𝑁⁡(0,𝜏 𝛴(𝑙)\\n𝑘), 𝑘=1,…, 𝐾, 𝑙 =1,2,3,\\n(5)\\n𝛴(𝑙)\\n𝑘|𝛾, 𝑆(𝑙)\\n𝑘~𝐼⁢𝑊⁡(𝛾,𝑆(𝑙)\\n𝑘), 𝑘=1,…, 𝐾, 𝑙 =1,2,3,\\n(6)\\nwhere τ is a hyperparameter that controls the strength of the prior and IW(γ, Λ) denotes the Inverse-Wishart distribution with parameter γ and scale matrix Λ.\\n\\nA beta prior is assumed for the outlier rate,\\n\\n𝜂|𝛼𝜂, 𝛽𝜂~𝐵⁢𝑒⁡(𝛼𝜂,𝛽𝜂),\\n(7)\\nwhere (αn, βn) are hyperparameters associated with the Beta prior. For the numerical results we set these as (1,1) to give a uniform distribution. \\n\\nA normal prior is assumed for the local GC content regression parameters,\\n\\n𝛽|𝜆𝛽~𝑁⁡(0,𝜆𝛽⁢𝐼2),\\n(8)\\nwhere Ip is a p × p identity matrix.\\n\\nA discrete prior is assumed for the stromal contamination content and intra-tumour heterogeneity levels,\\n\\n𝑝⁡(𝜋0)={ \\n𝛼𝜋0,𝜋0=0,\\n𝛽𝜋0,𝜋0>0,\\n \\n(9)\\nand\\n\\n𝑝⁡(𝜋𝑖)={ \\n𝛼𝜋,𝜋𝑖=0,\\n𝛽𝜋,𝜋𝑖>0,\\n  𝑖=1,…,𝑛,\\n(10)\\nwhere in the numerical results we have used απ0 = βπ0 = 1 and απ = 1, βπ = 2.\\n\\nThe tumor states are assumed to form an inhomogeneous Markov Chain with transition matrix,\\n\\n𝑝⁡(𝑥𝑖|𝑥𝑖−1)={ \\n1−𝜌,𝑥𝑖=𝑥𝑖−1,\\n𝜌,𝑥𝑖≠𝑥𝑖−1,\\n \\n(11)\\nwhere ρ = (1/2) (1-exp(-(1/2L) (si-si-1) and si is the physical coordinate of the i-th probe and L is a characteristic length which we set as L = 2,000,000 for the numerical results.\\n\\nPosterior inference\\nWe estimated the unknown model parameters using an expectation-maximization algorithm. Multiple restarts were used to explore different baseline of the Log R Ratio and the baseline with the greatest likelihood was chosen for the calculation of summary statistics.\\n\\nSummary statistics\\nWe used the Viterbi algorithm to extract the most likely sequence of tumors states and for each aberrant segment in the Viterbi sequence we calculated an approximate Bayes Factor (score) of that segment belonging to each of the tumor states. In addition we also recorded the maximum a posteriori estimates of the Log R Ratio baseline adjustment β0 and the stromal contamination π0.\\n\\nAvailability\\nA MATLAB based implementation (for 64 bit Linux systems) of our software is available for academic and non-commercial use from the associated website [45]. In addition, SNP data analyzed in this paper are also available from this website and from the Gene Expression Omnibus Database under Accession No.[GEO:GSE23785].\"]\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f49e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6a6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5758a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81feab17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
