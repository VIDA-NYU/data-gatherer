{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gatherer.data_gatherer import DataGatherer\n",
    "import pandas as pd\n",
    "import json\n",
    "import os, requests\n",
    "import numpy as np\n",
    "import re\n",
    "from data_gatherer.llm.response_schema import *\n",
    "from scripts.experiment_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ff7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGatherer(\n",
    "    log_level='INFO', \n",
    "    process_entire_document=True, \n",
    "    driver_path=None,\n",
    "    llm_name = 'gpt-5-mini',\n",
    "    \n",
    ")\n",
    "\n",
    "dg.logger.info(\"Data Gatherer initialized with params: %s\", dg.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the source publications\n",
    "\n",
    "with open('scripts/NYU_data_catalog/datacatalog_export-2025-10-02.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "datasets_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.DataFrame()\n",
    "\n",
    "for idx, row in datasets_df.iterrows():\n",
    "    pubs = row['publications']\n",
    "    if not pubs:\n",
    "        dg.logger.info(f\"No publications found for paper {idx + 1}, dataset title: {row['title']}\")\n",
    "        continue\n",
    "    dg.logger.info(f\"Found {len(pubs)} publication(s) for dataset {idx + 1}, dataset title: {row['title']}\")\n",
    "    dg.logger.debug(f\"Dataset title: {row['title']}\")\n",
    "    dg.logger.debug(f\"Row: {row.to_dict()}\")\n",
    "    for pub in pubs:\n",
    "        dg.logger.debug(f\"Processing publication: {pub}\")\n",
    "        new_row = row.to_dict()\n",
    "        new_row['publication'] = pub\n",
    "        new_row['publication_url'] = pub['url']\n",
    "        gt_df = pd.concat([gt_df, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.logger.info(f\"Total ground truth entries: {len(gt_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e29d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(set(gt_df['publication_url'].tolist()))\n",
    "# drop None or empty URLs\n",
    "urls = [url for url in urls if url]\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a24bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_path=f'scripts/NYU_data_catalog/batch_requests_openai_FDR_final_3.jsonl'\n",
    "ret_file='scripts/NYU_data_catalog/resp_FDR_89.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.run_integrated_batch_processing(\n",
    "    urls,\n",
    "    batch_file_path,\n",
    "    api_provider='openai',\n",
    "    prompt_name='GPT_FDR_FewShot_shortDescr',\n",
    "    response_format=dataset_response_schema_with_use_description_and_short,\n",
    "    submit_immediately=False,\n",
    "    batch_description='Prompting the Market? Batch Test 4',\n",
    "    grobid_for_pdf=True,\n",
    "    #local_fetch_file='scripts/exp_input/nyu_data_catalog_publications.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffd63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scripts/NYU_data_catalog/custom_id_src_mapping.json\") as f:\n",
    "    dg.custom_id_to_source_url = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc85992",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scripts/NYU_data_catalog/custom_id_src_mapping.json\", \"w\") as f:\n",
    "    json.dump(dg.custom_id_to_source_url, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351630d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chunking and submission - NO monitoring or result combination\n",
    "result = dg.split_jsonl_and_submit(\n",
    "    batch_file_path=batch_file_path,\n",
    "    max_file_size_mb=200.0,\n",
    "    api_provider='openai',\n",
    "    wait_between_submissions=30,\n",
    "    batch_description=f\"NYU Data Catalog GROBID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id_89 = 'batch_69530b9874ac819099b68c7179a3dd20'\n",
    "batch_id_95 = 'batch_69530ce762088190ab7217863e2d2ac2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab380c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.init_parser_by_input_type('XML')\n",
    "dg.custom_id_to_source_url = json.load(open(\"scripts/NYU_data_catalog/custom_id_src_mapping.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dg.parser.llm_client.download_batch_results(\n",
    "    batch_id=batch_id_95,\n",
    "    output_file_path='scripts/NYU_data_catalog/resp_FDR_95.jsonl',\n",
    "    api_provider='openai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = dg.from_batch_resp_file_to_df(\n",
    "    'scripts/NYU_data_catalog/resp_FDR_89.jsonl', output_file_path=ret_file, skip_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72847721",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.concat([pd.read_csv('scripts/NYU_data_catalog/supplementary_materials_metadata.csv'), res_df], axis=0)\n",
    "res_df.to_csv(ret_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(ret_file)\n",
    "dg.logger.info(f\"ret_file: {ret_file}, len(res_df): {len(res_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for i, gt_row in gt_df.iterrows():\n",
    "    for access_loc in gt_row['data_locations']:\n",
    "        new_row = gt_row.to_dict()\n",
    "        if 'accession_number' in access_loc:\n",
    "            new_row['identifier'] = access_loc['accession_number']\n",
    "        if 'data_access_url' in access_loc:\n",
    "            new_row['dataset_webpage'] = access_loc['data_access_url']\n",
    "        final_df = pd.concat([final_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "final_df.dropna(subset=['publication_url'], how='all', inplace=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ffdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dict mapping to a file\n",
    "#with open('scripts/NYU_data_catalog/redirect_mapping.json', 'w') as f:\n",
    "#    json.dump(dg.data_fetcher.redirect_mapping, f)\n",
    "\n",
    "red_map = json.load(open('scripts/NYU_data_catalog/redirect_mapping.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2563dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_recall = 0 \n",
    "\n",
    "ret_reportage = {'accession_ids': [], 'dataset_pages': [], 'missed_datasets': [], 'missed_accession_numbers': [], 'missed_dataset_pages': [], 'missed_dataset_names': [], \n",
    "                    'no_access_cnt': 0, 'data_request_form': 0}\n",
    "\n",
    "# in res_df for rows without url, copy source_url to url\n",
    "res_df['url'] = res_df.apply(lambda row: row['source_url'] if pd.isna(row['url']) or row['url']=='' else row['url'], axis=1)\n",
    "# also change hhtp prefix to https\n",
    "res_df['url'] = res_df['url'].apply(lambda x: x.replace('http://', 'https://') if isinstance(x, str) and x.startswith('http://') else x)\n",
    "\n",
    "for i, gt_row in gt_df.iterrows():\n",
    "    recall, alt_ids = 0, []\n",
    "    dg.logger.info(f\"Processing ground truth row {i+1},\\npublication URL: {gt_row['publication_url']},\\nciting dataset: {gt_row['title']}\")\n",
    "    cont = gt_row.to_dict()\n",
    "    dg.logger.info(f\"Alternate titles: {gt_row['dataset_alternate_titles']}\")\n",
    "    dg.logger.info(f\"Data locations: {gt_row['data_locations']}\")\n",
    "    dg.logger.debug(f\"Related Datasets: {gt_row['related_datasets']}\")\n",
    "    dg.logger.debug(f\"Other reources: {gt_row['other_resources']}\")\n",
    "    dg.logger.debug(f\"Publishers: {gt_row['publishers']}\")\n",
    "\n",
    "    alt_ids.append(gt_row['title'])\n",
    "    alt_ids.extend(gt_row['dataset_alternate_titles'])\n",
    "\n",
    "    accession_ids = [gt_row['data_locations'][k]['accession_number'] for k in range(len(gt_row['data_locations'])) if 'accession_number' in gt_row['data_locations'][k]]\n",
    "    ret_reportage['accession_ids'].extend(accession_ids)\n",
    "\n",
    "    accession_urls = [gt_row['data_locations'][k]['data_access_url'] for k in range(len(gt_row['data_locations'])) if 'data_access_url' in gt_row['data_locations'][k]]\n",
    "    ret_reportage['dataset_pages'].extend(accession_urls)\n",
    "    \n",
    "    ret_reportage['data_request_form']+=len(['1' for k in range(len(gt_row['data_locations'])) if gt_row['data_locations'][k] and 'Data Request Form' in gt_row['data_locations'][k]]['data_location'])\n",
    "\n",
    "    mapped_src = red_map.get(gt_row['publication_url'], '')\n",
    "    dg.logger.info(f\"Redirect mapping: {mapped_src}\")\n",
    "\n",
    "    pub_link = gt_row['publication_url']\n",
    "    if isinstance(pub_link, str):\n",
    "        if pub_link.startswith('http://'):\n",
    "            pub_link = pub_link.replace('http://', 'https://')\n",
    "        if 'labs/pmc/articles/' in pub_link:\n",
    "            pub_link = pub_link.replace('labs/pmc/articles/', 'pmc/articles/')\n",
    "\n",
    "    if (pub_link and 'doi.org/10.1016/' in pub_link) or (mapped_src and 'doi.org/10.1016/' in mapped_src):\n",
    "        ret_reportage['no_access_cnt'] += 1\n",
    "        continue\n",
    "\n",
    "    # match articles from res_df that have either source_url or url column matching publication_url\n",
    "    pred_row = res_df[res_df['source_url'] == pub_link]\n",
    "    pred_row = pd.concat([pred_row, res_df[res_df['url'] == pub_link]], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "    dg.logger.info(f\"Mapped source preds count: {len(res_df[res_df['source_url'] == mapped_src])}\")\n",
    "\n",
    "    if (pub_link and not re.search(r'PMC\\d+',pub_link, re.IGNORECASE)) or not re.search(r'PMC\\d+', mapped_src, re.IGNORECASE):\n",
    "        dg.logger.info(f\"Non-PMC publication URL: {pub_link} or mapped URL: {mapped_src}\")\n",
    "        #continue\n",
    "\n",
    "    pred_row = pd.concat([res_df[res_df['source_url'] == mapped_src], pred_row.reset_index(drop=True)], axis=0) if pub_link in red_map else pred_row\n",
    "    dg.logger.info(f\"Predicted row count: {len(pred_row)}\")\n",
    "    dg.logger.info(f\"Predicted row content: {pred_row.to_dict(orient='records')}\")\n",
    "\n",
    "    for j, row in pred_row.iterrows():\n",
    "        dg.logger.debug(f\"Evaluating predicted row {j+1} for ground truth row {i+1}\")\n",
    "        gt_title = gt_row['title']\n",
    "        pred_title = row.get('dataset_identifier', row.get('title', ''))\n",
    "        pred_repo = row.get('data_repository', '')\n",
    "\n",
    "        dg.logger.info(f\"GT Title: {gt_title}\")\n",
    "        dg.logger.info(f\"Pred Title: {pred_title}\")\n",
    "\n",
    "        # Check exact match or if in alternate titles\n",
    "        if type(pred_title) == str and (gt_title == pred_title or pred_title in gt_row['dataset_alternate_titles'] or gt_title in pred_title):\n",
    "            dg.logger.info(f\"Title match found: {gt_title}\")\n",
    "            recall = 1\n",
    "            break\n",
    "\n",
    "        elif type(pred_title) == str and pred_title.lower() in [acc.lower() for acc in accession_ids if acc]:\n",
    "            dg.logger.info(f\"Title match found via accession number: {pred_title}\")\n",
    "            recall = 1\n",
    "            break\n",
    "\n",
    "        elif isinstance(pred_repo, str) and pred_repo.lower() in [acc_url.lower() for acc_url in accession_urls if acc_url]:\n",
    "            dg.logger.info(f\"Dataset match found via accession URL: {pred_repo}\")\n",
    "            recall = 1\n",
    "            break\n",
    "\n",
    "        elif isinstance(pred_repo, str):\n",
    "            for acc in accession_urls:\n",
    "                if acc and pred_repo.lower() in acc.lower() or acc.lower() in pred_repo.lower():\n",
    "                    dg.logger.info(f\"Dataset partial match found via accession number in data repository: {pred_repo}\")\n",
    "                    recall = 1\n",
    "                    break\n",
    "\n",
    "        # Check substring matches with alternate titles\n",
    "        dg.logger.info(f\"Checking alternate titles for matches. possible ids: {set(alt_ids)}\")\n",
    "        for candidate_id in set(alt_ids):\n",
    "            dg.logger.info(f\"Checking alternate title candidate: {candidate_id}\")\n",
    "            if candidate_id and isinstance(candidate_id, str) and isinstance(pred_title, str):\n",
    "                if candidate_id in pred_title or pred_title in candidate_id:\n",
    "                    dg.logger.info(f\"Title match found via alternate titles: {candidate_id}\")\n",
    "                    recall = 1\n",
    "                    break\n",
    "        \n",
    "        if recall == 1:\n",
    "            break\n",
    "    \n",
    "    if recall == 0:\n",
    "        dg.logger.info(f\"No title match found for ground truth title: {gt_title}\")\n",
    "        ret_reportage['missed_datasets'].append(gt_row.to_dict())\n",
    "        ret_reportage['missed_accession_numbers'].extend([acc_id for acc_id in accession_ids if acc_id])\n",
    "        ret_reportage['missed_dataset_pages'].extend([url for url in accession_urls if url])\n",
    "        ret_reportage['missed_dataset_names'].extend([name for name in alt_ids if name])\n",
    "    \n",
    "    tot_recall += recall\n",
    "\n",
    "dg.logger.info(f\"Total recall: {tot_recall/(i+1-ret_reportage['no_access_cnt'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.logger.info(f\"ret_reportage: {ret_reportage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c597303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17106026",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_append = []\n",
    "for i, gt_row in gt_df.iterrows():\n",
    "    url = gt_row['publication_url']\n",
    "    if url and url.startswith('https://pmc.ncbi.nlm.nih.gov/'):\n",
    "        redirect_append.append(url)\n",
    "len(redirect_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46bae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6ded7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25eaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = evaluate_performance_dev(\n",
    "    res_df,\n",
    "    final_df,\n",
    "    dg,\n",
    "    'scripts/output/false_positives.txt', \n",
    "    false_negatives_file='scripts/output/false_negatives.txt',\n",
    "    repo_return=True,\n",
    "    gt_base = final_df['publication_url'].unique()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28993b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[res_df['source_url'] == 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11460830/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301fe588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
