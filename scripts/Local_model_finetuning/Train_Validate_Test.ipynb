{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227fc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re, json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a652b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_articles_pmcids = \"scripts/exp_input/REV.txt\"\n",
    "\n",
    "ground_truth_complete = \"scripts/Local_model_finetuning/ground_truth/gt_dataset_info_extraction_from_snippet.xlsx\"\n",
    "ground_truth_no_dspage = \"scripts/Local_model_finetuning/ground_truth/gt_dataset_info_no_dspage_extraction_from_snippet.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_articles_pmcids, 'r') as f:\n",
    "    pmc_links = f.read().splitlines()\n",
    "\n",
    "print(\"Total number of PMCIDs:\", len(pmc_links))\n",
    "\n",
    "\n",
    "train_pmc_links, test_pmc_links = train_test_split(pmc_links, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_pmc_links)}\")\n",
    "print(f\"Test set: {len(test_pmc_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.read_excel(ground_truth_no_dspage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame based on train and test PMC links\n",
    "train_df = train_test_df[train_test_df['url'].isin(train_pmc_links)]\n",
    "test_df = train_test_df[train_test_df['url'].isin(test_pmc_links)]\n",
    "\n",
    "print(f\"Original DataFrame: {len(train_test_df)} rows\")\n",
    "print(f\"Train DataFrame: {len(train_df)} rows\")\n",
    "print(f\"Test DataFrame: {len(test_df)} rows\")\n",
    "print(f\"Total matched: {len(train_df) + len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c927074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the input and output text to understand the task\n",
    "print(\"Sample input_text:\")\n",
    "print(train_df['input_text'].iloc[0][:500])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Sample output_text:\")\n",
    "print(train_df['output_text'].iloc[0][:500])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(f\"Average input length: {train_df['input_text'].str.len().mean():.0f} chars\")\n",
    "print(f\"Average output length: {train_df['output_text'].str.len().mean():.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de26dc7",
   "metadata": {},
   "source": [
    "# Model Training Strategy\n",
    "\n",
    "## Task: Dataset Information Extraction\n",
    "Extract structured dataset identifiers and repository references from scientific text.\n",
    "\n",
    "#### Approach: **Fine-tuned T5/Flan-T5** (Recommended)\n",
    "- Designed for text-to-text generation\n",
    "- Good at structured output\n",
    "- Fast inference\n",
    "- Model: `google/flan-t5-base` (250M params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bdc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in the format needed for training\n",
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    \"\"\"Convert DataFrame to HuggingFace Dataset format, filtering out NaN values\"\"\"\n",
    "    # Remove rows where input_text or output_text is NaN\n",
    "    df_clean = df.dropna(subset=['input_text', 'output_text']).copy()\n",
    "    \n",
    "    # Convert to string to ensure all values are strings\n",
    "    df_clean['input_text'] = df_clean['input_text'].astype(str)\n",
    "    df_clean['output_text'] = df_clean['output_text'].astype(str)\n",
    "    \n",
    "    data = {\n",
    "        'input': df_clean['input_text'].tolist(),\n",
    "        'output': df_clean['output_text'].tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"  Filtered out {len(df) - len(df_clean)} rows with missing values\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "print(\"Preparing train dataset...\")\n",
    "train_dataset = prepare_dataset(train_df)\n",
    "\n",
    "print(\"\\nPreparing test dataset...\")\n",
    "test_dataset = prepare_dataset(test_df)\n",
    "\n",
    "print(f\"\\nTrain dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")\n",
    "print(\"\\nSample:\")\n",
    "print(f\"Input: {train_dataset[0]['input'][:200]}...\")\n",
    "print(f\"Output: {train_dataset[0]['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c73f8",
   "metadata": {},
   "source": [
    "# Fine-tune Flan-T5\n",
    "\n",
    "This approach uses:\n",
    "- **Model**: `google/flan-t5-base` - instruction-tuned T5 model\n",
    "- **Training**: Parameter-efficient fine-tuning\n",
    "- **Framework**: HuggingFace Transformers + Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q transformers datasets accelerate evaluate rouge-score sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09faf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"  # 250M parameters\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and outputs\"\"\"\n",
    "    # Add task prefix to help the model understand the task\n",
    "    inputs = [\"Extract dataset information: \" + doc for doc in examples['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=False)\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(text_target=examples['output'], max_length=256, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "print(f\"Tokenized train dataset: {len(tokenized_train)} examples\")\n",
    "print(f\"Tokenized test dataset: {len(tokenized_test)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute ROUGE scores for evaluation\"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Replace -100 in labels (padding token)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract F1 scores\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "    \n",
    "    # Compute exact match (for structured output)\n",
    "    exact_match = sum([p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n",
    "    result[\"exact_match\"] = round(exact_match * 100, 2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training arguments\n",
    "output_dir = \"./results/flan-t5-dataset-extraction\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,  # Reduced from 8 to 4 for memory\n",
    "    per_device_eval_batch_size=4,   # Reduced from 8 to 4 for memory\n",
    "    gradient_accumulation_steps=4,  # Increased to 4 to maintain effective batch size = 16\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Generation settings for evaluation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Save settings\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"exact_match\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=False,  # Set to True if you have GPU with fp16 support\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training samples: {len(tokenized_train)}\")\n",
    "print(f\"Evaluation samples: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef29745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(f\"{output_dir}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "print(f\"\\nModel saved to {output_dir}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "# Test inference on a few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(test_dataset))):\n",
    "    input_text = test_dataset[i]['input']\n",
    "    expected_output = test_dataset[i]['output']\n",
    "    \n",
    "    # Prepare input\n",
    "    input_ids = tokenizer(\n",
    "        \"Extract dataset information: \" + input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).input_ids\n",
    "    \n",
    "    # Generate prediction\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=256,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {input_text[:200]}...\")\n",
    "    print(f\"Expected: {expected_output}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Match: {'✓' if prediction.strip() == expected_output.strip() else '✗'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
