{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T15:26:11.485223Z",
     "start_time": "2025-03-18T15:26:04.715441Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import from the files in this directory\n",
    "from dotenv import load_dotenv\n",
    "from data_gatherer import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61037ca47d2f13d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:29.357500Z",
     "start_time": "2025-03-15T08:41:29.354784Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.remove(\"exp_input/fetched_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38134199deb3f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:32.807113Z",
     "start_time": "2025-03-15T08:41:29.358837Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "config_path = 'config_experiment.json'\n",
    "orchestrator = DataGatherer(config_path)\n",
    "\n",
    "ground_truth_src = \"exp_input/dataset_citation_records_Table.parquet\"\n",
    "fetched_data_path = \"exp_input/fetched_data.parquet\"\n",
    "dataset_table = \"exp_input/Table_datasets.parquet\"\n",
    "\n",
    "orchestrator.logger.info(f\"Ground Truth file exists: {os.path.exists(ground_truth_src)}\")\n",
    "orchestrator.logger.info(f\"ground_truth_X_y file exists: {os.path.exists(fetched_data_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb73dae7df71d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:33.240211Z",
     "start_time": "2025-03-15T08:41:32.816132Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ground_truth_src = pd.read_parquet(ground_truth_src)\n",
    "orchestrator.logger.info(f\"len ground_truth: {len(df_ground_truth_src)}\")\n",
    "df_ground_truth_src.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0796baf29ce93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:35.265715Z",
     "start_time": "2025-03-15T08:41:33.244180Z"
    }
   },
   "outputs": [],
   "source": [
    "# raw_data_v1.csv \\ PRIDEid_HTML_data.csv is the file containing the old data\n",
    "try:\n",
    "    df_old = pd.read_parquet(fetched_data_path)\n",
    "    \n",
    "    if \"publication\" in df_old.columns:\n",
    "        df_old.set_index(\"publication\",inplace=True, drop=False) \n",
    "        \n",
    "    orchestrator.logger.info(f\"File found: {df_old.columns}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    df_old = pd.DataFrame(columns=['citing_publication_link','identifier','repository','raw_html'])\n",
    "\n",
    "    orchestrator.logger.info(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3153217696ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distinct values in dataset_uids\n",
    "flat_list = [item for sublist in df_old['dataset_uid'].dropna().str.split(',') for item in sublist]\n",
    "n_datasets = len(set(flat_list))\n",
    "print(f\"# of Datasets already added: {n_datasets}\")\n",
    "print(f\"# of Publications already added: {len(df_old)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8510527feb276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:42.661794Z",
     "start_time": "2025-03-15T08:41:35.280128Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ground_truth_src = df_ground_truth_src[df_ground_truth_src['citation_record_from_doi']==0].drop(['citation_record_from_doi'], axis=1)\n",
    "df_ground_truth_src = df_ground_truth_src.groupby('citing_publication_link').agg({\n",
    "        'citing_publication_link': 'first',  # Keep the first publication,\n",
    "        'identifier': lambda x: ','.join(set(x)),  # Concatenate unique dataset_uids\n",
    "        'repository': lambda x: ','.join(set(x)),   # Concatenate unique repo_names\n",
    "        'citation_record_source': lambda x: ','.join(set(x)),  # Keep the first title,\n",
    "        # 'title': lambda x: ','.join(set(x)),  # Keep the first title,\n",
    "    })\n",
    "\n",
    "print(len(df_ground_truth_src))\n",
    "print(df_ground_truth_src.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ced914bcb283fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:42.677637Z",
     "start_time": "2025-03-15T08:41:42.663329Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ground_truth_src['citation_record_source'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e78db3ea957e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:47.102800Z",
     "start_time": "2025-03-15T08:41:46.954412Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 25  # Adjust this value\n",
    "\n",
    "stratified_sample = df_ground_truth_src.groupby('citation_record_source', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), sample_size), random_state=142)  # Ensure it doesn't exceed available rows\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a8299a670e7d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:47.401371Z",
     "start_time": "2025-03-15T08:41:47.110397Z"
    }
   },
   "outputs": [],
   "source": [
    "data, i = [], 0\n",
    "t0 = time.time()\n",
    "iter_max = 15000  # Limit iterations\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "for link, id, repo, record_src in stratified_sample.itertuples(index=False):\n",
    "    i += 1\n",
    "    if i >= iter_max:\n",
    "        break\n",
    "\n",
    "    if link in df_old['publication'].values:\n",
    "        orchestrator.logger.info(f\"Skipping publication {link} (already in data)\")\n",
    "        i-=1  # Decrement i to ensure we still process the same number of links\n",
    "        continue\n",
    "\n",
    "    orchestrator.logger.info(f\"Processing URL: {link}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(link, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise error for HTTP failures (4xx, 5xx)\n",
    "\n",
    "        raw_data = response.text  # Extract HTML content\n",
    "\n",
    "        data.append({\n",
    "            \"publication\": link,\n",
    "            \"src_website\": 'ncbi',\n",
    "            \"dataset_uid\": id,\n",
    "            \"repo_name\": repo,\n",
    "            \"raw_html\": raw_data\n",
    "        })\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        orchestrator.logger.error(f\"Error fetching URL {link}: {e}\", exc_info=True)\n",
    "\n",
    "    # Log progress every 100 iterations\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        elapsed = time.time() - t0\n",
    "        eta = (elapsed / (i + 1)) * (len(stratified_sample) - i - 1)\n",
    "        orchestrator.logger.info(f\"\\nProgress {i+1}/{len(stratified_sample)}. ETA {time.strftime('%H:%M:%S', time.gmtime(eta))}\\n\")\n",
    "    \n",
    "    time.sleep(0.5)  # Optional: Add a delay to avoid overwhelming the server\n",
    "\n",
    "print(f\"Time elapsed for {len(data)} iterations: {time.strftime('%H:%M:%S', time.gmtime(time.time() - t0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b1674b84aec54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:47.426522Z",
     "start_time": "2025-03-15T08:41:47.406173Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "if 'publication' in df.columns:  \n",
    "    df.set_index(\"publication\",inplace=True, drop=False)\n",
    " \n",
    "if \"smallest_elements\" in df.columns:    \n",
    "    df[\"smallest_elements\"] = df[\"smallest_elements\"].apply(\n",
    "        lambda x: json.dumps(x) if isinstance(x, list) else (x if isinstance(x, str) else json.dumps([]))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce672b5b05c27f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:47.443806Z",
     "start_time": "2025-03-15T08:41:47.431026Z"
    }
   },
   "outputs": [],
   "source": [
    "# concat the new data with the old data\n",
    "print(len(df_old))\n",
    "print(len(df))\n",
    "df_merged = pd.concat([df_old, df], ignore_index=True)\n",
    "print(len(df_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af0bc91f28a859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:48.396437Z",
     "start_time": "2025-03-15T08:41:47.449409Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt, not_cnt, mask = 0,0, []\n",
    "df['identifiers_in_HTML'] = ''\n",
    "\n",
    "for pub,row in df_merged.iterrows():\n",
    "    keep = True\n",
    "    ids = row['dataset_uid'].split(',')\n",
    "    for id in ids:\n",
    "        if id in row['raw_html']:\n",
    "            cnt +=1\n",
    "            # append the id to  the row identifiers_in_HTML\n",
    "            df_merged.at[pub,'identifiers_in_HTML'] = df_merged.at[pub,'identifiers_in_HTML'] + ',' + id\n",
    "        else:\n",
    "            #print(f\"ID {id} not found in {pub}\")\n",
    "            not_cnt +=1\n",
    "            # drop row from df\n",
    "            keep = False\n",
    "    \n",
    "    mask.append(keep)\n",
    "    \n",
    "print(cnt, not_cnt)\n",
    "print(len(df_merged))\n",
    "print(len(mask))\n",
    "print(sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5be2a86596f73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:48.405356Z",
     "start_time": "2025-03-15T08:41:48.398148Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in df_merged.columns:\n",
    "    df_merged[col] = df_merged[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cad9fc11f01bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:48.412572Z",
     "start_time": "2025-03-15T08:41:48.407230Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_merged.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ee5662a254bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:41:48.418764Z",
     "start_time": "2025-03-15T08:41:48.414691Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"df_merged length: {len(df_merged)}\")\n",
    "print(f\"mask length: {len(mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225c9df220ff71c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:42:09.719104Z",
     "start_time": "2025-03-15T08:42:09.711105Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = df_merged[mask].copy()\n",
    "print(len(df_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e377d23de3af11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:42:18.101761Z",
     "start_time": "2025-03-15T08:42:14.529774Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.to_parquet(fetched_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282be356ded5f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8f05a78645d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854a6ece14ab528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51534593b20837c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b9d283303f667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5af18334d7280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12fe63d11c896e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3431d6e5ace1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:11:02.544783Z",
     "start_time": "2025-03-15T08:11:02.538362Z"
    }
   },
   "outputs": [],
   "source": [
    "# # let's run a quick check\n",
    "# ok, not_ok = 0,0 \n",
    "# iteration = 0\n",
    "# dataset_table = pd.read_parquet(dataset_table)\n",
    "# \n",
    "# for i,row in df_merged.iterrows():\n",
    "#     iteration+=1\n",
    "#     pub = row['publication'].lower()\n",
    "#     ids = set(sorted(row['dataset_uid'].split(',')))\n",
    "#     orchestrator.logger.debug(f\"Publication URL: {pub}, uids: {ids}\")\n",
    "#     \n",
    "#     if iteration%(len(df_merged)//20)==0:\n",
    "#         orchestrator.logger.debug(f\"Progress {iteration}/{len(df_merged)}\")\n",
    "#     \n",
    "#     matching_ids = dataset_table[dataset_table['citing_publications_links'] == pub]['identifier'].values\n",
    "#     orchestrator.logger.debug(f\"Matching row: {matching_ids}\")\n",
    "#     \n",
    "#     ground_truth = set(','.join(sorted(matching_ids)).split(','))\n",
    "#                     \n",
    "#     # set comparison\n",
    "#     if ground_truth == ids:\n",
    "#         ok+=1\n",
    "#         \n",
    "#     else:\n",
    "#         not_ok+=1\n",
    "#         orchestrator.logger.info(f\"Publication URL: {pub}\")\n",
    "#         orchestrator.logger.info(f\"Value found in source data: {ground_truth}\")\n",
    "#         orchestrator.logger.info(f\"Value found in merged data: {ids}\")\n",
    "#         #raise ValueError(f\"ERROR: Count mismatch for {pub}\")\n",
    "#         \n",
    "#         update_value=','.join(ground_truth)\n",
    "#         orchestrator.logger.info(f\"matching_ids: {update_value}\")\n",
    "#         df_merged.at[pub,'dataset_uid'] = update_value\n",
    "#     \n",
    "#     \n",
    "# orchestrator.logger.info(f\"Check completed. {ok} publications good. {not_ok} errors found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc58f9f583c8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, i = [], 0\n",
    "# \n",
    "# t0 = time.time()\n",
    "# iter_max = 1500  # Limit iterations\n",
    "# \n",
    "# for link, id, repo, record_src in stratified_sample.itertuples(index=False):\n",
    "#     i+=1\n",
    "#     if i >= iter_max:\n",
    "#         break\n",
    "# \n",
    "#     if link in df_old['citing_publication_link'].values:\n",
    "#         orchestrator.logger.info(f\"Skipping publication {link} (already in data)\")\n",
    "#         continue\n",
    "#     \n",
    "#     orchestrator.logger.info(f\"Processing URL: {link}\")\n",
    "# \n",
    "#     adjusted_url_for_fetch = link\n",
    "#     orchestrator.data_fetcher = orchestrator.data_fetcher.update_DataFetcher_settings(\n",
    "#         link, orchestrator.full_DOM, orchestrator.logger\n",
    "#     )\n",
    "# \n",
    "#     try:\n",
    "#         raw_data = orchestrator.data_fetcher.fetch_data(link)\n",
    "#         doi = orchestrator.data_fetcher.url_to_doi(link)\n",
    "# \n",
    "#         orchestrator.publisher = orchestrator.data_fetcher.url_to_publisher_domain(\n",
    "#             orchestrator.data_fetcher.scraper_tool.current_url\n",
    "#         )\n",
    "# \n",
    "#         if orchestrator.publisher == \"biorxiv\":\n",
    "#             adjusted_url_for_fetch = orchestrator.data_fetcher.scraper_tool.current_url + \".full\"\n",
    "#             raw_data = orchestrator.data_fetcher.fetch_data(adjusted_url_for_fetch)\n",
    "# \n",
    "#         elif orchestrator.publisher == \"pubmed\":\n",
    "#             PMC_ID = orchestrator.data_fetcher.get_PMCID_from_pubmed_html(raw_data)\n",
    "#             if PMC_ID:\n",
    "#                 adjusted_url_for_fetch = orchestrator.data_fetcher.reconstruct_PMC_link(PMC_ID)\n",
    "#                 raw_data = orchestrator.data_fetcher.fetch_data(adjusted_url_for_fetch)\n",
    "# \n",
    "#         # smallest_elements = (\n",
    "#         #     add_example_to_merged_df(row, raw_data) if re.search(id, raw_data, re.IGNORECASE) else \"n/a\"\n",
    "#         # )\n",
    "# \n",
    "#         data.append({\n",
    "#             \"publication\": link, \n",
    "#             \"fetch_from\": adjusted_url_for_fetch.lower(), \n",
    "#             \"doi\": doi, \n",
    "#             \"publisher\": orchestrator.publisher,\n",
    "#             \"dataset_uid\": id, \n",
    "#             \"repo_name\": repo, \n",
    "#             \"raw_html\": raw_data, \n",
    "#             # \"smallest_elements\": smallest_elements, \n",
    "#             # \"title\": title\n",
    "#         })\n",
    "# \n",
    "#     except Exception as e:\n",
    "#         orchestrator.logger.error(f\"Error processing URL {link}: {e}\", exc_info=True)\n",
    "# \n",
    "#     # Log every 100 iterations\n",
    "#     if i % 100 == 0 and i > 0:\n",
    "#         elapsed = time.time() - t0\n",
    "#         eta = (elapsed / (i + 1)) * (len(df_ground_truth_src) - i - 1)\n",
    "#         orchestrator.logger.info(f\"\\nProgress {i+1}/{len(df_ground_truth_src)}. ETA {time.strftime('%H:%M:%S', time.gmtime(eta))}\\n\")\n",
    "# \n",
    "# # Quit WebDriver after all iterations\n",
    "# driver.quit()\n",
    "# print(f\"Time elapsed for {len(data)} iterations: {time.strftime('%H:%M:%S', time.gmtime(time.time() - t0))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
