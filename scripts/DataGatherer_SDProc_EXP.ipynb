{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:30:53.289617Z",
     "start_time": "2025-09-04T16:30:29.500758Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data_gatherer.data_gatherer import DataGatherer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2b2bc4106d2ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:31:15.898169Z",
     "start_time": "2025-09-04T16:31:15.880961Z"
    }
   },
   "outputs": [],
   "source": [
    "input_file = \"scripts/exp_input/REV.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca05a29b7c100d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:31:17.295747Z",
     "start_time": "2025-09-04T16:31:17.291744Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"gemini-2.0-flash\"  # \"gemini-2.0-flash\" or \"gpt-4o-mini\"\n",
    "prompt = \"GPT_FewShot\"  # \"GPT_from_full_input_Examples\" or \"GPT_FewShot\"\n",
    "FDR = False\n",
    "semantic_retrieval = True\n",
    "section_filter= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae25cbd54d4fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:31:19.865917Z",
     "start_time": "2025-09-04T16:31:19.860295Z"
    }
   },
   "outputs": [],
   "source": [
    "# write list to a text file\n",
    "with open(input_file, 'r') as f:\n",
    "    pmcids = f.read().splitlines()[:20]\n",
    "\n",
    "print(\"Number of PMCIDs:\", len(pmcids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79cbd7265b497e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:31:22.492692Z",
     "start_time": "2025-09-04T16:31:22.479800Z"
    }
   },
   "outputs": [],
   "source": [
    "dg = DataGatherer(\n",
    "    llm_name=model_name, \n",
    "    log_level='INFO', \n",
    "    process_entire_document=FDR, \n",
    "    driver_path='../Firefox/geckodriver', \n",
    "    save_to_cache=False, \n",
    "    load_from_cache=False,\n",
    "    full_output_file=\"scripts/output/result.csv\"\n",
    ") #, save_dynamic_prompts=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e0500474a6c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:31:25.329784Z",
     "start_time": "2025-09-04T16:31:25.325226Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the problematic urls for information extraction for GEMINI\n",
    "#urls = [\"https://pmc.ncbi.nlm.nih.gov/articles/PMC9314356\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC4318527/\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11929800/\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11032436/\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10802452/\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8565335/\",\"https://pmc.ncbi.nlm.nih.gov/articles/PMC7778917/\"]\n",
    "# v1 = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7029360/\"\n",
    "# v2 = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8006362\"\n",
    "#urls = [\"https://pmc.ncbi.nlm.nih.gov/articles/PMC9710693/\"]\n",
    "\n",
    "# These are the problematic urls for information extraction for GPT - (2025 Aug 29) GPT gets stuck for some reason after 50 calls or so...\n",
    "#urls = [\"https://pmc.ncbi.nlm.nih.gov/articles/PMC11240079\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC5752539\",\"https://pmc.ncbi.nlm.nih.gov/articles/PMC7032692\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10547713\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8102856\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8055881\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8226229\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10769298\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11659981\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8082263\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8565335\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC9094742\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7029360\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC6289083\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7658217\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC9780309\",\"https://pmc.ncbi.nlm.nih.gov/articles/PMC3788619\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8859891\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10680627\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC6323985\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8131595\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11420198\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC9915613\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10238095\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10802452\" , \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11032436\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10329279\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10836119\",  \"https://pmc.ncbi.nlm.nih.gov/articles/PMC9280291\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11661334\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC5161470\", \"https://pmc.ncbi.nlm.nih.gov/articles/PMC4339277\"]\n",
    "\n",
    "#out = dg.process_articles(urls, semantic_retrieval=semantic_retrieval, section_filter=section_filter, prompt_name=prompt)\n",
    "\n",
    "#out\n",
    "#out['https://pmc.ncbi.nlm.nih.gov/articles/PMC9710693']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0572331e32fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T17:36:22.343172Z",
     "start_time": "2025-09-04T16:31:29.415907Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = dg.run(input_file=pmcids, semantic_retrieval=semantic_retrieval, section_filter=section_filter, prompt_name=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1be22241ced6b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#combined_df = pd.read_csv(\"scripts/output/result.csv\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d7d5f299551c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T21:20:38.136808Z",
     "start_time": "2025-09-04T21:20:38.134052Z"
    }
   },
   "outputs": [],
   "source": [
    "#ground_truth = pd.read_parquet(\"scripts/exp_input/dataset_citation_records_Table.parquet\")\n",
    "ground_truth = pd.read_parquet(\"scripts/output/gold/dataset_citation_records_Table.parquet\")\n",
    "#ground_truth = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadaf8654a9d103d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T21:20:38.857028Z",
     "start_time": "2025-09-04T21:20:38.851074Z"
    }
   },
   "outputs": [],
   "source": [
    "ground_truth.head()\n",
    "# rename colum synid to 'identifier'\n",
    "#ground_truth = ground_truth.rename(columns={'synid': 'identifier'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f880224783dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T21:26:52.754959Z",
     "start_time": "2025-09-04T21:26:52.722141Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(predict_df, ground_truth, orchestrator, false_positives_file, false_negatives_file=None):\n",
    "    \"\"\" Evaluates dataset extraction performance using precision, recall, and F1-score. \"\"\"\n",
    "\n",
    "    recall_list, false_positives_output, false_negatives_output = [], [], []\n",
    "    total_precision, total_recall, num_sources = 0, 0, 0\n",
    "\n",
    "    for source_page in predict_df['source_url'].unique():\n",
    "        #pub_id = source_page.split('/')[-1].lower()\n",
    "        pub_id = source_page\n",
    "        \n",
    "        orchestrator.logger.info(f\"Evaluating pub_id: {pub_id}\")\n",
    "        #gt_data = ground_truth[ground_truth['pmcid'].str.lower() == pub_id.lower()]  # extract ground truth\n",
    "        gt_data = ground_truth[ground_truth['citing_publication_link'].str.lower() == pub_id.lower()]  # extract ground truth\n",
    "\n",
    "        gt_datasets = set()\n",
    "        for dataset_string in gt_data['identifier'].dropna().str.lower():\n",
    "            gt_datasets.update(dataset_string.split(','))  # Convert CSV string into set of IDs\n",
    "\n",
    "        orchestrator.logger.info(f\"# of elements in gt_data: {len(gt_data)}. Element IDs: {gt_datasets}\")\n",
    "\n",
    "        num_sources += 1\n",
    "\n",
    "        # Extract evaluation datasets for this source page\n",
    "        eval_data = predict_df[predict_df['source_url'] == source_page]\n",
    "        eval_datasets = set(eval_data['dataset_identifier'].dropna().str.lower())\n",
    "        # Remove invalid entries\n",
    "        eval_datasets.discard('n/a')\n",
    "        eval_datasets.discard('')\n",
    "\n",
    "        orchestrator.logger.info(f\"Evaluation datasets: {eval_datasets}\")\n",
    "\n",
    "        # Handle cases where both ground truth and evaluation are empty\n",
    "        if not gt_datasets and not eval_datasets:\n",
    "            orchestrator.logger.info(\"No datasets in both ground truth and evaluation. Perfect precision and recall.\")\n",
    "            total_precision += 1\n",
    "            total_recall += 1\n",
    "            continue\n",
    "\n",
    "        # Match Extraction Logic\n",
    "        matched_gt, matched_eval = set(), set()\n",
    "\n",
    "        # Exact Matches\n",
    "        exact_matches = gt_datasets & eval_datasets  # Intersection of ground truth and extracted datasets\n",
    "        matched_gt.update(exact_matches)\n",
    "        matched_eval.update(exact_matches)\n",
    "\n",
    "        # Partial Matches (Aliased Identifiers)\n",
    "        for eval_id in eval_datasets - matched_eval:\n",
    "            for gt_id in gt_datasets - matched_gt:\n",
    "                if eval_id in gt_id or gt_id in eval_id:  # Partial match or alias\n",
    "                    orchestrator.logger.info(f\"Partial or alias match found: eval_id={eval_id}, gt_id={gt_id}\")\n",
    "                    matched_gt.add(gt_id)\n",
    "                    matched_eval.add(eval_id)\n",
    "                    break  # Stop once matched\n",
    "\n",
    "        # **False Positives (Unmatched extracted datasets)**\n",
    "        FP = eval_datasets - matched_eval\n",
    "        false_positives_output.extend([false_p, pub_id] for false_p in FP)\n",
    "\n",
    "        # **False Negatives (Unmatched ground truth datasets)**\n",
    "        FN = gt_datasets - matched_gt\n",
    "        false_negatives_output.extend((FN, pub_id)) if len(FN) > 0 else None\n",
    "\n",
    "        # **Precision and Recall Calculation**\n",
    "        true_positives = len(matched_gt)\n",
    "        false_positives = len(FP)\n",
    "        false_negatives = len(FN)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "        orchestrator.logger.info(f\"Precision for {source_page}: {precision}\")\n",
    "        orchestrator.logger.info(f\"Recall for {source_page}: {recall}\")\n",
    "\n",
    "        if recall == 0:\n",
    "            recall_list.append(source_page)\n",
    "\n",
    "        # Accumulate totals\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "\n",
    "    # **Compute Overall Metrics**\n",
    "    average_precision = total_precision / num_sources if num_sources > 0 else 0\n",
    "    average_recall = total_recall / num_sources if num_sources > 0 else 0\n",
    "    f1_score = (\n",
    "        2 * (average_precision * average_recall) / (average_precision + average_recall)\n",
    "        if (average_precision + average_recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    orchestrator.logger.info(f\"\\nPerformance evaluation completed for {num_sources} source pages.\")\n",
    "\n",
    "    # **Save false positives**\n",
    "    with open(false_positives_file, 'w') as f:\n",
    "        for item in false_positives_output:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    if false_negatives_file:\n",
    "        with open(false_negatives_file, 'w') as f:\n",
    "            for item in false_negatives_output:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "    return {\n",
    "        \"average_precision\": average_precision,\n",
    "        \"average_recall\": average_recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd9405ee5dcd60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T21:26:53.669418Z",
     "start_time": "2025-09-04T21:26:53.666829Z"
    }
   },
   "outputs": [],
   "source": [
    "#results = evaluate_performance(out, ground_truth, dg, \"output/false_positives.txt\")\n",
    "# PMC4318527, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae367545bb1b42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T21:26:55.530950Z",
     "start_time": "2025-09-04T21:26:54.622805Z"
    }
   },
   "outputs": [],
   "source": [
    "results = evaluate_performance(combined_df, ground_truth, dg, \"scripts/output/false_positives.txt\", \"scripts/output/false_negatives.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3dc3075cb13b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T21:25:01.310798Z",
     "start_time": "2025-09-04T21:25:01.304645Z"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37668da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
