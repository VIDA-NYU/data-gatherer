{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a039d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gatherer.data_gatherer import DataGatherer\n",
    "from data_gatherer.llm.response_schema import *\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "from lxml import etree\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01742ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.read_parquet(\"scripts/output/gold/dataset_citation_records_Table.parquet\")\n",
    "\n",
    "input_file = \"scripts/exp_input/REV.txt\"\n",
    "batch_file_path=f'scripts/tmp/train_data_openai_RTR-3_DataRef-REV.jsonl'\n",
    "output_batch_file = 'scripts/tmp/Train_results.jsonl'\n",
    "\n",
    "excel_output_path = 'scripts/tmp/training_dataset_validation.xlsx'\n",
    "\n",
    "model_name = \"gpt-4o-mini\" \n",
    "FDR = False\n",
    "semantic_retrieval = True\n",
    "brute_force_RegEx_ID_ptrs = True\n",
    "\n",
    "top_k = 3\n",
    "embeddings_retriever_model = None\n",
    "dedup = True\n",
    "prompt_name = \"GPT_FewShot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write list to a text file\n",
    "with open(input_file, 'r') as f:\n",
    "    pmcids = f.read().splitlines()\n",
    "\n",
    "print(\"Number of PMCIDs:\", len(pmcids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGatherer(\n",
    "    llm_name=model_name, \n",
    "    log_level='WARNING', \n",
    "    process_entire_document=FDR, \n",
    "    driver_path=None, \n",
    "    save_to_cache=False, \n",
    "    load_from_cache=False,\n",
    "    embeds_cache_read=True,\n",
    "    embeds_cache_write=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81352328",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('scripts/exp_input/Local_fulltext_pub_REV.parquet'):\n",
    "    publication_fulltext_df = pd.read_parquet('scripts/exp_input/Local_fulltext_pub_REV.parquet')\n",
    "    publication_fulltext = publication_fulltext_df.to_dict(orient='index')\n",
    "else:\n",
    "    publication_fulltext = dg.fetch_data(pmcids,write_df_to_path='scripts/exp_input/Local_fulltext_pub_REV.parquet')\n",
    "    publication_fulltext_df = pd.DataFrame.from_dict(publication_fulltext, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81352328",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_counts = {}\n",
    "for url, data in publication_fulltext.items():\n",
    "    if url not in pmcids:\n",
    "        continue\n",
    "    if data and 'raw_data_format' in data:\n",
    "        fmt = data['raw_data_format']\n",
    "        if fmt not in format_counts:\n",
    "            format_counts[fmt] = {'count': 0, 'urls': []}\n",
    "        format_counts[fmt]['count'] += 1\n",
    "        format_counts[fmt]['urls'].append(url)\n",
    "            \n",
    "# Log format frequencies (counts only for readability)\n",
    "frequency_summary = {fmt: info['count'] for fmt, info in format_counts.items()}\n",
    "dg.logger.info(f\"Fetched {len(publication_fulltext)} Papers. Format frequencies: {frequency_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_requests, cnt, last_url_raw_data_format = [], 0, False\n",
    "\n",
    "for url_raw_data_format, vals in format_counts.items():\n",
    "    for url in vals['urls']:\n",
    "\n",
    "        msg_already_added = ''\n",
    "        \n",
    "        data = publication_fulltext[url]\n",
    "        dg.logger.info(f\"type of data: {type(data['fetched_data'])}\")\n",
    "\n",
    "        if isinstance(data['fetched_data'], str) and url_raw_data_format.upper() == 'XML':\n",
    "            dg.logger.info(\"string data is not supported input, need etree\")\n",
    "            data['fetched_data'] = etree.fromstring(data['fetched_data'].encode('utf-8'))\n",
    "\n",
    "        try:                        \n",
    "            if cnt != 0 and url_raw_data_format == last_url_raw_data_format:\n",
    "                dg.logger.info(f\"Reusing existing parser of name: {dg.parser.__class__.__name__}\")\n",
    "            else:\n",
    "                dg.logger.info(f\"Creating new parser for format: {url_raw_data_format}\")\n",
    "                dg.init_parser_by_input_type(url_raw_data_format, data['fetched_data'], embeddings_retriever_model)\n",
    "                        \n",
    "            # Generate unique custom_id\n",
    "            article_id = dg.url_to_page_id(url)\n",
    "            pmcid = dg.data_fetcher.url_to_pmcid(url)\n",
    "            timestamp = int(time.time() * 1000)\n",
    "            custom_id = f\"{dg.llm}_{article_id}_{timestamp}\"\n",
    "            custom_id = re.sub(r'[^a-zA-Z0-9_-]', '_', custom_id)[:64]\n",
    "                        \n",
    "            if dg.full_document_read:\n",
    "                dg.logger.info(f'normalize input')\n",
    "                if url_raw_data_format.upper() == 'XML':\n",
    "                    normalized_input = (dg.parser.normalize_XML(data['fetched_data']) \n",
    "                                        if hasattr(dg.parser, 'normalize_XML') \n",
    "                                        else data['fetched_data'])\n",
    "                elif url_raw_data_format.upper() == 'HTML':\n",
    "                    normalized_input = (dg.parser.normalize_HTML(data['fetched_data']) \n",
    "                                                if hasattr(dg.parser, 'normalize_HTML') \n",
    "                                                else data['fetched_data'])\n",
    "                elif url_raw_data_format.upper() == 'PDF':\n",
    "                    normalized_input = data['fetched_data']\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported raw data format: {url_raw_data_format}\")\n",
    "                        \n",
    "            else:\n",
    "                dg.logger.info(f'relevant section retrieval')\n",
    "                data_availability_obj = dg.parser.retrieve_relevant_content(\n",
    "                                data['fetched_data'],\n",
    "                                semantic_retrieval=semantic_retrieval,\n",
    "                                top_k=top_k,\n",
    "                                skip_rule_based_retrieved_elm=dedup,\n",
    "                                include_snippets_with_ID_patterns=brute_force_RegEx_ID_ptrs,\n",
    "                                article_id=dg.data_fetcher.url_to_pmcid(url),\n",
    "                                output_format='json'\n",
    "                            )\n",
    "                dg.logger.info(f\"type of data_availability_obj: {type(data_availability_obj)}\")\n",
    "                dg.logger.info(f\"length of data_availability_obj: {len(data_availability_obj)}\")\n",
    "                dg.logger.info(f\"data_availability_obj content: {data_availability_obj}\")\n",
    "\n",
    "                for idx, obj in enumerate(data_availability_obj):\n",
    "                    dg.logger.info(f\"Object type in data_availability_obj: {type(obj)}\")\n",
    "\n",
    "                    # Extract text and metadata from obj\n",
    "                    if isinstance(obj, dict) and 'text' in obj:\n",
    "                        normalized_input = obj['text']\n",
    "                        # Preserve all other attributes in metadata (excluding 'text')\n",
    "                        obj_metadata = {k: v for k, v in obj.items() if k != 'text'}\n",
    "                    elif isinstance(obj, str):\n",
    "                        normalized_input = obj\n",
    "                        obj_metadata = {}\n",
    "                    else:\n",
    "                        dg.logger.warning(f\"Unsupported object type in data_availability_obj: {type(obj)}\")\n",
    "                        continue\n",
    "\n",
    "                    # Render prompt using the correct parser\n",
    "                    static_prompt = dg.parser.prompt_manager.load_prompt(prompt_name)\n",
    "                    messages = dg.parser.prompt_manager.render_prompt(\n",
    "                                    static_prompt,\n",
    "                                    entire_doc=dg.full_document_read,\n",
    "                                    content=normalized_input,\n",
    "                                    repos=', '.join(dg.parser.repo_names) if hasattr(dg.parser, 'repo_names') else '',\n",
    "                                    url=url\n",
    "                                )\n",
    "                    \n",
    "                    # Create unique custom_id for each snippet\n",
    "                    snippet_custom_id = f\"{custom_id}_snippet_{idx}\"\n",
    "                                \n",
    "                    # Create batch request for LLMClient\n",
    "                    batch_request = {\n",
    "                                    'custom_id': snippet_custom_id,\n",
    "                                    'messages': messages,\n",
    "                                    'metadata': {\n",
    "                                        'url': url,\n",
    "                                        'article_id': article_id,\n",
    "                                        'raw_data_format': url_raw_data_format,\n",
    "                                        'snippet_index': idx,\n",
    "                                        **obj_metadata  # Preserve all attributes from obj\n",
    "                                    }\n",
    "                                }\n",
    "                                \n",
    "                    batch_requests.append(batch_request)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            dg.logger.error(f\"Error preparing request for {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        last_url_raw_data_format = url_raw_data_format\n",
    "        cnt+=1\n",
    "            \n",
    "dg.logger.info(f\"Prepared {len(batch_requests)} batch requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_result = dg.parser.llm_client._handle_batch_mode(\n",
    "                batch_requests=batch_requests,\n",
    "                batch_file_path=batch_file_path,\n",
    "                temperature=0,\n",
    "                response_format=dataset_response_schema_gpt,\n",
    "                api_provider='openai'\n",
    "                )\n",
    "            \n",
    "result = {\n",
    "    'batch_file_created': batch_result,\n",
    "    'fetched_data_count': len(publication_fulltext),\n",
    "    'processed_requests': len(batch_requests),\n",
    "    'api_provider': 'openai',\n",
    "    'model': dg.llm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da50d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_filepath = batch_file_path\n",
    "prompts_load = []\n",
    "with open(prompts_filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            prompts_load.append(json.loads(line))\n",
    "len(prompts_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a279865",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_avg, bad_input, empty_prompt = 0, 0, []\n",
    "n_dfs = len(prompts_load)\n",
    "\n",
    "for prompt in prompts_load:\n",
    "    pmc_id = dg.data_fetcher.url_to_pmcid(prompt['custom_id'])\n",
    "    gt = df_gt[df_gt['pmcid'] == pmc_id]\n",
    "    datasets_gt = gt['identifier'].values.tolist()\n",
    "    #print(f\"datasets: {datasets_gt}\")\n",
    "    body_msg = [item['content'] for item in prompt['body']['input']]\n",
    "    #print (f\"prompt: {body_msg}\")\n",
    "    input_cont_str = \"\\n\".join(body_msg)\n",
    "\n",
    "    datasets_found, datasets_tot = 0, len(datasets_gt)\n",
    "    contains_one = False\n",
    "    for dataset in datasets_gt:\n",
    "        if dataset.lower() in input_cont_str.lower():\n",
    "            datasets_found += 1\n",
    "            contains_one = True\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Missing dataset {dataset} in prompt {prompt['custom_id']} for pmcid {pmc_id}\")\n",
    "    if not contains_one:\n",
    "        bad_input += 1\n",
    "        empty_prompt.append(prompt['custom_id'])\n",
    "    found_i = datasets_found / datasets_tot if datasets_tot > 0 else 1.0\n",
    "    found_avg += found_i/n_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_avg, bad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f47360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chunking and submission - NO monitoring or result combination\n",
    "result = dg.split_jsonl_and_submit(\n",
    "    batch_file_path=batch_file_path,\n",
    "    max_file_size_mb=200.0,\n",
    "    api_provider='openai',\n",
    "    wait_between_submissions=30,\n",
    "    batch_description=f\"Training Dataset Creation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = 'batch_691bf0fcc7d081909d396c54e2082ced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eef3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dg.parser:\n",
    "    dg.init_parser_by_input_type('XML')\n",
    "\n",
    "res = dg.parser.llm_client.download_batch_results(\n",
    "    batch_id=batch_train,\n",
    "    output_file_path=output_batch_file,\n",
    "    api_provider='openai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_batch_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Number of lines in combined file: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_file = 'scripts/output/semantic_search/Train_results.csv'\n",
    "ret_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = dg.from_batch_resp_file_to_df(output_batch_file, output_file_path=ret_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(ret_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcids_ret = set([re.sub('(https://www.ncbi.nlm.nih.gov/pmc/articles/.*)/','\\\\1',item).lower() for item in res_df['source_url'].to_list()])\n",
    "pmcids = set([idx.lower() for idx in pmcids])\n",
    "missing_urls = list(pmcids - pmcids_ret)\n",
    "len(missing_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datasets_append = dg.process_articles(\n",
    "    missing_urls,\n",
    "    prompt_name=\"GPT_FewShot\",\n",
    "    full_document_read=FDR,\n",
    "    top_k = top_k,\n",
    "    semantic_retrieval=semantic_retrieval,\n",
    "    response_format=dataset_response_schema_gpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bece05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_datasets_append), type(new_datasets_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union dataframes\n",
    "for pmc_link in new_datasets_append.keys():\n",
    "    final_df = pd.concat([res_df, new_datasets_append[pmc_link]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(ret_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9321b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(ret_file)\n",
    "res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8252538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict creation\n",
    "train_dict = {}\n",
    "\n",
    "# Load prompts from the request file\n",
    "prompts_file = batch_file_path\n",
    "prompts_data = {}\n",
    "\n",
    "with open(prompts_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            prompt = json.loads(line)\n",
    "            custom_id = prompt['custom_id']\n",
    "            \n",
    "            # Extract content values from body.input array\n",
    "            input_messages = prompt['body']['input']\n",
    "            content_values = [msg['content'] for msg in input_messages if 'content' in msg]\n",
    "            just_context = content_values[1][1016:]\n",
    "            \n",
    "            # Store in prompts_data\n",
    "            prompts_data[custom_id] = {\n",
    "                'input_content': just_context,\n",
    "                'metadata': prompt.get('metadata', {})\n",
    "            }\n",
    "\n",
    "#print(f\"Loaded {len(prompts_data)} prompts from request file\")\n",
    "\n",
    "# Load results from the results file\n",
    "results_file = output_batch_file\n",
    "results_data = {}\n",
    "\n",
    "with open(results_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            result = json.loads(line)\n",
    "            custom_id = result['custom_id']\n",
    "            \n",
    "            # Extract output.text from response\n",
    "            output_text = None\n",
    "            if 'response' in result and 'body' in result['response']:\n",
    "                body = result['response']['body']\n",
    "                # Navigate: body -> output[0] -> content[0] -> text\n",
    "                if 'output' in body and len(body['output']) > 0:\n",
    "                    output_item = body['output'][0]\n",
    "                    if 'content' in output_item and len(output_item['content']) > 0:\n",
    "                        content_item = output_item['content'][0]\n",
    "                        if 'text' in content_item:\n",
    "                            output_text = content_item['text']\n",
    "            \n",
    "            results_data[custom_id] = {\n",
    "                'output_text': output_text\n",
    "            }\n",
    "\n",
    "#print(f\"Loaded {len(results_data)} results from results file\")\n",
    "\n",
    "# Combine prompts and results into train_dict\n",
    "for custom_id in prompts_data.keys():\n",
    "    if custom_id in results_data:\n",
    "        train_dict[custom_id] = {\n",
    "            'custom_id': custom_id,\n",
    "            'input_content': prompts_data[custom_id]['input_content'],\n",
    "            'output_text': results_data[custom_id]['output_text'],\n",
    "            'metadata': prompts_data[custom_id]['metadata']\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Warning: No result found for custom_id: {custom_id}\")\n",
    "\n",
    "print(f\"\\nCreated train_dict with {len(train_dict)} entries\")\n",
    "print(f\"Missing results: {len(prompts_data) - len(train_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8252538",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941bfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_n = 0\n",
    "for custom_id, entry in train_dict.items():\n",
    "    print(custom_id)\n",
    "    print(entry['metadata'])\n",
    "    print(entry['input_content'])\n",
    "    print(entry['output_text'])\n",
    "    iter_n += 1\n",
    "    if iter_n==3:\n",
    "        break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523b04c",
   "metadata": {},
   "source": [
    "## Dataset Validation\n",
    "Convert train_dict to DataFrame for validation and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_dict to DataFrame for validation\n",
    "validation_records = []\n",
    "\n",
    "for custom_id, entry in train_dict.items():\n",
    "    # Join input content into a single string for readability\n",
    "    input_text = \"\\n---\\n\".join(entry['input_content']) if isinstance(entry['input_content'], list) else str(entry['input_content'])\n",
    "    \n",
    "    record = {\n",
    "        'custom_id': entry['custom_id'],\n",
    "        'url': entry['metadata'].get('url', ''),\n",
    "        'article_id': entry['metadata'].get('article_id', ''),\n",
    "        'raw_data_format': entry['metadata'].get('raw_data_format', ''),\n",
    "        'snippet_index': entry['metadata'].get('snippet_index', ''),\n",
    "        'section_title': entry['metadata'].get('section_title', ''),\n",
    "        'sec_type': entry['metadata'].get('sec_type', ''),\n",
    "        'L2_distance': entry['metadata'].get('L2_distance', ''),\n",
    "        'input_text': input_text,\n",
    "        'output_text': entry['output_text'],\n",
    "        'validated': '',  # Empty column for manual validation\n",
    "        'notes': ''  # Empty column for notes\n",
    "    }\n",
    "    validation_records.append(record)\n",
    "\n",
    "validation_df = pd.DataFrame(validation_records)\n",
    "print(f\"Created validation DataFrame with {len(validation_df)} rows\")\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel for validation (recommended - preserves formatting and allows filtering)\n",
    "validation_df.to_excel(excel_output_path, index=False, engine='openpyxl')\n",
    "print(f\"Exported to Excel: {excel_output_path}\")\n",
    "\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"1. Open the Excel file to validate with filtering, sorting, and cell-by-cell editing\")\n",
    "print(\"2. Open the CSV file in any spreadsheet application\")\n",
    "print(\"3. Use the DataFrame below for in-notebook validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac0c8a",
   "metadata": {},
   "source": [
    "### Option 1: Interactive In-Notebook Validation (Optional)\n",
    "Use widgets to validate entries one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7df5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive validation widget (uncomment to use)\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# current_idx = 0\n",
    "# validation_status = {}\n",
    "\n",
    "# def show_entry(idx):\n",
    "#     if idx >= len(validation_df):\n",
    "#         print(\"End of dataset reached!\")\n",
    "#         return\n",
    "    \n",
    "#     entry = validation_df.iloc[idx]\n",
    "    \n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Entry {idx + 1} / {len(validation_df)}\")\n",
    "#     print(\"=\" * 80)\n",
    "#     print(f\"Custom ID: {entry['custom_id']}\")\n",
    "#     print(f\"URL: {entry['url']}\")\n",
    "#     print(f\"Article ID: {entry['article_id']}\")\n",
    "#     print(f\"Section: {entry['section_title']} ({entry['sec_type']})\")\n",
    "#     print(f\"L2 Distance: {entry['L2_distance']}\")\n",
    "#     print(\"\\n--- INPUT TEXT ---\")\n",
    "#     print(entry['input_text'][:1000] + \"...\" if len(str(entry['input_text'])) > 1000 else entry['input_text'])\n",
    "#     print(\"\\n--- OUTPUT TEXT ---\")\n",
    "#     print(entry['output_text'])\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "# def mark_valid(b):\n",
    "#     global current_idx\n",
    "#     validation_status[current_idx] = 'valid'\n",
    "#     validation_df.loc[current_idx, 'validated'] = 'valid'\n",
    "#     current_idx += 1\n",
    "#     show_entry(current_idx)\n",
    "\n",
    "# def mark_invalid(b):\n",
    "#     global current_idx\n",
    "#     validation_status[current_idx] = 'invalid'\n",
    "#     validation_df.loc[current_idx, 'validated'] = 'invalid'\n",
    "#     current_idx += 1\n",
    "#     show_entry(current_idx)\n",
    "\n",
    "# def skip_entry(b):\n",
    "#     global current_idx\n",
    "#     current_idx += 1\n",
    "#     show_entry(current_idx)\n",
    "\n",
    "# # Create buttons\n",
    "# valid_btn = widgets.Button(description=\"✓ Valid\", button_style='success')\n",
    "# invalid_btn = widgets.Button(description=\"✗ Invalid\", button_style='danger')\n",
    "# skip_btn = widgets.Button(description=\"→ Skip\", button_style='info')\n",
    "\n",
    "# valid_btn.on_click(mark_valid)\n",
    "# invalid_btn.on_click(mark_invalid)\n",
    "# skip_btn.on_click(skip_entry)\n",
    "\n",
    "# # Display interface\n",
    "# show_entry(current_idx)\n",
    "# display(widgets.HBox([valid_btn, invalid_btn, skip_btn]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aeb5b9",
   "metadata": {},
   "source": [
    "### Load Validated Data Back\n",
    "After validation in Excel/CSV, load the validated data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634889e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validated data back from Excel or CSV\n",
    "# validated_df = pd.read_excel('scripts/output/training_dataset_validation.xlsx')\n",
    "# # OR\n",
    "# validated_df = pd.read_csv('scripts/output/training_dataset_validation.csv')\n",
    "\n",
    "# Filter only valid entries\n",
    "# valid_entries = validated_df[validated_df['validated'] == 'valid']\n",
    "# print(f\"Valid entries: {len(valid_entries)} / {len(validated_df)}\")\n",
    "\n",
    "# # Get statistics\n",
    "# validation_summary = validated_df['validated'].value_counts()\n",
    "# print(\"\\nValidation Summary:\")\n",
    "# print(validation_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed271994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split pmcids into train / val / test using sklearn (70/15/15 by default)\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "random_state = 42\n",
    "\n",
    "# first split off the test set\n",
    "train_val, test_ids = sklearn.model_selection.train_test_split(\n",
    "    pmcids, test_size=test_size, random_state=random_state, shuffle=True\n",
    ")\n",
    "\n",
    "# compute relative validation size w.r.t. the remaining data and split train/val\n",
    "val_relative = val_size / (1.0 - test_size)\n",
    "train_ids, val_ids = sklearn.model_selection.train_test_split(\n",
    "    train_val, test_size=val_relative, random_state=random_state, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Total: {len(pmcids)}  -> train: {len(train_ids)}, val: {len(val_ids)}, test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07152885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc86b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b74a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
